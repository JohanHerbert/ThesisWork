{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim\n",
    "from scipy.interpolate import BarycentricInterpolator\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446496a",
   "metadata": {},
   "source": [
    "# To test method for neural copula\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f741d78",
   "metadata": {},
   "source": [
    "## Defining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d97c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class MarginalModel(nn.Module):\n",
    "    def __init__(self, num_layers=5, num_neurons=5, lr=0.01):\n",
    "        super(MarginalModel, self).__init__()\n",
    "\n",
    "        # Model specification\n",
    "        layers = [nn.Linear(1, num_neurons), nn.Tanh()]  # Input layer\n",
    "        for _ in range(num_layers - 1):  # Hidden layers\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons, 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # Data for training\n",
    "        self.ObservedData = None\n",
    "        self.uniform_data = torch.tensor(np.linspace(0, 1, 10000), dtype=torch.float32).view(-1, 1)\n",
    "        self.lower_bound = torch.tensor([[0.0]]) \n",
    "        self.upper_bound = torch.tensor([[1.0]])\n",
    "\n",
    "        ## For sampling\n",
    "        self.domainUpper = torch.tensor([[1.0]])\n",
    "        self.domainLower= torch.tensor([[0.0]])\n",
    "        self.inverted = False\n",
    "        self.inverseInterpolator = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def loss_function(self, x):\n",
    "        x.requires_grad = True\n",
    "        self.uniform_data.requires_grad = True\n",
    "        y_pred = self(x)\n",
    "        y_pred_uniform = self(self.uniform_data)\n",
    "        dydx = torch.autograd.grad(y_pred, x, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "        dydx_uniform = torch.autograd.grad(y_pred_uniform, self.uniform_data, torch.ones_like(y_pred_uniform), create_graph=True)[0]\n",
    "\n",
    "        L1 = -torch.mean(torch.log(torch.relu(dydx) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-dydx_uniform))\n",
    "        L3 = torch.abs(1 - torch.sum(dydx_uniform)/self.uniform_data.shape[0])\n",
    "        L4 = self(self.lower_bound) + torch.abs(1 - self(self.upper_bound))\n",
    "        Loss = L1 + L2 + L3 +  L4\n",
    "        return Loss , L1, L2, L3, L4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self, X, epochs=5000, log_interval=500):\n",
    "        print('Training model')\n",
    "        self.ObservedData = X\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss, L1, L2, L3, L4 = self.loss_function(X)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if epoch % log_interval == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}, Constraint losses: L1: {L1.item()}, L2: {L2.item()}, L3: {L3.item()}, L4: {L4.item()}')\n",
    "\n",
    "    def newSamples(self, ProbabilityValues = None, n = 1000): \n",
    "        # Sampling method that replaces the interpolator which struggles with values close to 0 and 1\n",
    "        if ProbabilityValues is None:\n",
    "            ProbabilityValues = np.random.uniform(0, 1, n)\n",
    "        if torch.is_tensor(ProbabilityValues) == False:\n",
    "            ProbabilityValues = torch.tensor(ProbabilityValues, dtype=torch.float32).view(-1, 1)\n",
    "        sampledData = self._vectorized_bisection(ProbabilityValues).detach().numpy()\n",
    "        return sampledData\n",
    "\n",
    "    # def sample(self, n = 1000, ProbabilityValues = None): # Can sample only from marginal and with given probability values\n",
    "    #     if self.inverted == False:\n",
    "    #         print(\"Model not inverted. Inverting model...\")\n",
    "    #         self.inverseInterpolator = self._invertModel(asTensor=False)\n",
    "    #         self.inverted = True\n",
    "\n",
    "    #     if ProbabilityValues is None:\n",
    "    #         ProbabilityValues = np.random.uniform(0, 1, n)\n",
    "            \n",
    "    #     # Generate random samples from a uniform distribution\n",
    "    #     sampledData = self.inverseInterpolator(ProbabilityValues)\n",
    "    #     return sampledData\n",
    "\n",
    "    # def _invertModel(self, asTensor=True, plot=False):\n",
    "    #     # Invert the model to get inverse CDF function\n",
    "    #     rangeUpper = self(self.domainUpper)\n",
    "    #     rangeLower= self(self.domainLower)\n",
    "    #     # Generate Chebyshev nodes\n",
    "    #     n = 3000\n",
    "    #     rangePoints = self._chebyshev_nodes(n, rangeLower, rangeUpper).view(-1, 1)\n",
    "\n",
    "    #     # use nodes to find inverses\n",
    "    #     domainPoints = self._vectorized_bisection(rangePoints)\n",
    "\n",
    "    #     # Add boundary points\n",
    "    #     rangePoints = torch.cat((torch.tensor([0.0]), rangePoints.squeeze(), torch.tensor([1.0])))\n",
    "    #     domainPoints = torch.cat((torch.tensor([0.0]), domainPoints.squeeze(), torch.tensor([1.0])))\n",
    "    #     rangePoints_np = rangePoints.numpy()\n",
    "    #     domainPoints_np = domainPoints.numpy()\n",
    "\n",
    "    #     # Create interpolator\n",
    "    #     interpolant = BarycentricInterpolator(rangePoints_np, domainPoints_np)\n",
    "    #     interpolantTensor = lambda x: self._tensor_interpolant(x, interpolant) # interpolant using PyTorch tensors\n",
    "\n",
    "    #     if plot:\n",
    "    #         self.plotModel(model = interpolantTensor)\n",
    "    #     if asTensor:\n",
    "    #         return interpolantTensor\n",
    "    #     else:\n",
    "    #         return interpolant\n",
    "        \n",
    "    # def _tensor_interpolant(self, p_tensor, interpolant):\n",
    "    #     \"\"\"Interpolant function that takes PyTorch tensors as input.\"\"\"\n",
    "    #     p_numpy = p_tensor.detach().cpu().numpy()\n",
    "    #     x_numpy = interpolant(p_numpy)\n",
    "    #     return torch.tensor(x_numpy, dtype=torch.float32)\n",
    "\n",
    "    def PlotModel(self):\n",
    "        trainingData = self.ObservedData.detach().numpy()\n",
    "\n",
    "        # # Generate x values for plotting\n",
    "        x_points = np.linspace(0, 1, 100)\n",
    "        x_plot =torch.tensor(x_points, dtype=torch.float32).view(-1, 1)\n",
    "        x_plot.requires_grad = True  # Enable gradients for x_plot\n",
    "        y_pred = self(x_plot)  # Keep y_pred in computation graph\n",
    "        pdfPred = torch.autograd.grad(y_pred, x_plot, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "\n",
    "        ### Convert to numpy for plotting\n",
    "        y_vals = y_pred.detach().numpy()\n",
    "        grad_vals = pdfPred.detach().numpy()\n",
    "\n",
    "        ### Plot the neural network approximation\n",
    "        plt.plot(x_points, y_vals, label='CDF approximation', linestyle='dashed')\n",
    "        ### Plot the derivative\n",
    "        plt.plot(x_points, grad_vals, label=\"PDF Approximation\", linestyle='solid')\n",
    "        plt.hist(trainingData, bins=1000, density=True, alpha=0.6, label='True Distribution of data');\n",
    "        plt.legend()\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Neural Network Approximation of PDF and CDF ')\n",
    "        plt.show()\n",
    "\n",
    "    # def PlotInverse(self):\n",
    "    #     if self.inverted == False:\n",
    "    #         print(\"Model not inverted. Inverting model...\")\n",
    "    #         self.inverseInterpolator = self._invertModel(asTensor=False)\n",
    "    #         self.inverted = True    \n",
    "        \n",
    "    #     x_points = np.linspace(0, 1, 1000)\n",
    "    #     y_pred = self.inverseInterpolator(x_points)  \n",
    "    #     #ChebyshovPoints = self._chebyshev_nodes(3000, 0, 1).numpy()\n",
    "    #     plt.plot(x_points, y_pred, color='blue', label='Inverse CDF approximation')  # Blue line\n",
    "    #     #plt.scatter(ChebyshovPoints, self.inverseInterpolator(ChebyshovPoints), color='red', label='Chebyshev nodes')  # Red points\n",
    "    #     plt.xlabel('x')\n",
    "    #     plt.ylabel('Inverse CDF')\n",
    "    #     plt.title('Inverse CDF Approximation')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "        \n",
    "    # def _chebyshev_nodes(self, n, a, b):\n",
    "    #     \"\"\"Generate n Chebyshev nodes in the interval [a, b].\"\"\"\n",
    "    #     return torch.tensor([0.5 * (a + b) + 0.5 * (b - a) * np.cos((2 * k + 1) * np.pi / (2 * n)) for k in range(n)], dtype=torch.float32)\n",
    "\n",
    "    def _vectorized_bisection(self, y, tol=1e-6, max_iter=100):\n",
    "        \"\"\"\n",
    "        Vectorized Bisection Method to find roots of a function f(y) in the interval [0,1] for multiple values of y simultaneously.\n",
    "        \n",
    "        Parameters:\n",
    "        f : function\n",
    "            The function whose roots are to be found.\n",
    "        y : torch.Tensor\n",
    "            Tensor of values for which roots are to be found.\n",
    "        tol : float, optional\n",
    "            The tolerance for stopping the iteration (default is 1e-6).\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations (default is 100).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor\n",
    "            Tensor of estimated root values.\n",
    "        \"\"\"\n",
    "        a, b = torch.zeros_like(y), torch.ones_like(y)\n",
    "        fa, fb = self(a) , self(b) \n",
    "        fa, fb = fa - y, fb - y\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            c = (a + b) / 2  # Midpoint\n",
    "            fc = self(c) - y\n",
    "            left_mask = fc * fa < 0\n",
    "            right_mask = fc * fb < 0\n",
    "            a, b = torch.where(left_mask, a, c), torch.where(right_mask, b, c)\n",
    "            if torch.all(torch.abs(b - a) < tol):\n",
    "                break\n",
    "        return (a + b) / 2\n",
    "\n",
    "    def CreateNormalizedTensor(self, data, scaling=1.0):\n",
    "        boundaryPoints = scaling * np.array([np.max(data), np.min(data)])\n",
    "        data = np.concatenate((data, boundaryPoints))\n",
    "        dataWithBoundaryPoints = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "        data_tensor = torch.tensor(dataWithBoundaryPoints, dtype=torch.float32).view(-1, 1)\n",
    "        return data_tensor\n",
    "\n",
    "    def DenormalizeData(self, normalizedDataTensor):\n",
    "        normalizedData = normalizedDataTensor.detach().numpy()\n",
    "        denormalizedData = normalizedData * (np.max(self.ObservedData, axis=0) - np.min(self.ObservedData, axis=0)) + np.min(self.ObservedData, axis=0)\n",
    "        return denormalizedData\n",
    "\n",
    "    def evaluateCDFData(self, x):\n",
    "        x.requires_grad = True\n",
    "        y_pred = self(x)\n",
    "        return y_pred.detach().numpy()\n",
    "\n",
    "    def evaluatePDFData(self, x):\n",
    "        x.requires_grad = True\n",
    "        y_pred = self(x)\n",
    "        dydx = torch.autograd.grad(y_pred, x, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "        return dydx.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CopulaModel(nn.Module):\n",
    "    def __init__(self, dataPoints, Marginal1, Marginal2, num_layers=5, num_neurons=5, lr=0.01,\n",
    "                solver = 'sgd', scheduler = None, batch_size = 512 ,boundary_points = 100, uniform_points = 15):\n",
    "        super(CopulaModel, self).__init__()\n",
    "        dimensions = dataPoints.size(1)\n",
    "        layers = [nn.Linear(dimensions, num_neurons), nn.Tanh()]  # Input layer\n",
    "        for _ in range(num_layers - 1):  # Hidden layers\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons, 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "        ## Optimizer\n",
    "        if solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        elif solver == 'adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        ## Learning rate scheduler\n",
    "        if scheduler == 'step':\n",
    "            self.scheduler = StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
    "        elif scheduler == 'exponential':\n",
    "            self.scheduler = ExponentialLR(self.optimizer, gamma=0.9)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        ### Data for training\n",
    "        self.ObservedData = dataPoints\n",
    "        self.Marginal1 = Marginal1\n",
    "        self.Marginal2 = Marginal2\n",
    "        # Boundary points\n",
    "        self.upperBoundary = self._generateUpperBoundaryPoints(dimensions,num_points=boundary_points)\n",
    "        self.lowerBoundary = self._generateLowerBoundPoints(dimensions,num_points=boundary_points)\n",
    "\n",
    "        # Uniform grid points\n",
    "        u= np.linspace(0.0, 1.0, uniform_points)\n",
    "        U1, U2 = np.meshgrid(u, u, indexing=\"ij\")\n",
    "        unitSquarePoints= np.column_stack((U1.ravel(), U2.ravel()))\n",
    "        self.unitSquaretensor = torch.tensor(unitSquarePoints, dtype=torch.float32)\n",
    "\n",
    "        # FlagSum calculator\n",
    "        self.flagSumData = self._FlagSum(self.unitSquaretensor, self.unitSquaretensor)\n",
    "        self.delta_m = 1 / dataPoints.shape[0]\n",
    "\n",
    "        # Practicality\n",
    "        self.batch_size = batch_size\n",
    "        self.isTrained = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def _CopulaGradient(self,x):\n",
    "        x1 = x[:,0]\n",
    "        x2 = x[:,1]\n",
    "        # Probability integral transform\n",
    "        ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "        ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "        u = torch.cat((ProbVals1, ProbVals2), dim=1)\n",
    "        # Calculate marginal densities\n",
    "        gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True, allow_unused=True)[0]\n",
    "        gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True, allow_unused=True )[0]\n",
    "        # Prediction and gradient of copula CDF\n",
    "        y_pred = self(u)\n",
    "        gradCopulaModel = torch.autograd.grad(y_pred, u, torch.ones_like(y_pred), create_graph=True, allow_unused=True )[0]\n",
    "        CopulaGradient = gradCopulaModel[:, 0] * gradM1 * gradCopulaModel[:, 1] * gradM2\n",
    "        return CopulaGradient\n",
    "\n",
    "    def Copula_loss_function(self, x): ## MAKE SURE TO PASS IN THE SAME DATAPOINTS AS IN THE INITIALIZATION\n",
    "        x = x.detach().requires_grad_()\n",
    "        self.unitSquaretensor = self.unitSquaretensor.detach().requires_grad_()\n",
    "        CopulaGradientObserved = self._CopulaGradient(x)\n",
    "        n_observed = x.shape[0]\n",
    "        CopulaGradientUnitSquare = self._CopulaGradient(self.unitSquaretensor)\n",
    "        n_unitsquare = self.unitSquaretensor.shape[0]\n",
    "        flagSumData = self._FlagSum(x, self.unitSquaretensor)\n",
    "        pred_unitSquare = self(self.unitSquaretensor)\n",
    "\n",
    "        L1 = -torch.mean(torch.log(torch.relu(CopulaGradientObserved) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-CopulaGradientUnitSquare))\n",
    "        L3 = torch.abs(1 - torch.sum(CopulaGradientUnitSquare / CopulaGradientUnitSquare.shape[0]))\n",
    "        L4 = torch.sum(self(self.lowerBoundary)) + torch.sum(torch.abs(self(self.upperBoundary) - torch.min(self.upperBoundary, dim=1).values.view(-1,1)))      \n",
    "        L5 = 1/(n_observed*n_unitsquare) * torch.sum(torch.abs(pred_unitSquare - flagSumData))\n",
    "\n",
    "        Loss = L1 + L2 + L3 + L4 + L5\n",
    "        return Loss, L1, L2, L3, L4, L5\n",
    "    \n",
    "    def Copula_loss_function_Pretrain2(self, x): ## MAKE SURE TO PASS IN THE SAME DATAPOINTS AS IN THE INITIALIZATION\n",
    "        x = x.detach().requires_grad_()\n",
    "        self.unitSquaretensor = self.unitSquaretensor.detach().requires_grad_()\n",
    "        CopulaGradientObserved = self._CopulaGradient(x)\n",
    "        n_observed = x.shape[0]\n",
    "        CopulaGradientUnitSquare = self._CopulaGradient(self.unitSquaretensor)\n",
    "        n_unitsquare = self.unitSquaretensor.shape[0]\n",
    "        flagSumData = self._FlagSum(x, self.unitSquaretensor)\n",
    "        pred_unitSquare = self(self.unitSquaretensor)\n",
    "\n",
    "        L1 = -torch.mean(torch.log(torch.relu(CopulaGradientObserved) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-CopulaGradientUnitSquare))\n",
    "        L3 = torch.abs(1 - torch.sum(CopulaGradientUnitSquare / CopulaGradientUnitSquare.shape[0]))\n",
    "        L4 = torch.sum(self(self.lowerBoundary)) + torch.sum(torch.abs(self(self.upperBoundary) - torch.min(self.upperBoundary, dim=1).values.view(-1,1)))      \n",
    "        L5 = 1/(n_observed*n_unitsquare) * torch.sum(torch.abs(pred_unitSquare - flagSumData))\n",
    "\n",
    "        Loss = L2 + L3 + L4 + L5\n",
    "        return Loss, L1, L2, L3, L4, L5\n",
    "    \n",
    "    def Copula_loss_function_Pretrain3(self, x): ## MAKE SURE TO PASS IN THE SAME DATAPOINTS AS IN THE INITIALIZATION\n",
    "        x = x.detach().requires_grad_()\n",
    "        self.unitSquaretensor = self.unitSquaretensor.detach().requires_grad_()\n",
    "        CopulaGradientObserved = self._CopulaGradient(x)\n",
    "        n_observed = x.shape[0]\n",
    "        CopulaGradientUnitSquare = self._CopulaGradient(self.unitSquaretensor)\n",
    "        n_unitsquare = self.unitSquaretensor.shape[0]\n",
    "        flagSumData = self._FlagSum(x, self.unitSquaretensor)\n",
    "        pred_unitSquare = self(self.unitSquaretensor)\n",
    "\n",
    "        L1 = -torch.mean(torch.log(torch.relu(CopulaGradientObserved) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-CopulaGradientUnitSquare))\n",
    "        L3 = torch.abs(1 - torch.sum(CopulaGradientUnitSquare / CopulaGradientUnitSquare.shape[0]))\n",
    "        L4 = torch.sum(self(self.lowerBoundary)) + torch.sum(torch.abs(self(self.upperBoundary) - torch.min(self.upperBoundary, dim=1).values.view(-1,1)))      \n",
    "        L5 = 1/(n_observed*n_unitsquare) * torch.sum(torch.abs(pred_unitSquare - flagSumData))\n",
    "\n",
    "        x1 = x[:,0]\n",
    "        x2 = x[:,1]\n",
    "        # Probability integral transform\n",
    "        ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "        ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "        u = torch.cat((ProbVals1, ProbVals2), dim=1)\n",
    "        ## prediction of copula\n",
    "        copulaPred = self(u)\n",
    "        Loss = torch.mean((copulaPred - u[0,:]*u[1,:])**2)\n",
    "\n",
    "        return Loss, L1, L2, L3, L4, L5\n",
    "    \n",
    "\n",
    "    def _generateLowerBoundPoints(self, d, num_points=100):\n",
    "        grid = np.linspace(0, 1, num_points)\n",
    "        all_surfaces = []\n",
    "        for k in range(d):\n",
    "            grid_points = np.meshgrid(*([grid] * (d - 1)), indexing=\"ij\")\n",
    "            points = np.stack(grid_points, axis=-1).reshape(-1, d - 1)\n",
    "            surface_points = np.insert(points, k, 0, axis=1)\n",
    "            all_surfaces.append(surface_points)\n",
    "        return torch.tensor(np.vstack(all_surfaces), dtype=torch.float32)\n",
    "\n",
    "    def _generateUpperBoundaryPoints(self, d, num_points=100):\n",
    "        oneArray = np.ones((d * num_points, d))\n",
    "        u = np.linspace(0, 1, num_points)\n",
    "        for i in range(d):\n",
    "            oneArray[i * num_points:(i + 1) * num_points, i] = u\n",
    "        return torch.tensor(oneArray, dtype=torch.float32)\n",
    "    \n",
    "    def _flag(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized version of _flag that returns a matrix where each element is a comparison result.\"\"\"\n",
    "        return torch.all(y.unsqueeze(0) < x.unsqueeze(1), dim=2).float()\n",
    "\n",
    "    def _FlagSum(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized computation of FlagSum without explicit Python loops.\"\"\"\n",
    "        return self._flag(x, y).sum(dim=1)\n",
    "\n",
    "    def train_model(self, X, epochs=5000, log_interval=500, method = 'regular'):\n",
    "            # dataset = torch.utils.data.TensorDataset(X)\n",
    "            # data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            for epoch in range(epochs):\n",
    "                #for batch_idx, (data,) in enumerate(data_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if method == 'regular':\n",
    "                    loss, L1, L2, L3, L4, L5 = self.Copula_loss_function(X)\n",
    "                elif method == 'pretrain2':\n",
    "                    loss, L1, L2, L3, L4, L5 = self.Copula_loss_function_Pretrain2(X)\n",
    "                elif method == 'pretrain3':\n",
    "                    loss, L1, L2, L3, L4, L5 = self.Copula_loss_function_Pretrain3(X)\n",
    "\n",
    "                #loss, L1, L2, L3, L4, L5 = self.Copula_loss_function(X)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                #loss, L1, L2, L3, L4, L5 = self.Copula_loss_function(X)\n",
    "                if epoch % log_interval == 0:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss.item()}, Losses: L1: {L1.item()}, L2: {L2.item()}, L3: {L3.item()}, L4: {L4.item()}, L5: {L5.item()}')\n",
    "\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            self.isTrained = True\n",
    "\n",
    "\n",
    "    def sample(self, n = 1000):\n",
    "        if self.isTrained == False:\n",
    "            print(\"Model not trained. Training model...\")\n",
    "            self.train_model(self.ObservedData)\n",
    "            self.isTrained = True\n",
    "\n",
    "        # Generate random samples from a uniform distribution\n",
    "        u = np.random.uniform(0, 1, (n, 2))\n",
    "        samples = np.zeros(u.shape)\n",
    "        samples[:,0] = u[:,0]\n",
    "        u = torch.tensor(u, dtype=torch.float32)\n",
    "        u1 = u[:,0].view(-1, 1)\n",
    "        ones = torch.ones_like(u1)\n",
    "        uBoundary = torch.cat((u1, ones), dim=1)\n",
    "        # Generate random samples from the copula\n",
    "        scalings = self(uBoundary)\n",
    "        z1 = u[:,1].unsqueeze(1) * scalings # Scaled random numbers on height of copula\n",
    "        ## solve for u2 given u1 and z1\n",
    "        u2 = self._vectorized_bisection(z1, u1_fixed = u1)\n",
    "        samples[:,1] = u2.detach().numpy().flatten()\n",
    "        return samples\n",
    "    \n",
    "    def _PartialCopula(self, u1_fixed, u2):\n",
    "        u = torch.cat((u1_fixed, u2), dim=1)\n",
    "        return self(u)\n",
    "    \n",
    "    def _vectorized_bisection(self, z, u1_fixed, tol=1e-6, max_iter=100):\n",
    "        \"\"\"\n",
    "        Vectorized Bisection Method to find roots of a function f(z) in the interval [0,1] for multiple values of z simultaneously.\n",
    "        \n",
    "        Parameters:\n",
    "        f : function\n",
    "            The function whose roots are to be found.\n",
    "        z : torch.Tensor\n",
    "            Tensor of values for which roots are to be found.\n",
    "        tol : float, optional\n",
    "            The tolerance for stopping the iteration (default is 1e-6).\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations (default is 100).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor\n",
    "            Tensor of estimated root values.\n",
    "        \"\"\"\n",
    "        a, b = torch.zeros_like(z), torch.ones_like(z)\n",
    "        fa, fb = self._PartialCopula(u1_fixed, a) , self._PartialCopula(u1_fixed, b) \n",
    "        fa, fb = fa - z, fb - z\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            c = ((a + b) / 2)  # Midpoint\n",
    "            fc = self._PartialCopula(u1_fixed, c) - z\n",
    "            left_mask = fc * fa < 0\n",
    "            right_mask = fc * fb < 0\n",
    "            a, b = torch.where(left_mask, a, c), torch.where(right_mask, b, c)\n",
    "            if torch.all(torch.abs(b - a) < tol):\n",
    "                break\n",
    "        return (a + b) / 2\n",
    "    \n",
    "    def plotSamples(self, sample, ProbSpace = True):\n",
    "        df_samples = pd.DataFrame({\n",
    "        \"U1\": sample[:,0].flatten(),  \n",
    "        \"U2\": sample[:,1].flatten()\n",
    "        })\n",
    "        sns.jointplot(\n",
    "            data=df_samples, x=\"U1\", y=\"U2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "        if ProbSpace:\n",
    "            plt.suptitle(\"Sampled data in probability space\", y=1.02);  \n",
    "        else:\n",
    "            plt.suptitle(\"Sampled data in return space\", y=1.02);\n",
    "        plt.show()\n",
    "        pass\n",
    "\n",
    "\n",
    "class NeuralCopula():\n",
    "    def __init__(self, data, params):\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.num_neurons = params['num_neurons']\n",
    "        self.lr = params['lr']\n",
    "        self.scheduler = params['scheduler']\n",
    "        self.solver = params['solver']\n",
    "        self.epochs = params['epochs']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.uniform_points = params['uniform_points']\n",
    "        self.boundary_points = params['boundary_points']\n",
    "        \n",
    "        self.Marginal1 = None\n",
    "        self.Marginal2 = None\n",
    "        self.Copula = None\n",
    "        self.initialCopulaWeights = None\n",
    "        self.SetInitialWeights = None\n",
    "        self.copulaTrainingTime = None\n",
    "        self.data = data\n",
    "        self.normalizedData = None\n",
    "        self.normalizedDataAsTensor = None\n",
    "        self.isNormalized = False\n",
    "\n",
    "        ## Normalization variables\n",
    "        self.scaling = 2.0\n",
    "        self.M1_upper = None\n",
    "        self.M1_lower = None\n",
    "        self.M2_upper = None\n",
    "        self.M2_lower = None\n",
    "\n",
    "    def normalizeData(self,scaling=2.0):\n",
    "        scaling = 2.0\n",
    "        n = self.data.shape[0]\n",
    "        d = self.data.shape[1]\n",
    "\n",
    "        self.M1_upper = scaling * np.max(self.data[:,0])\n",
    "        self.M1_lower = scaling * np.min(self.data[:,0])\n",
    "        self.M2_upper = scaling * np.max(self.data[:,1])\n",
    "        self.M2_lower = scaling * np.min(self.data[:,1])\n",
    "\n",
    "        M1boundaryPoints =  np.array([self.M1_upper, self.M1_lower]) # Creates points for bounds of what the data generated can be\n",
    "        M2boundaryPoints =  np.array([self.M2_upper, self.M2_lower]) # Creates points for bounds of what the data generated can be\n",
    "        extendedData = np.zeros((n+2, d))\n",
    "        extendedData[:,0] = np.concatenate((self.data[:,0], M1boundaryPoints)) # Adding boundary points to the data\n",
    "        extendedData[:,1] = np.concatenate((self.data[:,1], M2boundaryPoints))\n",
    "        self.normalizedData= (extendedData - np.min(extendedData,axis=0)) / (np.max(extendedData,axis=0) - np.min(extendedData,axis=0))\n",
    "        self.normalizedDataAsTensor = torch.tensor(self.normalizedData, dtype=torch.float32)\n",
    "        self.isNormalized = True\n",
    "        pass\n",
    "\n",
    "    def denormalizeData(self,NormalizedData):\n",
    "        DeNormalizedData = NormalizedData * (np.max(self.data, axis=0) - np.min(self.data, axis=0)) + np.min(self.data, axis=0)\n",
    "        return DeNormalizedData\n",
    "\n",
    "    def fitModel(self, method = 'regular'):\n",
    "        ## Training marginals\n",
    "        self.Marginal1 = MarginalModel(num_layers=6, num_neurons=10, lr=0.01)\n",
    "        self.Marginal2 = MarginalModel(num_layers=6, num_neurons=10, lr=0.01)\n",
    "        print('Marginal Model 1 Training')\n",
    "        self.Marginal1.train_model(self.normalizedDataAsTensor[:-2,0].view(-1, 1), epochs=2500, log_interval=500)\n",
    "        print('Marginal Model 2 Training')\n",
    "        self.Marginal2.train_model(self.normalizedDataAsTensor[:-2,1].view(-1, 1), epochs=2500, log_interval=500)\n",
    "        ## Training copula\n",
    "        self.Copula = CopulaModel(self.normalizedDataAsTensor[:-2], self.Marginal1, self.Marginal2, num_layers=self.num_layers, num_neurons=self.num_neurons, lr=self.lr, solver=self.solver, scheduler=self.scheduler, batch_size=self.batch_size, boundary_points=self.boundary_points, uniform_points=self.uniform_points) \n",
    "        if self.SetInitialWeights is not None:\n",
    "            self.Copula.load_state_dict(self.SetInitialWeights)\n",
    "\n",
    "        self.initialCopulaWeights = self.Copula.state_dict()\n",
    "        print('Training copula model')\n",
    "        start_time = time.time()\n",
    "        self.Copula.train_model(self.normalizedDataAsTensor[:-2], epochs=self.epochs, log_interval=500,method=method)\n",
    "        self.copulaTrainingTime = time.time() - start_time\n",
    "        print('Training done')\n",
    "        pass\n",
    "\n",
    "    def sample(self, Plot = False, n=1000):\n",
    "        copulaSample = self.Copula.sample(n)\n",
    "        marginalSamples = self.sample_marginals(copulaSample)\n",
    "        denormalizedSamples = self.denormalizeData(marginalSamples)\n",
    "        if Plot:\n",
    "            self.Copula.plotSamples(copulaSample, ProbSpace = True)\n",
    "            self.Copula.plotSamples(denormalizedSamples, ProbSpace = False)\n",
    "        return denormalizedSamples\n",
    "\n",
    "    def sample_marginals(self,probabilityValues, n=1000):\n",
    "        sample_marginal1 = self.Marginal1.newSamples(ProbabilityValues=probabilityValues[:,0])\n",
    "        sample_marginal2 = self.Marginal2.newSamples(ProbabilityValues=probabilityValues[:,1])\n",
    "        samples = np.column_stack((sample_marginal1, sample_marginal2))\n",
    "        return samples\n",
    "\n",
    "    def sample_copula(self, Plot = False, n = 1000):\n",
    "        copulaSample = self.Copula.sample(n)\n",
    "        if Plot:\n",
    "            self.Copula.plotSamples(copulaSample, ProbSpace = True)\n",
    "        return copulaSample\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class NC_validator():\n",
    "    def __init__(self, NeuralCopula):\n",
    "        self.NC = NeuralCopula\n",
    "        self.Marginal1 = NeuralCopula.Marginal1\n",
    "        self.Marginal2 = NeuralCopula.Marginal2\n",
    "        self.Copula = NeuralCopula.Copula\n",
    "        self.normalizedDataASTensor = NeuralCopula.normalizedDataAsTensor\n",
    "        \n",
    "\n",
    "    def validate(self):\n",
    "        print('Marginal model validation')\n",
    "        self._validateMarginals()\n",
    "        print('Copula model validation')\n",
    "        self._validateCopula()\n",
    "        pass\n",
    "\n",
    "    def _validateMarginals(self): \n",
    "        ## Plot the marginal models\n",
    "        plt.fisize=(3, 3)\n",
    "        print('Maginal model 1')\n",
    "        self.Marginal1.PlotModel()\n",
    "        print('Maginal model 2')\n",
    "        self.Marginal2.PlotModel()\n",
    "\n",
    "        # ## Plot inverses of models to check if they are correct\n",
    "        # print('Maginal model 1')\n",
    "        # self.Marginal1.PlotInverse()\n",
    "        # print('Maginal model 2')\n",
    "        # self.Marginal2.PlotInverse()\n",
    "\n",
    "        ## Plot sampled data from the models\n",
    "        Marginal1_samples = self.Marginal1.newSamples(n = 5000)\n",
    "        Marginal2_samples = self.Marginal2.newSamples(n = 5000)\n",
    "\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\n",
    "        x_tensor.requires_grad = True\n",
    "\n",
    "        y_pred1 = self.Marginal1(x_tensor)\n",
    "        dydx1 = torch.autograd.grad(y_pred1, x_tensor, torch.ones_like(y_pred1), create_graph=True)[0]\n",
    "        plt.hist(Marginal1_samples, bins=100, density=True, alpha=1, label='Model 1 samples')\n",
    "        plt.plot(x, dydx1.detach().numpy(), label='Fitted distribution', color='red')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        y_pred2 = self.Marginal2(x_tensor)\n",
    "        dydx2 = torch.autograd.grad(y_pred2, x_tensor, torch.ones_like(y_pred2), create_graph=True)[0]\n",
    "        plt.hist(Marginal2_samples, bins=100, density=True, alpha=1, label='Model 2 samples')\n",
    "        plt.plot(x, dydx2.detach().numpy(), label='Fitted distribution', color='red')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        ## Plot initial data\n",
    "        OriginalMarginal1 = self.normalizedDataASTensor[:-2,0].view(-1, 1).detach().numpy()\n",
    "        OriginalMarginal2 = self.normalizedDataASTensor[:-2,1].view(-1, 1).detach().numpy()\n",
    "\n",
    "        df_returnSpace = pd.DataFrame({\n",
    "            \"X1\": OriginalMarginal1.flatten(),  \n",
    "            \"X2\": OriginalMarginal2.flatten()\n",
    "        })\n",
    "        sns.jointplot(\n",
    "            data=df_returnSpace, x=\"X1\", y=\"X2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "\n",
    "        # Plot Transformed data\n",
    "        TransformedMarginal1 = self.Marginal1(self.normalizedDataASTensor[:-2,0].view(-1, 1)).detach().numpy()\n",
    "        TransformedMarginal2 = self.Marginal2(self.normalizedDataASTensor[:-2,1].view(-1, 1)).detach().numpy()\n",
    "\n",
    "        df_probabilitySpace = pd.DataFrame({\n",
    "            \"U1\": TransformedMarginal1.flatten(),  # Flatten in case of (N,1) shape\n",
    "            \"U2\": TransformedMarginal2.flatten()\n",
    "        })\n",
    "        plt.suptitle(\"Original data in return space\", y=1.02);\n",
    "\n",
    "        # Plot the jointplot\n",
    "        sns.jointplot(\n",
    "            data=df_probabilitySpace, x=\"U1\", y=\"U2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "        plt.suptitle(\"Data when transformed to probability space\", y=1.02);\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _validateCopula(self):\n",
    "        # Create meshgrid\n",
    "        u1 = np.linspace(0, 1, 100)\n",
    "        u2 = np.linspace(0, 1, 100)\n",
    "        U1, U2 = np.meshgrid(u1, u2, indexing=\"ij\")\n",
    "        grid = np.column_stack((U1.ravel(), U2.ravel()))\n",
    "        grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "        # Get model predictions\n",
    "        self.Copula.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.Copula(grid_tensor)\n",
    "        Z = predictions.numpy().reshape(100, 100)  \n",
    "\n",
    "        # Plot\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(U1, U2, Z, cmap=\"viridis\")\n",
    "        ax.set_xlabel(\"u1\")\n",
    "        ax.set_ylabel(\"u2\")\n",
    "        ax.set_zlabel(\"C(u1, u2)\")\n",
    "        ax.set_title(\"Fitted Model Surface\")\n",
    "        ax.view_init(elev=15, azim=256)\n",
    "        plt.show()\n",
    "\n",
    "        print('Is the model 2-increasing?')\n",
    "        resultCol = Z[:, 1:] >= Z[:, :-1]\n",
    "        resultRow = Z[1:, :] >= Z[:-1, :]\n",
    "        has_false_col = np.any(resultCol == False)\n",
    "        has_false_row = np.any(resultRow == False)\n",
    "        print(\"Any False in resultCol?\", has_false_col)\n",
    "        print(\"Any False in resultRow?\", has_false_row)\n",
    "\n",
    "        if has_false_col or has_false_row:\n",
    "            print(\"Model is not 2-increasing\")\n",
    "            # Find indexes where the result is False\n",
    "            falseColIdx = np.where(resultCol == False)  # Indices in column-wise comparison\n",
    "            falseRowIdx = np.where(resultRow == False)  # Indices in row-wise comparison\n",
    "\n",
    "            # Adjust indices for full matrix (since resultCol and resultRow have reduced dimensions)\n",
    "            falseColIdx = (falseColIdx[0], falseColIdx[1] + 1)  # Shift column indices to match original Z\n",
    "            falseRowIdx = (falseRowIdx[0] + 1, falseRowIdx[1])  # Shift row indices to match original Z\n",
    "\n",
    "            # Combine results if needed\n",
    "            falseIdx = list(zip(*falseColIdx)) + list(zip(*falseRowIdx))\n",
    "\n",
    "            print(\"Indices where False in column-wise comparison:\", list(zip(*falseColIdx)))\n",
    "            print(\"Indices where False in row-wise comparison:\", list(zip(*falseRowIdx)))\n",
    "            print(\"All False indices:\", falseIdx)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Generating datasets \n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "# corrMat = np.array([[1, 0.], [0., 1]])\n",
    "# A = np.linalg.cholesky(corrMat)\n",
    "# Z = np.random.standard_normal((2000, 2))\n",
    "# X = (A @ Z.T).T\n",
    "\n",
    "\n",
    "# NC = NeuralCopula(X)\n",
    "# NC.normalizeData()\n",
    "# NC.fitModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a243289",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodTester():\n",
    "    def __init__(self, data, grid, runs = 1 ):\n",
    "        self.data = data\n",
    "        self.grid = grid\n",
    "        self.runs = runs\n",
    "        self.resultDict = {}\n",
    "\n",
    "    def runTest(self):\n",
    "        \n",
    "        for params in ParameterGrid(self.grid):\n",
    "            print(f\"Running test with options: {params}\")\n",
    "            ## Regular training\n",
    "            print('Regular training')\n",
    "            self._trainRegular(params)\n",
    "            print('----------------------------------------------------------------------------')\n",
    "            ## Pretraining scheme 1\n",
    "            print('Pretraining scheme 1')\n",
    "            self._trainPretrainingScheme1(params)\n",
    "            print('----------------------------------------------------------------------------')\n",
    "            # ## Pretraining scheme 2\n",
    "            print('Pretraining scheme 2')\n",
    "            self._trainPretrainingScheme2(params, method='pretrain2')\n",
    "            print('----------------------------------------------------------------------------')\n",
    "            # ## Pretraining scheme 3\n",
    "            print('Pretraining scheme 3')\n",
    "            self._trainPretrainingScheme2(params, method='pretrain3')\n",
    "            print('----------------------------------------------------------------------------')\n",
    "\n",
    "    def _trainRegular(self, params):\n",
    "        testerList = []\n",
    "        for run in range(self.runs):\n",
    "            ## Set random seed for reproducibility (maybe)\n",
    "            print(f\"Run {run+1} of {self.runs}\")\n",
    "            tester = ParameterTester(self.data, params)\n",
    "            tester.runTest()\n",
    "            testerList.append(tester)\n",
    "\n",
    "        ## Inspect model results and choose best\n",
    "        testerLosses = [tester.finalLoss.item()  for tester in testerList]   \n",
    "        #testerConstraintLosses = [tester.L2 + tester.L3 + tester.L4 + tester.L5 for tester in testerList]\n",
    "        \n",
    "        ## store model with smallest loss \n",
    "        bestTester = testerList[np.argmin(testerLosses)]## [testerConstraintLosses < 0.01]\n",
    "        key = ('Regular', frozenset(params.items()))\n",
    "        self.resultDict[key] = bestTester\n",
    "        \n",
    "    def _trainPretrainingScheme1(self, params): ## just regular training \n",
    "\n",
    "        ## Pretraining data \n",
    "        X = np.random.randn(1000, 2)\n",
    "\n",
    "        testerList = []\n",
    "        for run in range(self.runs):\n",
    "            print(f\"Run {run+1} of {self.runs}\")\n",
    "            print('----------------------------------------------------------------------------')\n",
    "\n",
    "            pretrainer = ParameterTester(X, params)\n",
    "            pretrainer.runTest()  \n",
    "            print('Pretraining done')\n",
    "            weights = pretrainer.finalWeights\n",
    "            tester = ParameterTester(self.data, params, initialWeights=weights)\n",
    "            tester.runTest()\n",
    "            print('Regular training done')\n",
    "            testerList.append(tester)\n",
    "\n",
    "        ## Inspect model results and choose best\n",
    "        testerLosses = [tester.finalLoss.item()  for tester in testerList]   \n",
    "        #testerConstraintLosses = [tester.L2 + tester.L3 + tester.L4 + tester.L5 for tester in testerList]\n",
    "        \n",
    "        ## store model with smallest loss \n",
    "        bestTester = testerList[np.argmin(testerLosses)]\n",
    "        key = ('Pretrain1', frozenset(params.items()))\n",
    "        self.resultDict[key] = bestTester\n",
    "        \n",
    "\n",
    "    def _trainPretrainingScheme2(self, params, method): ## training without L1 or MSE loss\n",
    "\n",
    "        ## Pretraining data \n",
    "        X = np.random.randn(1000, 2)\n",
    "\n",
    "        testerList = []\n",
    "        for run in range(self.runs):\n",
    "            print(f\"Run {run+1} of {self.runs}\")\n",
    "            print('----------------------------------------------------------------------------')\n",
    "            pretrainer = ParameterTester(X, params)\n",
    "            pretrainer.runTest(method=method)\n",
    "            print('Pretraining done')\n",
    "            \n",
    "            weights = pretrainer.finalWeights\n",
    "            tester = ParameterTester(self.data, params, initialWeights=weights)\n",
    "            tester.runTest()\n",
    "            print('Regular training done')\n",
    "            testerList.append(tester)\n",
    "\n",
    "        ## Inspect model results and choose best\n",
    "        testerLosses = [tester.finalLoss.item()  for tester in testerList]   \n",
    "        #testerConstraintLosses = [tester.L2 + tester.L3 + tester.L4 + tester.L5 for tester in testerList]\n",
    "        \n",
    "        ## store model with smallest loss \n",
    "        bestTester = testerList[np.argmin(testerLosses)]\n",
    "        MethodName = 'Pretrain2' if method == 'pretrain2' else 'Pretrain3'\n",
    "        key = (MethodName, frozenset(params.items()))\n",
    "        self.resultDict[key] = bestTester\n",
    "\n",
    "\n",
    "class ParameterTester():\n",
    "    def __init__(self, data, parameters, initialWeights = None):\n",
    "        #self.data = data\n",
    "        self.NC = NeuralCopula(data, parameters)\n",
    "        self.NC.SetInitialWeights = initialWeights\n",
    "\n",
    "        self.NC.normalizeData()\n",
    "        self.initialWeights = None\n",
    "        self.finalWeights = None\n",
    "        self.finalLoss = None\n",
    "        self.L1 = None\n",
    "        self.L2 = None \n",
    "        self.L3 = None\n",
    "        self.L4 = None\n",
    "        self.L5 = None\n",
    "        self.trainingTime = None\n",
    "        \n",
    "    def runTest(self, method = 'regular'):\n",
    "        if method == 'regular':\n",
    "            self.NC.fitModel()\n",
    "        elif method == 'pretrain2':\n",
    "            self.NC.fitModel(method = method)\n",
    "        elif method == 'pretrain3':\n",
    "            self.NC.fitModel(method = method)\n",
    "\n",
    "        self.copulaTrainingTime = self.NC.copulaTrainingTime\n",
    "        self.finalLoss, self.L1, self.L2, self.L3, self.L4, self.L5 = self.NC.Copula.Copula_loss_function(self.NC.normalizedDataAsTensor[:-2])\n",
    "        self.initialWeights = self.NC.initialCopulaWeights\n",
    "        self.finalWeights = self.NC.Copula.state_dict()\n",
    "        self.trainingTime = self.NC.copulaTrainingTime\n",
    "        \n",
    "\n",
    "class DatasetTester():\n",
    "    def __init__(self, datasets, grid, runs = 1 ):\n",
    "        self.datasets = datasets\n",
    "        self.grid = grid\n",
    "        self.runs = runs\n",
    "        self.resultDict = {}\n",
    "        self.isTrained = False\n",
    "        self.LossDict = {}\n",
    "\n",
    "    def runTest(self):\n",
    "        for name, data in self.datasets.items():\n",
    "            print(f'Running test with dataset: {name}')\n",
    "            Mtester = MethodTester(data=data, grid = self.grid, runs = 1)\n",
    "            Mtester.runTest()\n",
    "            self.resultDict[name] = Mtester\n",
    "        \n",
    "        self.isTrained = True\n",
    "        pass\n",
    "\n",
    "    def evaluateResults(self):\n",
    "        DatasetsKeys = list(self.resultDict.keys())\n",
    "        ModelKeys = list(self.resultDict[DatasetsKeys[0]].resultDict.keys())\n",
    "\n",
    "        for modelKey in ModelKeys:\n",
    "            Loss = 0\n",
    "            trainingTime = 0\n",
    "            constraintLoss = 0\n",
    "            for dataKey in DatasetsKeys:\n",
    "                Loss += self.resultDict[dataKey].resultDict[modelKey].finalLoss \n",
    "                trainingTime += self.resultDict[dataKey].resultDict[modelKey].copulaTrainingTime\n",
    "                constraintLoss += self.resultDict[dataKey].resultDict[modelKey].L2 + self.resultDict[dataKey].resultDict[modelKey].L3 + self.resultDict[dataKey].resultDict[modelKey].L4 + self.resultDict[dataKey].resultDict[modelKey].L5\n",
    "            print('----------------------------------------------------------------------------')\n",
    "            print(f'Model: {modelKey} has total loss: {Loss}')\n",
    "            print(f'Model: {modelKey} has average training time: {trainingTime/len(DatasetsKeys)}')\n",
    "            print(f'Model: {modelKey} has constraint loss: {constraintLoss}')\n",
    "            self.LossDict[modelKey] = Loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f662ed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with options: {'batch_size': 512, 'boundary_points': 100, 'epochs': 1500, 'lr': 0.001, 'num_layers': 3, 'num_neurons': 10, 'scheduler': None, 'solver': 'adam', 'uniform_points': 15}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 19.613964080810547, Constraint losses: L1: 17.613677978515625, L2: 0.0001217599055962637, L3: 1.0000817775726318, L4: 1.0000817775726318\n",
      "Epoch 500, Loss: -1.0771613121032715, Constraint losses: L1: -1.0917326211929321, L2: 0.0, L3: 0.00733494758605957, L4: 0.007236383855342865\n",
      "Epoch 1000, Loss: -1.1117583513259888, Constraint losses: L1: -1.1155465841293335, L2: 0.0, L3: 0.0019439458847045898, L4: 0.0018443347653374076\n",
      "Epoch 1500, Loss: -1.115680456161499, Constraint losses: L1: -1.1175992488861084, L2: 0.0, L3: 0.0010094046592712402, L4: 0.0009093715343624353\n",
      "Epoch 2000, Loss: -1.1174869537353516, Constraint losses: L1: -1.1186712980270386, L2: 0.0, L3: 0.0006421804428100586, L4: 0.0005422066897153854\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 10.292905807495117, Constraint losses: L1: 8.293357849121094, L2: 5.243354621597973e-07, L3: 0.9997734427452087, L4: 0.9997735023498535\n",
      "Epoch 500, Loss: -1.0441557168960571, Constraint losses: L1: -1.0479580163955688, L2: 0.0, L3: 0.001950979232788086, L4: 0.0018513209652155638\n",
      "Epoch 1000, Loss: -1.0521368980407715, Constraint losses: L1: -1.0534600019454956, L2: 0.0, L3: 0.0007115006446838379, L4: 0.0006115422584116459\n",
      "Epoch 1500, Loss: -1.0537898540496826, Constraint losses: L1: -1.0545744895935059, L2: 0.0, L3: 0.00044226646423339844, L4: 0.0003423528396524489\n",
      "Epoch 2000, Loss: -1.0547949075698853, Constraint losses: L1: -1.0553292036056519, L2: 0.0, L3: 0.0003170967102050781, L4: 0.00021722132805734873\n",
      "Training copula model\n",
      "Epoch 0, Loss: 210.16738891601562, Losses: L1: 18.120609283447266, L2: 0.00013669613690581173, L3: 1.0001364946365356, L4: 134.60946655273438, L5: 56.437042236328125\n",
      "Epoch 500, Loss: 69.60150146484375, Losses: L1: 0.896120011806488, L2: 0.0, L3: 0.6430186033248901, L4: 11.403762817382812, L5: 56.65860366821289\n",
      "Epoch 1000, Loss: 64.85347747802734, Losses: L1: 1.2854660749435425, L2: 0.0, L3: 0.6442980766296387, L4: 6.25791072845459, L5: 56.66580581665039\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 1\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 13.083486557006836, Constraint losses: L1: 11.083507537841797, L2: 7.41041458240943e-06, L3: 0.9999859929084778, L4: 0.9999860525131226\n",
      "Epoch 500, Loss: -1.1529525518417358, Constraint losses: L1: -1.2341777086257935, L2: 0.0, L3: 0.04066026210784912, L4: 0.040564920753240585\n",
      "Epoch 1000, Loss: -1.2624940872192383, Constraint losses: L1: -1.287814974784851, L2: 0.0, L3: 0.012709736824035645, L4: 0.012611199170351028\n",
      "Epoch 1500, Loss: -1.2873713970184326, Constraint losses: L1: -1.2997020483016968, L2: 0.0, L3: 0.006214916706085205, L4: 0.00611563865095377\n",
      "Epoch 2000, Loss: -1.2954245805740356, Constraint losses: L1: -1.3027654886245728, L2: 0.0, L3: 0.0037202835083007812, L4: 0.0036206250078976154\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.020974159240723, Constraint losses: L1: 6.025849342346191, L2: 0.0, L3: 0.9975626468658447, L4: 0.9975626468658447\n",
      "Epoch 500, Loss: -1.2162163257598877, Constraint losses: L1: -1.2182955741882324, L2: 0.0, L3: 0.0010895729064941406, L4: 0.000989629770629108\n",
      "Epoch 1000, Loss: -1.2188526391983032, Constraint losses: L1: -1.2197089195251465, L2: 0.0, L3: 0.0004781484603881836, L4: 0.00037811591755598783\n",
      "Epoch 1500, Loss: -1.2196444272994995, Constraint losses: L1: -1.220173954963684, L2: 0.0, L3: 0.0003147721290588379, L4: 0.00021468382328748703\n",
      "Epoch 2000, Loss: -1.2234625816345215, Constraint losses: L1: -1.223936915397644, L2: 0.0, L3: 0.00028711557388305664, L4: 0.00018714879115577787\n",
      "Training copula model\n",
      "Epoch 0, Loss: 204.7647705078125, Losses: L1: 5.583226680755615, L2: 0.0, L3: 0.9997530579566956, L4: 143.58050537109375, L5: 54.601280212402344\n",
      "Epoch 500, Loss: 165.4462432861328, Losses: L1: 7.128272533416748, L2: 0.0, L3: 0.9999180436134338, L4: 102.31633758544922, L5: 55.001712799072266\n",
      "Epoch 1000, Loss: 165.02322387695312, Losses: L1: 6.907539367675781, L2: 0.0, L3: 0.9999262094497681, L4: 102.09803771972656, L5: 55.01771545410156\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.425704956054688, Constraint losses: L1: 18.42068099975586, L2: 0.0016748302150517702, L3: 1.001674771308899, L4: 1.0016748905181885\n",
      "Epoch 500, Loss: -1.114884853363037, Constraint losses: L1: -1.1166317462921143, L2: 0.0, L3: 0.0009233355522155762, L4: 0.0008234630804508924\n",
      "Epoch 1000, Loss: -1.1188304424285889, Constraint losses: L1: -1.119442105293274, L2: 0.0, L3: 0.00035578012466430664, L4: 0.00025580747751519084\n",
      "Epoch 1500, Loss: -1.1203402280807495, Constraint losses: L1: -1.1207233667373657, L2: 0.0, L3: 0.0002415180206298828, L4: 0.0001416386803612113\n",
      "Epoch 2000, Loss: -1.1211706399917603, Constraint losses: L1: -1.121473789215088, L2: 0.0, L3: 0.0002015829086303711, L4: 0.00010159429803024977\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 7.5238037109375, Constraint losses: L1: 5.531032562255859, L2: 0.0, L3: 0.9963854551315308, L4: 0.996385395526886\n",
      "Epoch 500, Loss: -1.0507923364639282, Constraint losses: L1: -1.0527304410934448, L2: 0.0, L3: 0.0010189414024353027, L4: 0.0009192381985485554\n",
      "Epoch 1000, Loss: -1.054526448249817, Constraint losses: L1: -1.0553735494613647, L2: 0.0, L3: 0.00047343969345092773, L4: 0.0003736831422429532\n",
      "Epoch 1500, Loss: -1.0554478168487549, Constraint losses: L1: -1.0559600591659546, L2: 0.0, L3: 0.00030612945556640625, L4: 0.00020607827173080295\n",
      "Epoch 2000, Loss: -1.0552046298980713, Constraint losses: L1: -1.0555452108383179, L2: 0.0, L3: 0.00022029876708984375, L4: 0.00012030042125843465\n",
      "Training copula model\n",
      "Epoch 0, Loss: 167.2916259765625, Losses: L1: 7.414892196655273, L2: 0.0, L3: 0.9999269843101501, L4: 102.04695129394531, L5: 56.829856872558594\n",
      "Epoch 500, Loss: 167.03863525390625, Losses: L1: 7.217364311218262, L2: 0.0, L3: 0.9999246001243591, L4: 101.98176574707031, L5: 56.83958435058594\n",
      "Epoch 1000, Loss: 166.99868774414062, Losses: L1: 7.183369159698486, L2: 0.0, L3: 0.9999233484268188, L4: 101.97415924072266, L5: 56.84123229980469\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 2\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.428857803344727, Constraint losses: L1: 18.42068099975586, L2: 0.0027264270465821028, L3: 1.002726435661316, L4: 1.002726435661316\n",
      "Epoch 500, Loss: -1.002374529838562, Constraint losses: L1: -1.0043286085128784, L2: 0.0, L3: 0.001026928424835205, L4: 0.0009271427989006042\n",
      "Epoch 1000, Loss: -1.005195140838623, Constraint losses: L1: -1.0059268474578857, L2: 0.0, L3: 0.000415802001953125, L4: 0.0003159260959364474\n",
      "Epoch 1500, Loss: -1.0061759948730469, Constraint losses: L1: -1.0066083669662476, L2: 0.0, L3: 0.00026613473892211914, L4: 0.00016618234803900123\n",
      "Epoch 2000, Loss: -1.0072307586669922, Constraint losses: L1: -1.0075761079788208, L2: 0.0, L3: 0.00022268295288085938, L4: 0.0001226310560014099\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.018497467041016, Constraint losses: L1: 6.023421287536621, L2: 0.0, L3: 0.9975380301475525, L4: 0.9975380897521973\n",
      "Epoch 500, Loss: -1.057328701019287, Constraint losses: L1: -1.0608447790145874, L2: 0.0, L3: 0.0018079280853271484, L4: 0.0017081181285902858\n",
      "Epoch 1000, Loss: -1.0635532140731812, Constraint losses: L1: -1.0647318363189697, L2: 0.0, L3: 0.0006392598152160645, L4: 0.000539400614798069\n",
      "Epoch 1500, Loss: -1.0654903650283813, Constraint losses: L1: -1.066184163093567, L2: 0.0, L3: 0.0003968477249145508, L4: 0.00029690327937714756\n",
      "Epoch 2000, Loss: -1.0664081573486328, Constraint losses: L1: -1.06690514087677, L2: 0.0, L3: 0.00029844045639038086, L4: 0.00019855864229612052\n",
      "Training copula model\n",
      "Epoch 0, Loss: 209.8589324951172, Losses: L1: 17.2030086517334, L2: 2.2728399926563725e-05, L3: 1.000022292137146, L4: 154.66574096679688, L5: 54.19314956665039\n",
      "Epoch 500, Loss: 66.72166442871094, Losses: L1: 1.0076121091842651, L2: 0.0, L3: 0.6343252658843994, L4: 11.566259384155273, L5: 54.52107620239258\n",
      "Epoch 1000, Loss: 62.61207962036133, Losses: L1: 1.383884072303772, L2: 0.0, L3: 0.6247610449790955, L4: 7.455524444580078, L5: 54.531795501708984\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.389496803283691, Constraint losses: L1: 6.3926849365234375, L2: 0.0, L3: 0.9984058737754822, L4: 0.998405933380127\n",
      "Epoch 500, Loss: -1.102489948272705, Constraint losses: L1: -1.1056662797927856, L2: 0.0, L3: 0.0016379952430725098, L4: 0.001538233831524849\n",
      "Epoch 1000, Loss: -1.118681788444519, Constraint losses: L1: -1.1198956966400146, L2: 0.0, L3: 0.0006569623947143555, L4: 0.0005569482455030084\n",
      "Epoch 1500, Loss: -1.1206210851669312, Constraint losses: L1: -1.121291160583496, L2: 0.0, L3: 0.00038498640060424805, L4: 0.00028499806649051607\n",
      "Epoch 2000, Loss: -1.122659683227539, Constraint losses: L1: -1.1230982542037964, L2: 0.0, L3: 0.00026923418045043945, L4: 0.0001693285012152046\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.424715042114258, Constraint losses: L1: 18.42068099975586, L2: 0.00134542107116431, L3: 1.0013453960418701, L4: 1.0013453960418701\n",
      "Epoch 500, Loss: -1.0495574474334717, Constraint losses: L1: -1.0522664785385132, L2: 0.0, L3: 0.0014044642448425293, L4: 0.0013045715168118477\n",
      "Epoch 1000, Loss: -1.0535829067230225, Constraint losses: L1: -1.054645299911499, L2: 0.0, L3: 0.0005811452865600586, L4: 0.00048121422878466547\n",
      "Epoch 1500, Loss: -1.055300235748291, Constraint losses: L1: -1.0559055805206299, L2: 0.0, L3: 0.00035262107849121094, L4: 0.0002526830357965082\n",
      "Epoch 2000, Loss: -1.0579854249954224, Constraint losses: L1: -1.058373212814331, L2: 0.0, L3: 0.00024384260177612305, L4: 0.00014391643344424665\n",
      "Training copula model\n",
      "Epoch 0, Loss: 61.85660171508789, Losses: L1: 1.9627095460891724, L2: 0.0, L3: 0.6178919672966003, L4: 2.6018409729003906, L5: 56.67416000366211\n",
      "Epoch 500, Loss: 61.252342224121094, Losses: L1: 1.354373574256897, L2: 0.0, L3: 0.6248133182525635, L4: 2.6026406288146973, L5: 56.67051315307617\n",
      "Epoch 1000, Loss: 60.76124954223633, Losses: L1: 1.0614013671875, L2: 0.0, L3: 0.631103515625, L4: 2.403090000152588, L5: 56.665653228759766\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 3\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.424571990966797, Constraint losses: L1: 18.42068099975586, L2: 0.0012970964889973402, L3: 1.001297116279602, L4: 1.001297116279602\n",
      "Epoch 500, Loss: -1.1050206422805786, Constraint losses: L1: -1.1070499420166016, L2: 0.0, L3: 0.0010644793510437012, L4: 0.0009647052502259612\n",
      "Epoch 1000, Loss: -1.10805344581604, Constraint losses: L1: -1.1087839603424072, L2: 0.0, L3: 0.0004152059555053711, L4: 0.0003152719291392714\n",
      "Epoch 1500, Loss: -1.1103931665420532, Constraint losses: L1: -1.1108096837997437, L2: 0.0, L3: 0.00025832653045654297, L4: 0.00015822869318071753\n",
      "Epoch 2000, Loss: -1.1052870750427246, Constraint losses: L1: -1.1056017875671387, L2: 0.0, L3: 0.0002073049545288086, L4: 0.00010735252726590261\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.42433738708496, Constraint losses: L1: 18.42068099975586, L2: 0.0012188812252134085, L3: 1.0012189149856567, L4: 1.0012187957763672\n",
      "Epoch 500, Loss: -1.117659568786621, Constraint losses: L1: -1.1196205615997314, L2: 0.0, L3: 0.0010303854942321777, L4: 0.0009305199491791427\n",
      "Epoch 1000, Loss: -1.1209949254989624, Constraint losses: L1: -1.1217464208602905, L2: 0.0, L3: 0.00042575597763061523, L4: 0.00032585032749921083\n",
      "Epoch 1500, Loss: -1.1219276189804077, Constraint losses: L1: -1.1223878860473633, L2: 0.0, L3: 0.00028008222579956055, L4: 0.00018014005036093295\n",
      "Epoch 2000, Loss: -1.122255563735962, Constraint losses: L1: -1.122584581375122, L2: 0.0, L3: 0.00021445751190185547, L4: 0.00011455050844233483\n",
      "Training copula model\n",
      "Epoch 0, Loss: 0.09021638333797455, Losses: L1: 14.410575866699219, L2: 5.463301204144955e-05, L3: 1.0000293254852295, L4: 153.495849609375, L5: 54.731204986572266\n",
      "Epoch 500, Loss: 0.04980329051613808, Losses: L1: 12.229631423950195, L2: 5.74833666178165e-06, L3: 0.9999415278434753, L4: 119.80848693847656, L5: 54.93169021606445\n",
      "Epoch 1000, Loss: 0.04979899525642395, Losses: L1: 11.589029312133789, L2: 3.1314264106185874e-06, L3: 0.9999741911888123, L4: 118.91302490234375, L5: 54.931907653808594\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.966954231262207, Constraint losses: L1: 6.968869209289551, L2: 0.0, L3: 0.9990425705909729, L4: 0.9990425705909729\n",
      "Epoch 500, Loss: -1.1128993034362793, Constraint losses: L1: -1.1161572933197021, L2: 0.0, L3: 0.0016788840293884277, L4: 0.0015790896723046899\n",
      "Epoch 1000, Loss: -1.1173239946365356, Constraint losses: L1: -1.118464708328247, L2: 0.0, L3: 0.0006203055381774902, L4: 0.0005203936016187072\n",
      "Epoch 1500, Loss: -1.1186778545379639, Constraint losses: L1: -1.1193325519561768, L2: 0.0, L3: 0.00037735700607299805, L4: 0.00027731049340218306\n",
      "Epoch 2000, Loss: -1.1195355653762817, Constraint losses: L1: -1.1200275421142578, L2: 0.0, L3: 0.00029599666595458984, L4: 0.00019592774333432317\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.42339324951172, Constraint losses: L1: 18.42068099975586, L2: 0.0009042398305609822, L3: 1.0009042024612427, L4: 1.0009043216705322\n",
      "Epoch 500, Loss: -1.0499670505523682, Constraint losses: L1: -1.052565097808838, L2: 0.0, L3: 0.001349031925201416, L4: 0.0012490879744291306\n",
      "Epoch 1000, Loss: -1.0541654825210571, Constraint losses: L1: -1.0552092790603638, L2: 0.0, L3: 0.0005718469619750977, L4: 0.00047192448982968926\n",
      "Epoch 1500, Loss: -1.056413173675537, Constraint losses: L1: -1.0570745468139648, L2: 0.0, L3: 0.00038063526153564453, L4: 0.00028079180628992617\n",
      "Epoch 2000, Loss: -1.0582093000411987, Constraint losses: L1: -1.0586403608322144, L2: 0.0, L3: 0.00026553869247436523, L4: 0.0001655578671488911\n",
      "Training copula model\n",
      "Epoch 0, Loss: 187.9222412109375, Losses: L1: 11.466655731201172, L2: 3.3006717785610817e-06, L3: 0.9999785423278809, L4: 118.90467834472656, L5: 56.55093002319336\n",
      "Epoch 500, Loss: 68.27433776855469, Losses: L1: 0.6087715029716492, L2: 0.0, L3: 0.6192134618759155, L4: 10.387592315673828, L5: 56.65876007080078\n",
      "Epoch 1000, Loss: 66.01206970214844, Losses: L1: 0.8103951215744019, L2: 0.0, L3: 0.6118810176849365, L4: 7.92337703704834, L5: 56.666412353515625\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Running test with options: {'batch_size': 512, 'boundary_points': 100, 'epochs': 1500, 'lr': 0.001, 'num_layers': 3, 'num_neurons': 10, 'scheduler': None, 'solver': 'adam', 'uniform_points': 15}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.802080154418945, Constraint losses: L1: 6.80401611328125, L2: 0.0, L3: 0.9990320801734924, L4: 0.9990320801734924\n",
      "Epoch 500, Loss: -1.1148372888565063, Constraint losses: L1: -1.117004156112671, L2: 0.0, L3: 0.0011333227157592773, L4: 0.001033496460877359\n",
      "Epoch 1000, Loss: -1.118190050125122, Constraint losses: L1: -1.1189748048782349, L2: 0.0, L3: 0.00044226646423339844, L4: 0.0003424336318857968\n",
      "Epoch 1500, Loss: -1.1199429035186768, Constraint losses: L1: -1.1203815937042236, L2: 0.0, L3: 0.0002694129943847656, L4: 0.0001692725927568972\n",
      "Epoch 2000, Loss: -1.1210256814956665, Constraint losses: L1: -1.1213397979736328, L2: 0.0, L3: 0.00020700693130493164, L4: 0.0001071256774594076\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 16.181846618652344, Constraint losses: L1: 14.181334495544434, L2: 0.000590965966694057, L3: 0.9999604225158691, L4: 0.9999604225158691\n",
      "Epoch 500, Loss: -1.0876898765563965, Constraint losses: L1: -1.1017727851867676, L2: 0.0, L3: 0.007090926170349121, L4: 0.00699202623218298\n",
      "Epoch 1000, Loss: -1.1094847917556763, Constraint losses: L1: -1.1137785911560059, L2: 0.0, L3: 0.002196788787841797, L4: 0.0020969840697944164\n",
      "Epoch 1500, Loss: -1.1147205829620361, Constraint losses: L1: -1.116891860961914, L2: 0.0, L3: 0.0011355280876159668, L4: 0.0010356956627219915\n",
      "Epoch 2000, Loss: -1.1166852712631226, Constraint losses: L1: -1.1180764436721802, L2: 0.0, L3: 0.0007455348968505859, L4: 0.0006456914125010371\n",
      "Training copula model\n",
      "Epoch 0, Loss: 248.955810546875, Losses: L1: 18.42068099975586, L2: 2.0845483959419653e-05, L3: 1.0000208616256714, L4: 176.25540161132812, L5: 53.279693603515625\n",
      "Epoch 500, Loss: 68.30467224121094, Losses: L1: 1.5353893041610718, L2: 0.0, L3: 0.6466999053955078, L4: 12.424711227416992, L5: 53.69786834716797\n",
      "Epoch 1000, Loss: 64.59159088134766, Losses: L1: 1.9419034719467163, L2: 0.0, L3: 0.6439917683601379, L4: 8.299697875976562, L5: 53.705997467041016\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 1\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.425230026245117, Constraint losses: L1: 18.42068099975586, L2: 0.0015155189903452992, L3: 1.001515507698059, L4: 1.001515507698059\n",
      "Epoch 500, Loss: -1.0702177286148071, Constraint losses: L1: -1.0721302032470703, L2: 0.0, L3: 0.0010062456130981445, L4: 0.0009062668541446328\n",
      "Epoch 1000, Loss: -1.0731489658355713, Constraint losses: L1: -1.0739551782608032, L2: 0.0, L3: 0.00045305490493774414, L4: 0.0003530444228090346\n",
      "Epoch 1500, Loss: -1.0744001865386963, Constraint losses: L1: -1.0748547315597534, L2: 0.0, L3: 0.0002772212028503418, L4: 0.00017734414723236114\n",
      "Epoch 2000, Loss: -1.0753064155578613, Constraint losses: L1: -1.0756289958953857, L2: 0.0, L3: 0.00021129846572875977, L4: 0.00011135709792142734\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.745445251464844, Constraint losses: L1: 6.748374938964844, L2: 2.6921883545583114e-05, L3: 0.9985218048095703, L4: 0.9985218048095703\n",
      "Epoch 500, Loss: -1.0508674383163452, Constraint losses: L1: -1.0531524419784546, L2: 0.0, L3: 0.0011923909187316895, L4: 0.0010925986571237445\n",
      "Epoch 1000, Loss: -1.0553232431411743, Constraint losses: L1: -1.0561139583587646, L2: 0.0, L3: 0.00044542551040649414, L4: 0.000345367647241801\n",
      "Epoch 1500, Loss: -1.0571953058242798, Constraint losses: L1: -1.05769681930542, L2: 0.0, L3: 0.0003007650375366211, L4: 0.0002007703296840191\n",
      "Epoch 2000, Loss: -1.058436393737793, Constraint losses: L1: -1.058848261833191, L2: 0.0, L3: 0.00025588274002075195, L4: 0.0001559371012263\n",
      "Training copula model\n",
      "Epoch 0, Loss: 220.22012329101562, Losses: L1: 17.715917587280273, L2: 3.943249339499744e-06, L3: 1.0000038146972656, L4: 145.78578186035156, L5: 55.718414306640625\n",
      "Epoch 500, Loss: 167.25294494628906, Losses: L1: 7.631900310516357, L2: 0.0, L3: 0.9999317526817322, L4: 102.4890365600586, L5: 56.132076263427734\n",
      "Epoch 1000, Loss: 166.7252655029297, Losses: L1: 7.4120941162109375, L2: 0.0, L3: 0.9999197721481323, L4: 102.15880584716797, L5: 56.154449462890625\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.303874015808105, Constraint losses: L1: 6.308250904083252, L2: 0.0, L3: 0.9978113174438477, L4: 0.9978114366531372\n",
      "Epoch 500, Loss: -1.1143522262573242, Constraint losses: L1: -1.1168254613876343, L2: 0.0, L3: 0.001286625862121582, L4: 0.0011865734122693539\n",
      "Epoch 1000, Loss: -1.1186723709106445, Constraint losses: L1: -1.1195213794708252, L2: 0.0, L3: 0.00047451257705688477, L4: 0.0003745508147403598\n",
      "Epoch 1500, Loss: -1.1200817823410034, Constraint losses: L1: -1.1205403804779053, L2: 0.0, L3: 0.00027930736541748047, L4: 0.00017926504369825125\n",
      "Epoch 2000, Loss: -1.1211057901382446, Constraint losses: L1: -1.1214215755462646, L2: 0.0, L3: 0.0002078413963317871, L4: 0.0001078858767868951\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.428789138793945, Constraint losses: L1: 18.42068099975586, L2: 0.0027019535191357136, L3: 1.002701997756958, L4: 1.002701997756958\n",
      "Epoch 500, Loss: -1.1125102043151855, Constraint losses: L1: -1.115243673324585, L2: 0.0, L3: 0.001416623592376709, L4: 0.0013168223667889833\n",
      "Epoch 1000, Loss: -1.1164631843566895, Constraint losses: L1: -1.117417812347412, L2: 0.0, L3: 0.0005272626876831055, L4: 0.0004273886443115771\n",
      "Epoch 1500, Loss: -1.113568902015686, Constraint losses: L1: -1.114155888557434, L2: 0.0, L3: 0.0003433823585510254, L4: 0.0002434946654830128\n",
      "Epoch 2000, Loss: -1.1194385290145874, Constraint losses: L1: -1.1198877096176147, L2: 0.0, L3: 0.0002745389938354492, L4: 0.00017460816889069974\n",
      "Training copula model\n",
      "Epoch 0, Loss: 164.0654296875, Losses: L1: 7.111538887023926, L2: 0.0, L3: 0.9999181032180786, L4: 102.08904266357422, L5: 53.86493682861328\n",
      "Epoch 500, Loss: 157.56768798828125, Losses: L1: 5.4229021072387695, L2: 0.03007286787033081, L3: 1.0299384593963623, L4: 97.38094329833984, L5: 53.703826904296875\n",
      "Epoch 1000, Loss: 70.49586486816406, Losses: L1: 1.2018842697143555, L2: 0.00023311690893024206, L3: 0.7897221446037292, L4: 14.866268157958984, L5: 53.637760162353516\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 2\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.42719841003418, Constraint losses: L1: 18.42068099975586, L2: 0.002172250999137759, L3: 1.0021722316741943, L4: 1.0021722316741943\n",
      "Epoch 500, Loss: -1.121493935585022, Constraint losses: L1: -1.123048186302185, L2: 0.0, L3: 0.0008271336555480957, L4: 0.0007270521018654108\n",
      "Epoch 1000, Loss: -1.1253032684326172, Constraint losses: L1: -1.1258996725082397, L2: 0.0, L3: 0.0003482699394226074, L4: 0.0002482518902979791\n",
      "Epoch 1500, Loss: -1.125968337059021, Constraint losses: L1: -1.1263713836669922, L2: 0.0, L3: 0.00025159120559692383, L4: 0.00015153310960158706\n",
      "Epoch 2000, Loss: -1.1267729997634888, Constraint losses: L1: -1.1270794868469238, L2: 0.0, L3: 0.00020325183868408203, L4: 0.00010325522453058511\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 7.959378242492676, Constraint losses: L1: 5.964390754699707, L2: 0.0, L3: 0.9974938631057739, L4: 0.9974938631057739\n",
      "Epoch 500, Loss: -1.237226963043213, Constraint losses: L1: -1.2402105331420898, L2: 0.0, L3: 0.0015416741371154785, L4: 0.0014419020153582096\n",
      "Epoch 1000, Loss: -1.2426607608795166, Constraint losses: L1: -1.243888258934021, L2: 0.0, L3: 0.0006636977195739746, L4: 0.0005638530710712075\n",
      "Epoch 1500, Loss: -1.2437779903411865, Constraint losses: L1: -1.244523048400879, L2: 0.0, L3: 0.00042253732681274414, L4: 0.00032255923724733293\n",
      "Epoch 2000, Loss: -1.2454015016555786, Constraint losses: L1: -1.245919942855835, L2: 0.0, L3: 0.00030928850173950195, L4: 0.00020921911345794797\n",
      "Training copula model\n",
      "Epoch 0, Loss: 205.49966430664062, Losses: L1: 7.384146213531494, L2: 0.0, L3: 0.9999461770057678, L4: 151.54141235351562, L5: 52.958309173583984\n",
      "Epoch 500, Loss: 65.28895568847656, Losses: L1: 0.6572871208190918, L2: 0.0, L3: 0.660192608833313, L4: 11.372137069702148, L5: 53.256622314453125\n",
      "Epoch 1000, Loss: 60.78733444213867, Losses: L1: 1.2895233631134033, L2: 0.0, L3: 0.6348855495452881, L4: 6.88426399230957, L5: 53.268184661865234\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 15.304365158081055, Constraint losses: L1: 13.304116249084473, L2: 0.00019540206994861364, L3: 1.0000271797180176, L4: 1.0000271797180176\n",
      "Epoch 500, Loss: -1.0930023193359375, Constraint losses: L1: -1.103174090385437, L2: 0.0, L3: 0.005135536193847656, L4: 0.005036286078393459\n",
      "Epoch 1000, Loss: -1.1115106344223022, Constraint losses: L1: -1.1143035888671875, L2: 0.0, L3: 0.0014463067054748535, L4: 0.0013466785894706845\n",
      "Epoch 1500, Loss: -1.1164098978042603, Constraint losses: L1: -1.1176960468292236, L2: 0.0, L3: 0.0006929636001586914, L4: 0.0005931734922342002\n",
      "Epoch 2000, Loss: -1.118052363395691, Constraint losses: L1: -1.11881422996521, L2: 0.0, L3: 0.0004309415817260742, L4: 0.00033097920822910964\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.429012298583984, Constraint losses: L1: 18.42068099975586, L2: 0.002777197165414691, L3: 1.0027772188186646, L4: 1.002777099609375\n",
      "Epoch 500, Loss: -1.1072734594345093, Constraint losses: L1: -1.1096118688583374, L2: 0.0, L3: 0.001219034194946289, L4: 0.0011193626560270786\n",
      "Epoch 1000, Loss: -1.1161344051361084, Constraint losses: L1: -1.1167817115783691, L2: 0.0, L3: 0.00037354230880737305, L4: 0.00027365138521417975\n",
      "Epoch 1500, Loss: -1.1167510747909546, Constraint losses: L1: -1.1171278953552246, L2: 0.0, L3: 0.0002383589744567871, L4: 0.0001384076167596504\n",
      "Epoch 2000, Loss: -1.1184730529785156, Constraint losses: L1: -1.1187498569488525, L2: 0.0, L3: 0.00018835067749023438, L4: 8.840586087899283e-05\n",
      "Training copula model\n",
      "Epoch 0, Loss: 61.50766372680664, Losses: L1: 4.733222961425781, L2: 0.0, L3: 0.5404080152511597, L4: 2.520477533340454, L5: 53.71355438232422\n",
      "Epoch 500, Loss: 60.192840576171875, Losses: L1: 3.1009461879730225, L2: 0.0, L3: 0.5770515203475952, L4: 2.805229663848877, L5: 53.70961380004883\n",
      "Epoch 1000, Loss: 59.276004791259766, Losses: L1: 2.865549087524414, L2: 0.0, L3: 0.578560471534729, L4: 2.122777223587036, L5: 53.7091178894043\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 3\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.42635154724121, Constraint losses: L1: 18.42068099975586, L2: 0.0018894943641498685, L3: 1.0018894672393799, L4: 1.0018894672393799\n",
      "Epoch 500, Loss: -1.0521949529647827, Constraint losses: L1: -1.0538800954818726, L2: 0.0, L3: 0.0008924603462219238, L4: 0.000792584614828229\n",
      "Epoch 1000, Loss: -1.0555483102798462, Constraint losses: L1: -1.056215763092041, L2: 0.0, L3: 0.00038367509841918945, L4: 0.00028389025828801095\n",
      "Epoch 1500, Loss: -1.0517494678497314, Constraint losses: L1: -1.0522152185440063, L2: 0.0, L3: 0.0002828240394592285, L4: 0.00018284667748957872\n",
      "Epoch 2000, Loss: -1.0629971027374268, Constraint losses: L1: -1.0634584426879883, L2: 0.0, L3: 0.00028055906295776367, L4: 0.00018068705685436726\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.43770408630371, Constraint losses: L1: 18.42068099975586, L2: 0.005673736799508333, L3: 1.0056737661361694, L4: 1.0056737661361694\n",
      "Epoch 500, Loss: -1.2311254739761353, Constraint losses: L1: -1.2338343858718872, L2: 0.0, L3: 0.0014042854309082031, L4: 0.0013045985251665115\n",
      "Epoch 1000, Loss: -1.2347095012664795, Constraint losses: L1: -1.2357345819473267, L2: 0.0, L3: 0.0005624890327453613, L4: 0.00046247930731624365\n",
      "Epoch 1500, Loss: -1.2356594800949097, Constraint losses: L1: -1.2362895011901855, L2: 0.0, L3: 0.0003649592399597168, L4: 0.00026499020168557763\n",
      "Epoch 2000, Loss: -1.2369526624679565, Constraint losses: L1: -1.237433671951294, L2: 0.0, L3: 0.0002905130386352539, L4: 0.00019047787645831704\n",
      "Training copula model\n",
      "Epoch 0, Loss: 0.17569512128829956, Losses: L1: 13.000049591064453, L2: 1.4270075553213246e-05, L3: 1.000002384185791, L4: 172.90383911132812, L5: 58.077369689941406\n",
      "Epoch 500, Loss: 0.010850364342331886, Losses: L1: 18.42068099975586, L2: 0.00025511812418699265, L3: 1.0002551078796387, L4: 108.91496276855469, L5: 58.48323440551758\n",
      "Epoch 1000, Loss: 0.010826891288161278, Losses: L1: 18.397598266601562, L2: 0.00011543228902155533, L3: 1.0001153945922852, L4: 108.91793823242188, L5: 58.483158111572266\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 12.450596809387207, Constraint losses: L1: 10.450867652893066, L2: 0.00024063537421170622, L3: 0.9997439980506897, L4: 0.9997440576553345\n",
      "Epoch 500, Loss: -1.067466378211975, Constraint losses: L1: -1.0864816904067993, L2: 0.0, L3: 0.009556829929351807, L4: 0.009458412416279316\n",
      "Epoch 1000, Loss: -1.1096643209457397, Constraint losses: L1: -1.1144386529922485, L2: 0.0, L3: 0.002436995506286621, L4: 0.002337368205189705\n",
      "Epoch 1500, Loss: -1.1144661903381348, Constraint losses: L1: -1.1168498992919922, L2: 0.0, L3: 0.0012418031692504883, L4: 0.001141906250268221\n",
      "Epoch 2000, Loss: -1.1162279844284058, Constraint losses: L1: -1.1176656484603882, L2: 0.0, L3: 0.0007687211036682129, L4: 0.0006688495632261038\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.156583786010742, Constraint losses: L1: 18.15536880493164, L2: 0.0005397126078605652, L3: 1.0003374814987183, L4: 1.0003376007080078\n",
      "Epoch 500, Loss: -1.1086928844451904, Constraint losses: L1: -1.1126471757888794, L2: 0.0, L3: 0.002026975154876709, L4: 0.0019274085061624646\n",
      "Epoch 1000, Loss: -1.1144587993621826, Constraint losses: L1: -1.1156363487243652, L2: 0.0, L3: 0.0006386637687683105, L4: 0.0005388322751969099\n",
      "Epoch 1500, Loss: -1.1166983842849731, Constraint losses: L1: -1.1172972917556763, L2: 0.0, L3: 0.00034940242767333984, L4: 0.000249501783400774\n",
      "Epoch 2000, Loss: -1.1084465980529785, Constraint losses: L1: -1.108834147453308, L2: 0.0, L3: 0.00024372339248657227, L4: 0.00014380243374034762\n",
      "Training copula model\n",
      "Epoch 0, Loss: 182.0075225830078, Losses: L1: 18.42068099975586, L2: 4.535792686510831e-05, L3: 1.0000450611114502, L4: 108.8916244506836, L5: 53.695125579833984\n",
      "Epoch 500, Loss: 67.38540649414062, Losses: L1: 2.9013442993164062, L2: 0.0, L3: 0.6362419724464417, L4: 10.147085189819336, L5: 53.70073318481445\n",
      "Epoch 1000, Loss: 65.17606353759766, Losses: L1: 3.4570095539093018, L2: 0.0, L3: 0.6361441612243652, L4: 7.378354549407959, L5: 53.70455551147461\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Running test with options: {'batch_size': 512, 'boundary_points': 100, 'epochs': 1500, 'lr': 0.001, 'num_layers': 3, 'num_neurons': 10, 'scheduler': None, 'solver': 'adam', 'uniform_points': 15}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.43865966796875, Constraint losses: L1: 18.42068099975586, L2: 0.005992727354168892, L3: 1.0059927701950073, L4: 1.0059927701950073\n",
      "Epoch 500, Loss: -1.1154409646987915, Constraint losses: L1: -1.1172195672988892, L2: 0.0, L3: 0.0009392499923706055, L4: 0.0008393657626584172\n",
      "Epoch 1000, Loss: -1.1190513372421265, Constraint losses: L1: -1.1197112798690796, L2: 0.0, L3: 0.00037986040115356445, L4: 0.00027996819699183106\n",
      "Epoch 1500, Loss: -1.1204391717910767, Constraint losses: L1: -1.120897650718689, L2: 0.0, L3: 0.0002791881561279297, L4: 0.00017928807938005775\n",
      "Epoch 2000, Loss: -1.1209805011749268, Constraint losses: L1: -1.121350884437561, L2: 0.0, L3: 0.00023514032363891602, L4: 0.0001352167164441198\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.4278564453125, Constraint losses: L1: 18.42068099975586, L2: 0.002391025424003601, L3: 1.00239098072052, L4: 1.0023910999298096\n",
      "Epoch 500, Loss: -1.1174163818359375, Constraint losses: L1: -1.1192504167556763, L2: 0.0, L3: 0.0009669661521911621, L4: 0.0008671313989907503\n",
      "Epoch 1000, Loss: -1.1206872463226318, Constraint losses: L1: -1.121423363685608, L2: 0.0, L3: 0.00041806697845458984, L4: 0.0003181070787832141\n",
      "Epoch 1500, Loss: -1.1218287944793701, Constraint losses: L1: -1.1222878694534302, L2: 0.0, L3: 0.00027948617935180664, L4: 0.00017951785412151366\n",
      "Epoch 2000, Loss: -1.1226986646652222, Constraint losses: L1: -1.123023509979248, L2: 0.0, L3: 0.0002124309539794922, L4: 0.00011238825391046703\n",
      "Training copula model\n",
      "Epoch 0, Loss: 224.5194091796875, Losses: L1: 18.42068099975586, L2: 6.161529313430947e-07, L3: 1.0000005960464478, L4: 150.80233764648438, L5: 54.29638671875\n",
      "Epoch 500, Loss: 166.1583251953125, Losses: L1: 7.506038188934326, L2: 0.0, L3: 0.9999403357505798, L4: 102.93395233154297, L5: 54.7183837890625\n",
      "Epoch 1000, Loss: 165.28970336914062, Losses: L1: 7.252375602722168, L2: 0.0, L3: 0.9999240636825562, L4: 102.28761291503906, L5: 54.749794006347656\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 1\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 9.509336471557617, Constraint losses: L1: 7.510459899902344, L2: 0.0, L3: 0.9994385242462158, L4: 0.9994385242462158\n",
      "Epoch 500, Loss: -1.086377501487732, Constraint losses: L1: -1.0898288488388062, L2: 0.0, L3: 0.0017755627632141113, L4: 0.0016757489647716284\n",
      "Epoch 1000, Loss: -1.0905637741088867, Constraint losses: L1: -1.0917644500732422, L2: 0.0, L3: 0.0006502866744995117, L4: 0.0005504232831299305\n",
      "Epoch 1500, Loss: -1.0896259546279907, Constraint losses: L1: -1.0902705192565918, L2: 0.0, L3: 0.00037229061126708984, L4: 0.0002722947101574391\n",
      "Epoch 2000, Loss: -1.0934969186782837, Constraint losses: L1: -1.0939412117004395, L2: 0.0, L3: 0.0002721548080444336, L4: 0.00017215529805980623\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.424137115478516, Constraint losses: L1: 18.42068099975586, L2: 0.001151183620095253, L3: 1.001151204109192, L4: 1.001151204109192\n",
      "Epoch 500, Loss: -1.1262874603271484, Constraint losses: L1: -1.128238320350647, L2: 0.0, L3: 0.001025378704071045, L4: 0.000925563566852361\n",
      "Epoch 1000, Loss: -1.128690481185913, Constraint losses: L1: -1.1294407844543457, L2: 0.0, L3: 0.00042510032653808594, L4: 0.00032519863452762365\n",
      "Epoch 1500, Loss: -1.1291942596435547, Constraint losses: L1: -1.1296534538269043, L2: 0.0, L3: 0.0002796053886413574, L4: 0.00017950066830962896\n",
      "Epoch 2000, Loss: -1.1294257640838623, Constraint losses: L1: -1.129754662513733, L2: 0.0, L3: 0.00021445751190185547, L4: 0.0001144490743172355\n",
      "Training copula model\n",
      "Epoch 0, Loss: 212.74099731445312, Losses: L1: 8.185053825378418, L2: 9.0661899321276e-07, L3: 0.9999441504478455, L4: 148.34793090820312, L5: 55.20806121826172\n",
      "Epoch 500, Loss: 166.32861328125, Losses: L1: 7.581294536590576, L2: 0.0, L3: 0.9999274611473083, L4: 102.10360717773438, L5: 55.64379119873047\n",
      "Epoch 1000, Loss: 166.0305938720703, Losses: L1: 7.3319501876831055, L2: 0.0, L3: 0.9999203085899353, L4: 102.04271697998047, L5: 55.656009674072266\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 8.92476749420166, Constraint losses: L1: 6.9265851974487305, L2: 0.0, L3: 0.9990913271903992, L4: 0.9990913271903992\n",
      "Epoch 500, Loss: -1.1094914674758911, Constraint losses: L1: -1.1149816513061523, L2: 0.0, L3: 0.002794921398162842, L4: 0.0026951543986797333\n",
      "Epoch 1000, Loss: -1.1170148849487305, Constraint losses: L1: -1.1189208030700684, L2: 0.0, L3: 0.0010029077529907227, L4: 0.0009029541979543865\n",
      "Epoch 1500, Loss: -1.1194860935211182, Constraint losses: L1: -1.1205570697784424, L2: 0.0, L3: 0.0005854368209838867, L4: 0.00048549496568739414\n",
      "Epoch 2000, Loss: -1.1205874681472778, Constraint losses: L1: -1.1213222742080688, L2: 0.0, L3: 0.00041729211807250977, L4: 0.0003174478770233691\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.4454345703125, Constraint losses: L1: 18.42068099975586, L2: 0.008250242099165916, L3: 1.0082502365112305, L4: 1.00825035572052\n",
      "Epoch 500, Loss: -1.1154226064682007, Constraint losses: L1: -1.1181399822235107, L2: 0.0, L3: 0.0014085769653320312, L4: 0.0013087466359138489\n",
      "Epoch 1000, Loss: -1.119236946105957, Constraint losses: L1: -1.120211124420166, L2: 0.0, L3: 0.0005370974540710449, L4: 0.0004370695096440613\n",
      "Epoch 1500, Loss: -1.1207984685897827, Constraint losses: L1: -1.1213672161102295, L2: 0.0, L3: 0.0003343820571899414, L4: 0.00023441328085027635\n",
      "Epoch 2000, Loss: -1.121929407119751, Constraint losses: L1: -1.122344970703125, L2: 0.0, L3: 0.00025779008865356445, L4: 0.00015786287258379161\n",
      "Training copula model\n",
      "Epoch 0, Loss: 165.0842742919922, Losses: L1: 7.293399333953857, L2: 0.0, L3: 0.9999213218688965, L4: 102.02043914794922, L5: 54.77051544189453\n",
      "Epoch 500, Loss: 83.48068237304688, Losses: L1: 3.8326680660247803, L2: 0.0, L3: 0.6067714691162109, L4: 24.441749572753906, L5: 54.59949493408203\n",
      "Epoch 1000, Loss: 74.38212585449219, Losses: L1: 2.379589080810547, L2: 0.0, L3: 0.588290274143219, L4: 16.818395614624023, L5: 54.59585189819336\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 2\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.431278228759766, Constraint losses: L1: 18.42068099975586, L2: 0.0035331512335687876, L3: 1.003533124923706, L4: 1.003533124923706\n",
      "Epoch 500, Loss: -1.2140076160430908, Constraint losses: L1: -1.216602087020874, L2: 0.0, L3: 0.001347184181213379, L4: 0.0012472624657675624\n",
      "Epoch 1000, Loss: -1.2170073986053467, Constraint losses: L1: -1.2181216478347778, L2: 0.0, L3: 0.0006071329116821289, L4: 0.0005071362247690558\n",
      "Epoch 1500, Loss: -1.2188318967819214, Constraint losses: L1: -1.219560146331787, L2: 0.0, L3: 0.00041407346725463867, L4: 0.00031408661743626\n",
      "Epoch 2000, Loss: -1.2194135189056396, Constraint losses: L1: -1.2198843955993652, L2: 0.0, L3: 0.0002854466438293457, L4: 0.00018547906074672937\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 7.99300479888916, Constraint losses: L1: 5.997960090637207, L2: 0.0, L3: 0.9975221157073975, L4: 0.9975221753120422\n",
      "Epoch 500, Loss: -1.1642221212387085, Constraint losses: L1: -1.165729284286499, L2: 0.0, L3: 0.000803530216217041, L4: 0.0007036655442789197\n",
      "Epoch 1000, Loss: -1.1676433086395264, Constraint losses: L1: -1.16817045211792, L2: 0.0, L3: 0.0003134608268737793, L4: 0.0002135705726686865\n",
      "Epoch 1500, Loss: -1.1684410572052002, Constraint losses: L1: -1.1687724590301514, L2: 0.0, L3: 0.00021564960479736328, L4: 0.00011571817594813183\n",
      "Epoch 2000, Loss: -1.1686216592788696, Constraint losses: L1: -1.1688929796218872, L2: 0.0, L3: 0.0001856684684753418, L4: 8.568922930862755e-05\n",
      "Training copula model\n",
      "Epoch 0, Loss: 198.02291870117188, Losses: L1: 18.42068099975586, L2: 8.028130650927778e-06, L3: 1.0000079870224, L4: 145.5186767578125, L5: 51.50421905517578\n",
      "Epoch 500, Loss: 62.98750686645508, Losses: L1: 0.8646643161773682, L2: 0.0, L3: 0.6319354772567749, L4: 10.563400268554688, L5: 51.792171478271484\n",
      "Epoch 1000, Loss: 59.46793746948242, Losses: L1: 1.2831475734710693, L2: 0.0, L3: 0.6158833503723145, L4: 7.051095008850098, L5: 51.800960540771484\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 7.35695219039917, Constraint losses: L1: 5.365629196166992, L2: 0.0, L3: 0.9956614971160889, L4: 0.9956613779067993\n",
      "Epoch 500, Loss: -1.114358901977539, Constraint losses: L1: -1.1170670986175537, L2: 0.0, L3: 0.0014039874076843262, L4: 0.0013041867641732097\n",
      "Epoch 1000, Loss: -1.1192039251327515, Constraint losses: L1: -1.1201825141906738, L2: 0.0, L3: 0.000539243221282959, L4: 0.0004393082344904542\n",
      "Epoch 1500, Loss: -1.1205742359161377, Constraint losses: L1: -1.1211618185043335, L2: 0.0, L3: 0.00034373998641967773, L4: 0.0002438639639876783\n",
      "Epoch 2000, Loss: -1.121247410774231, Constraint losses: L1: -1.1216827630996704, L2: 0.0, L3: 0.0002676844596862793, L4: 0.0001677508989814669\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 10.051679611206055, Constraint losses: L1: 8.052423477172852, L2: 0.0, L3: 0.9996278285980225, L4: 0.9996278285980225\n",
      "Epoch 500, Loss: -1.1086716651916504, Constraint losses: L1: -1.1152492761611938, L2: 0.0, L3: 0.0033386945724487305, L4: 0.0032389392144978046\n",
      "Epoch 1000, Loss: -1.116750717163086, Constraint losses: L1: -1.1189978122711182, L2: 0.0, L3: 0.0011734366416931152, L4: 0.0010735446121543646\n",
      "Epoch 1500, Loss: -1.1191867589950562, Constraint losses: L1: -1.1203930377960205, L2: 0.0, L3: 0.0006531476974487305, L4: 0.0005531556671485305\n",
      "Epoch 2000, Loss: -1.1203389167785645, Constraint losses: L1: -1.1211493015289307, L2: 0.0, L3: 0.0004552006721496582, L4: 0.0003552204871084541\n",
      "Training copula model\n",
      "Epoch 0, Loss: 55.32841491699219, Losses: L1: -2.0886073112487793, L2: 0.0, L3: 0.5314410924911499, L4: 2.264096260070801, L5: 54.62148666381836\n",
      "Epoch 500, Loss: 54.870880126953125, Losses: L1: -2.187284231185913, L2: 0.0, L3: 0.505743145942688, L4: 1.9292625188827515, L5: 54.6231575012207\n",
      "Epoch 1000, Loss: 54.28192901611328, Losses: L1: -2.1829769611358643, L2: 0.0, L3: 0.5214715600013733, L4: 1.3192319869995117, L5: 54.624202728271484\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "Pretraining scheme 3\n",
      "Run 1 of 1\n",
      "----------------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 9.484833717346191, Constraint losses: L1: 7.486184120178223, L2: 0.0, L3: 0.99932461977005, L4: 0.99932461977005\n",
      "Epoch 500, Loss: -1.320063591003418, Constraint losses: L1: -1.322874903678894, L2: 0.0, L3: 0.001455545425415039, L4: 0.001355731743387878\n",
      "Epoch 1000, Loss: -1.3253082036972046, Constraint losses: L1: -1.3263572454452515, L2: 0.0, L3: 0.0005744099617004395, L4: 0.000474556814879179\n",
      "Epoch 1500, Loss: -1.3101170063018799, Constraint losses: L1: -1.3107694387435913, L2: 0.0, L3: 0.00037604570388793945, L4: 0.0002762737567536533\n",
      "Epoch 2000, Loss: -1.3283395767211914, Constraint losses: L1: -1.328852891921997, L2: 0.0, L3: 0.00030666589736938477, L4: 0.0002066876768367365\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 7.2482380867004395, Constraint losses: L1: 5.258471965789795, L2: 0.0, L3: 0.9948831796646118, L4: 0.9948831796646118\n",
      "Epoch 500, Loss: -1.20432448387146, Constraint losses: L1: -1.2059454917907715, L2: 0.0, L3: 0.0008603334426879883, L4: 0.0007606286089867353\n",
      "Epoch 1000, Loss: -1.201890468597412, Constraint losses: L1: -1.2025506496429443, L2: 0.0, L3: 0.000380098819732666, L4: 0.0002801464288495481\n",
      "Epoch 1500, Loss: -1.2072722911834717, Constraint losses: L1: -1.2076894044876099, L2: 0.0, L3: 0.00025850534439086914, L4: 0.00015849072951823473\n",
      "Epoch 2000, Loss: -1.2059893608093262, Constraint losses: L1: -1.206289529800415, L2: 0.0, L3: 0.00020009279251098633, L4: 0.00010010273399529979\n",
      "Training copula model\n",
      "Epoch 0, Loss: 0.023139704018831253, Losses: L1: 13.695754051208496, L2: 3.0180106023180997e-06, L3: 0.9999986886978149, L4: 154.1798095703125, L5: 64.2354736328125\n",
      "Epoch 500, Loss: 0.015181073918938637, Losses: L1: 17.144527435302734, L2: 6.185277925396804e-06, L3: 1.0000051259994507, L4: 138.16270446777344, L5: 64.32450103759766\n",
      "Epoch 1000, Loss: 0.015179462730884552, Losses: L1: 13.035008430480957, L2: 8.08881353009383e-08, L3: 0.9999980330467224, L4: 138.0676727294922, L5: 64.32453155517578\n",
      "Training done\n",
      "Pretraining done\n",
      "Marginal Model 1 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.425479888916016, Constraint losses: L1: 18.42068099975586, L2: 0.0015993342967703938, L3: 1.0015993118286133, L4: 1.0015993118286133\n",
      "Epoch 500, Loss: -1.1132283210754395, Constraint losses: L1: -1.1162195205688477, L2: 0.0, L3: 0.0015454888343811035, L4: 0.001445723231881857\n",
      "Epoch 1000, Loss: -1.1180061101913452, Constraint losses: L1: -1.1190561056137085, L2: 0.0, L3: 0.0005748867988586426, L4: 0.00047502329107373953\n",
      "Epoch 1500, Loss: -1.119762897491455, Constraint losses: L1: -1.1204102039337158, L2: 0.0, L3: 0.00037354230880737305, L4: 0.0002736481255851686\n",
      "Epoch 2000, Loss: -1.1206661462783813, Constraint losses: L1: -1.121158242225647, L2: 0.0, L3: 0.00029599666595458984, L4: 0.00019606674322858453\n",
      "Marginal Model 2 Training\n",
      "Training model\n",
      "Epoch 0, Loss: 20.425390243530273, Constraint losses: L1: 18.42068099975586, L2: 0.0015703215030953288, L3: 1.0015703439712524, L4: 1.0015703439712524\n",
      "Epoch 500, Loss: -1.116151213645935, Constraint losses: L1: -1.1183888912200928, L2: 0.0, L3: 0.0011687278747558594, L4: 0.0010688917245715857\n",
      "Epoch 1000, Loss: -1.1162089109420776, Constraint losses: L1: -1.117070198059082, L2: 0.0, L3: 0.0004805922508239746, L4: 0.0003806845052167773\n",
      "Epoch 1500, Loss: -1.1208317279815674, Constraint losses: L1: -1.121364712715149, L2: 0.0, L3: 0.0003165006637573242, L4: 0.00021653926523867995\n",
      "Epoch 2000, Loss: -1.120746374130249, Constraint losses: L1: -1.1211438179016113, L2: 0.0, L3: 0.0002487301826477051, L4: 0.00014880168600939214\n",
      "Training copula model\n",
      "Epoch 0, Loss: 206.32879638671875, Losses: L1: 12.91313362121582, L2: 0.0, L3: 0.999998152256012, L4: 138.0521240234375, L5: 54.36354064941406\n",
      "Epoch 500, Loss: 165.5793914794922, Losses: L1: 7.584641456604004, L2: 0.0, L3: 0.9998879432678223, L4: 102.23981475830078, L5: 54.75505447387695\n",
      "Epoch 1000, Loss: 164.83267211914062, Losses: L1: 6.865375995635986, L2: 3.3252372209913172e-12, L3: 0.9998422861099243, L4: 102.1928939819336, L5: 54.77455139160156\n",
      "Training done\n",
      "Regular training done\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "Model: ('Regular', frozenset({('lr', 0.001), ('num_neurons', 10), ('solver', 'adam'), ('uniform_points', 15), ('scheduler', None), ('boundary_points', 100), ('epochs', 1500), ('batch_size', 512), ('num_layers', 3)})) has total loss: 288.42974853515625\n",
      "Model: ('Regular', frozenset({('lr', 0.001), ('num_neurons', 10), ('solver', 'adam'), ('uniform_points', 15), ('scheduler', None), ('boundary_points', 100), ('epochs', 1500), ('batch_size', 512), ('num_layers', 3)})) has average training time: 23.521579424540203\n",
      "Model: ('Regular', frozenset({('lr', 0.001), ('num_neurons', 10), ('solver', 'adam'), ('uniform_points', 15), ('scheduler', None), ('boundary_points', 100), ('epochs', 1500), ('batch_size', 512), ('num_layers', 3)})) has constraint loss: 276.834228515625\n",
      "----------------------------------------------------------------------------\n",
      "Model: ('Pretrain1', frozenset({('lr', 0.001), ('num_neurons', 10), ('solver', 'adam'), ('uniform_points', 15), ('scheduler', None), ('boundary_points', 100), ('epochs', 1500), ('batch_size', 512), ('num_layers', 3)})) has total loss: 235.63131713867188\n",
      "Model: ('Pretrain1', frozenset({('lr', 0.001), ('num_neurons', 10), ('solver', 'adam'), ('uniform_points', 15), ('scheduler', None), ('boundary_points', 100), ('epochs', 1500), ('batch_size', 512), ('num_layers', 3)})) has average training time: 26.336110830307007\n",
      "Model: ('Pretrain1', frozenset({('lr', 0.001), ('num_neurons', 10), ('solver', 'adam'), ('uniform_points', 15), ('scheduler', None), ('boundary_points', 100), ('epochs', 1500), ('batch_size', 512), ('num_layers', 3)})) has constraint loss: 220.8319091796875\n"
     ]
    }
   ],
   "source": [
    "## Generating datasets \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "corrMat = np.array([[1, 0.5], [0.5, 1]])\n",
    "A = np.linalg.cholesky(corrMat)\n",
    "Z = np.random.standard_normal((1000, 2))\n",
    "X = (A @ Z.T).T\n",
    "\n",
    "corrMatFrechetUpper = np.array([[1, 0.9999], [0.9999, 1]])\n",
    "corrMatFrechetLower = np.array([[1, -0.9999], [-0.9999, 1]])\n",
    "A_U = np.linalg.cholesky(corrMatFrechetUpper)\n",
    "A_L = np.linalg.cholesky(corrMatFrechetLower)\n",
    "X_U = (A_U @ Z.T).T\n",
    "X_L = (A_L @ Z.T).T\n",
    "\n",
    "\n",
    "\n",
    "## Generating NN grid\n",
    "# grid = {\n",
    "#     'num_layers': [5, 6],\n",
    "#     'num_neurons': [5, 10],\n",
    "#     'lr': [0.01, 0.001, 'scheduler'],\n",
    "#     'solver': ['adam', 'sgd'],\n",
    "#     'epochs': [5000, 10000],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'uniform_data': [100, 500, 1000], \n",
    "#     'marginal_epochs': [5000],\n",
    "# }\n",
    "\n",
    "datasets = {\n",
    "    'Independence': Z,\n",
    "    # 'Positive dependence': X,\n",
    "    # 'Negative dependence': -X,\n",
    "    'FrechetUpper' : X_U,\n",
    "    'FrechetLower' : X_L,\n",
    "}\n",
    "\n",
    "grid = {\n",
    "    'num_layers': [3],\n",
    "    'num_neurons': [10],\n",
    "    'lr': [0.001],\n",
    "    'scheduler': [None],\n",
    "    'solver': ['adam'],\n",
    "    'epochs': [1500],\n",
    "    'batch_size': [512], ## currently not used in copula model\n",
    "    'uniform_points': [15], # per dimension really n^2\n",
    "    'boundary_points': [100], # per dimension\n",
    "}\n",
    "\n",
    "DataTester = DatasetTester(datasets, grid, runs = 1)\n",
    "DataTester.runTest()\n",
    "DataTester.evaluateResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ee8b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.83250975608826 seconds\n"
     ]
    }
   ],
   "source": [
    "key = list(Mtester.resultDict.keys())[0]\n",
    "resdict = Mtester.resultDict\n",
    "print(resdict[key].trainingTime, 'seconds') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
