{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0097f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim\n",
    "from scipy.interpolate import BarycentricInterpolator\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e29be0",
   "metadata": {},
   "source": [
    "# Test of Neural Copula training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ca2ba",
   "metadata": {},
   "source": [
    "## Defining classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad1dc7",
   "metadata": {},
   "source": [
    "### marginal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960bd8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginalModel(nn.Module):\n",
    "    def __init__(self, device, num_layers=5, num_neurons=5, lr=0.01):\n",
    "        super(MarginalModel, self).__init__()\n",
    "\n",
    "        # Model specification\n",
    "        layers = [nn.Linear(1, num_neurons), nn.Tanh()]  # Input layer\n",
    "        for _ in range(num_layers - 1):  # Hidden layers\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons, 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.batch_size = 128\n",
    "\n",
    "        # Data for training\n",
    "        self.ObservedData = None\n",
    "        self.uniform_data = torch.tensor(np.linspace(0, 1, 500), dtype=torch.float32).view(-1, 1)\n",
    "        self.lower_bound = torch.tensor([[0.0]]).to(device)\n",
    "        self.upper_bound = torch.tensor([[1.0]]).to(device)\n",
    "\n",
    "        ## For sampling\n",
    "        self.domainUpper = torch.tensor([[1.0]]).to(device)\n",
    "        self.domainLower= torch.tensor([[0.0]]).to(device)\n",
    "        self.inverted = False\n",
    "        self.inverseInterpolator = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def loss_function(self, x):\n",
    "        x = x.detach().requires_grad_()\n",
    "        self.uniform_data= self.uniform_data.detach().requires_grad_()\n",
    "        y_pred = self(x)\n",
    "        y_pred_uniform = self(self.uniform_data)\n",
    "        dydx = torch.autograd.grad(y_pred, x, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "        dydx_uniform = torch.autograd.grad(y_pred_uniform, self.uniform_data, torch.ones_like(y_pred_uniform), create_graph=True)[0]\n",
    "\n",
    "        L1 = -torch.mean(torch.log(torch.relu(dydx) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-dydx_uniform))\n",
    "        L3 = torch.abs(1 - torch.sum(dydx_uniform)/self.uniform_data.shape[0])\n",
    "        L4 = self(self.lower_bound) + torch.abs(1 - self(self.upper_bound))\n",
    "        Loss = L1*0.001 + L2 + L3 +  L4\n",
    "        return Loss , L1, L2, L3, L4\n",
    "\n",
    "\n",
    "    def train_model(self, X, epochs=5000, log_interval=500):\n",
    "        self.ObservedData = X\n",
    "\n",
    "        # dataset = torch.utils.data.TensorDataset(X)\n",
    "        # data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            #for batch_idx, (data,) in enumerate(data_loader):\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            loss, L1, L2, L3, L4 = self.loss_function(X)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if epoch % log_interval == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}, Constraint losses: L1: {L1.item()}, L2: {L2.item()}, L3: {L3.item()}, L4: {L4.item()}')\n",
    "\n",
    "    def newSamples(self, ProbabilityValues = None, n = 1000): \n",
    "        # Sampling method that replaces the interpolator which struggles with values close to 0 and 1\n",
    "        if ProbabilityValues is None:\n",
    "            ProbabilityValues = np.random.uniform(0, 1, n)\n",
    "        if torch.is_tensor(ProbabilityValues) == False:\n",
    "            ProbabilityValues = torch.tensor(ProbabilityValues, dtype=torch.float32).view(-1, 1)\n",
    "        sampledData = self._vectorized_bisection(ProbabilityValues).detach().numpy()\n",
    "        return sampledData\n",
    "\n",
    "    # def sample(self, n = 1000, ProbabilityValues = None): # Can sample only from marginal and with given probability values\n",
    "    #     if self.inverted == False:\n",
    "    #         print(\"Model not inverted. Inverting model...\")\n",
    "    #         self.inverseInterpolator = self._invertModel(asTensor=False)\n",
    "    #         self.inverted = True\n",
    "\n",
    "    #     if ProbabilityValues is None:\n",
    "    #         ProbabilityValues = np.random.uniform(0, 1, n)\n",
    "            \n",
    "    #     # Generate random samples from a uniform distribution\n",
    "    #     sampledData = self.inverseInterpolator(ProbabilityValues)\n",
    "    #     return sampledData\n",
    "\n",
    "    # def _invertModel(self, asTensor=True, plot=False):\n",
    "    #     # Invert the model to get inverse CDF function\n",
    "    #     rangeUpper = self(self.domainUpper)\n",
    "    #     rangeLower= self(self.domainLower)\n",
    "    #     # Generate Chebyshev nodes\n",
    "    #     n = 3000\n",
    "    #     rangePoints = self._chebyshev_nodes(n, rangeLower, rangeUpper).view(-1, 1)\n",
    "\n",
    "    #     # use nodes to find inverses\n",
    "    #     domainPoints = self._vectorized_bisection(rangePoints)\n",
    "\n",
    "    #     # Add boundary points\n",
    "    #     rangePoints = torch.cat((torch.tensor([0.0]), rangePoints.squeeze(), torch.tensor([1.0])))\n",
    "    #     domainPoints = torch.cat((torch.tensor([0.0]), domainPoints.squeeze(), torch.tensor([1.0])))\n",
    "    #     rangePoints_np = rangePoints.numpy()\n",
    "    #     domainPoints_np = domainPoints.numpy()\n",
    "\n",
    "    #     # Create interpolator\n",
    "    #     interpolant = BarycentricInterpolator(rangePoints_np, domainPoints_np)\n",
    "    #     interpolantTensor = lambda x: self._tensor_interpolant(x, interpolant) # interpolant using PyTorch tensors\n",
    "\n",
    "    #     if plot:\n",
    "    #         self.plotModel(model = interpolantTensor)\n",
    "    #     if asTensor:\n",
    "    #         return interpolantTensor\n",
    "    #     else:\n",
    "    #         return interpolant\n",
    "        \n",
    "    # def _tensor_interpolant(self, p_tensor, interpolant):\n",
    "    #     \"\"\"Interpolant function that takes PyTorch tensors as input.\"\"\"\n",
    "    #     p_numpy = p_tensor.detach().cpu().numpy()\n",
    "    #     x_numpy = interpolant(p_numpy)\n",
    "    #     return torch.tensor(x_numpy, dtype=torch.float32)\n",
    "\n",
    "    def PlotModel(self):\n",
    "        trainingData = self.ObservedData.detach().numpy()\n",
    "\n",
    "        # # Generate x values for plotting\n",
    "        x_points = np.linspace(0, 1, 100)\n",
    "        x_plot =torch.tensor(x_points, dtype=torch.float32).view(-1, 1)\n",
    "        x_plot.requires_grad = True  # Enable gradients for x_plot\n",
    "        y_pred = self(x_plot)  # Keep y_pred in computation graph\n",
    "        pdfPred = torch.autograd.grad(y_pred, x_plot, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "\n",
    "        ### Convert to numpy for plotting\n",
    "        y_vals = y_pred.detach().numpy()\n",
    "        grad_vals = pdfPred.detach().numpy()\n",
    "\n",
    "        ### Plot the neural network approximation\n",
    "        plt.plot(x_points, y_vals, label='CDF approximation', linestyle='dashed')\n",
    "        ### Plot the derivative\n",
    "        plt.plot(x_points, grad_vals, label=\"PDF Approximation\", linestyle='solid')\n",
    "        plt.hist(trainingData, bins=1000, density=True, alpha=0.6, label='True Distribution of data');\n",
    "        plt.legend()\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Neural Network Approximation of PDF and CDF ')\n",
    "        plt.show()\n",
    "\n",
    "    # def PlotInverse(self):\n",
    "    #     if self.inverted == False:\n",
    "    #         print(\"Model not inverted. Inverting model...\")\n",
    "    #         self.inverseInterpolator = self._invertModel(asTensor=False)\n",
    "    #         self.inverted = True    \n",
    "        \n",
    "    #     x_points = np.linspace(0, 1, 1000)\n",
    "    #     y_pred = self.inverseInterpolator(x_points)  \n",
    "    #     #ChebyshovPoints = self._chebyshev_nodes(3000, 0, 1).numpy()\n",
    "    #     plt.plot(x_points, y_pred, color='blue', label='Inverse CDF approximation')  # Blue line\n",
    "    #     #plt.scatter(ChebyshovPoints, self.inverseInterpolator(ChebyshovPoints), color='red', label='Chebyshev nodes')  # Red points\n",
    "    #     plt.xlabel('x')\n",
    "    #     plt.ylabel('Inverse CDF')\n",
    "    #     plt.title('Inverse CDF Approximation')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "        \n",
    "    # def _chebyshev_nodes(self, n, a, b):\n",
    "    #     \"\"\"Generate n Chebyshev nodes in the interval [a, b].\"\"\"\n",
    "    #     return torch.tensor([0.5 * (a + b) + 0.5 * (b - a) * np.cos((2 * k + 1) * np.pi / (2 * n)) for k in range(n)], dtype=torch.float32)\n",
    "\n",
    "    def _vectorized_bisection(self, y, tol=1e-6, max_iter=100):\n",
    "        \"\"\"\n",
    "        Vectorized Bisection Method to find roots of a function f(y) in the interval [0,1] for multiple values of y simultaneously.\n",
    "        \n",
    "        Parameters:\n",
    "        f : function\n",
    "            The function whose roots are to be found.\n",
    "        y : torch.Tensor\n",
    "            Tensor of values for which roots are to be found.\n",
    "        tol : float, optional\n",
    "            The tolerance for stopping the iteration (default is 1e-6).\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations (default is 100).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor\n",
    "            Tensor of estimated root values.\n",
    "        \"\"\"\n",
    "        a, b = torch.zeros_like(y), torch.ones_like(y)\n",
    "        fa, fb = self(a) , self(b) \n",
    "        fa, fb = fa - y, fb - y\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            c = (a + b) / 2  # Midpoint\n",
    "            fc = self(c) - y\n",
    "            left_mask = fc * fa < 0\n",
    "            right_mask = fc * fb < 0\n",
    "            a, b = torch.where(left_mask, a, c), torch.where(right_mask, b, c)\n",
    "            if torch.all(torch.abs(b - a) < tol):\n",
    "                break\n",
    "        return (a + b) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b1e32",
   "metadata": {},
   "source": [
    "### copula class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0570c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CopulaModel(nn.Module):\n",
    "    def __init__(self, device, dataPoints, Marginal1, Marginal2, num_layers=5, num_neurons=5, lr=0.01,\n",
    "                solver = 'sgd', scheduler = None, batch_size = 512 ,boundary_points = 100, uniform_points = 15, lossWeight = np.ones(5)):\n",
    "\n",
    "        super(CopulaModel, self).__init__()\n",
    "        dimensions = dataPoints.size(1)\n",
    "        layers = [nn.Linear(dimensions, num_neurons), nn.Tanh()]  # Input layer\n",
    "        for _ in range(num_layers - 1):  # Hidden layers\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons, 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "        ## Optimizer\n",
    "        if solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        elif solver == 'adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        ## Learning rate scheduler\n",
    "        if scheduler == 'step':\n",
    "            self.scheduler = StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
    "        elif scheduler == 'exponential':\n",
    "            self.scheduler = ExponentialLR(self.optimizer, gamma=0.9)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        ### Data for training\n",
    "        self.ObservedData = dataPoints\n",
    "        self.Marginal1 = Marginal1\n",
    "        self.Marginal2 = Marginal2\n",
    "\n",
    "        x1 = self.ObservedData[:,0]\n",
    "        x2 = self.ObservedData[:,1]\n",
    "        ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "        ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "        self.u = torch.cat((ProbVals1, ProbVals2), dim=1).to(device)\n",
    "\n",
    "        # Boundary points\n",
    "        self.upperBoundary = self._generateUpperBoundaryPoints(dimensions, num_points=boundary_points).to(device)\n",
    "        self.lowerBoundary = self._generateLowerBoundPoints(dimensions, num_points=boundary_points).to(device)\n",
    "        # Uniform grid points\n",
    "        u= np.linspace(0.0, 1.0, uniform_points)\n",
    "        U1, U2 = np.meshgrid(u, u, indexing=\"ij\")\n",
    "        unitSquarePoints= np.column_stack((U1.ravel(), U2.ravel()))\n",
    "        self.unitSquaretensor = torch.tensor(unitSquarePoints, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Look at later\n",
    "        self.flagSumData = self._FlagSum(self.unitSquaretensor, self.u).to(device)\n",
    "        self.delta_m = 1 / dataPoints.shape[0]\n",
    "\n",
    "        # Practicality\n",
    "        self.batch_size = batch_size\n",
    "        self.isTrained = False\n",
    "        self.lossWeight = lossWeight\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    # def _CopulaGradient(self,x):\n",
    "    #     x1 = x[:,0]\n",
    "    #     x2 = x[:,1]\n",
    "    #     # Probability integral transform\n",
    "    #     ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "    #     ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "    #     u = torch.cat((ProbVals1, ProbVals2), dim=1)\n",
    "    #     # Calculate marginal densities\n",
    "    #     gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True, allow_unused=True)[0]\n",
    "    #     gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True, allow_unused=True )[0]\n",
    "    #     # Prediction and gradient of copula CDF\n",
    "    #     y_pred = self(u)\n",
    "    #     gradCopulaModel = torch.autograd.grad(y_pred, u, torch.ones_like(y_pred), create_graph=True, allow_unused=True )[0]\n",
    "    #     #cross_derivative = torch.autograd.grad(outputs=gradCopulaModel[:, 0], inputs=u, grad_outputs=torch.ones_like(gradCopulaModel[:, 0]),retain_graph=True)[0][:, 1]\n",
    "    #     CopulaGradient = gradCopulaModel[:, 0] * gradM1 * gradCopulaModel[:, 1] * gradM2\n",
    "    #     #CopulaGradient = cross_derivative * gradM1 * gradM2  \n",
    "    #     return CopulaGradient\n",
    "        \n",
    "    def _CopulaGradient(self, x, AsUnitsquare = False):\n",
    "        if AsUnitsquare == False:\n",
    "            x1 = x[:, 0]\n",
    "            x2 = x[:, 1]\n",
    "            \n",
    "            # Compute PIT-transformed inputs\n",
    "            ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "            ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "            u = torch.cat((ProbVals1, ProbVals2), dim=1)\n",
    "            \n",
    "            # Marginal densities: f1(x1), f2(x2)\n",
    "            # gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True)[0]\n",
    "            # gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True)[0]\n",
    "            \n",
    "        else:\n",
    "            u = x\n",
    "            # x1 = torch.tensor(self.Marginal1.newSamples(ProbabilityValues = x[:, 0].detach().numpy()))\n",
    "            # x2 = torch.tensor(self.Marginal2.newSamples(ProbabilityValues = x[:, 1].detach().numpy()))\n",
    "            # x1.requires_grad = True\n",
    "            # x2.requires_grad = True\n",
    "            # ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "            # ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "            # # Marginal densities: f1(x1), f2(x2)\n",
    "            # gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True)[0]\n",
    "            # gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True)[0]\n",
    "        # Evaluate model C(u,v)\n",
    "        y_pred = self(u)\n",
    "\n",
    "        # First partial: ∂C/∂u\n",
    "        grad_u = torch.autograd.grad(outputs=y_pred,inputs=u,grad_outputs=torch.ones_like(y_pred),create_graph=True)[0][:, 0]\n",
    "        # Second partial: ∂²C/∂u∂v\n",
    "        grad_uv = torch.autograd.grad(outputs=grad_u,inputs=u,grad_outputs=torch.ones_like(grad_u),create_graph=True)[0][:, 1]\n",
    "\n",
    "        # Combine to form full joint density\n",
    "        #joint_density = grad_uv * gradM1 * gradM2\n",
    "        return grad_uv\n",
    "\n",
    "    # def _HVolume(self, unitsquareTensor):\n",
    "    #     lowerLeftPoints = unitsquareTensor[(unitsquareTensor[:,0] < 0.999) & (unitsquareTensor[:,1] < 0.999)]\n",
    "    #     lowerRightPoints = lowerLeftPoints.clone()\n",
    "    #     upperRightPoints = lowerLeftPoints.clone()\n",
    "    #     upperLeftPoints = lowerLeftPoints.clone()\n",
    "\n",
    "    #     lowerRightPoints[:,0] += 0.001 \n",
    "    #     upperRightPoints += 0.001\n",
    "    #     upperLeftPoints[:,1] += 0.001 \n",
    "\n",
    "    #     #print(lowerLeftPoints.shape, lowerRightPoints.shape, upperRightPoints.shape, upperLeftPoints.shape)\n",
    "\n",
    "    #     Hvolume = self(lowerLeftPoints) - self(lowerRightPoints) + self(upperRightPoints) - self(upperLeftPoints)\n",
    "    #     return Hvolume\n",
    "\n",
    "    def Copula_loss_function(self, x): ## MAKE SURE TO PASS IN THE SAME DATAPOINTS AS IN THE INITIALIZATION\n",
    "        x.requires_grad = True\n",
    "        self.unitSquaretensor.requires_grad = True\n",
    "        CopulaGradientObserved = self._CopulaGradient(x)\n",
    "        n_observed = x.shape[0]\n",
    "        CopulaGradientUnitSquare = self._CopulaGradient(self.unitSquaretensor, AsUnitsquare=True)  \n",
    "        n_unitsquare = self.unitSquaretensor.shape[0]\n",
    "        #flagSumData = self._FlagSum(x, self.unitSquaretensor)\n",
    "        pred_unitSquare = self(self.unitSquaretensor)\n",
    "\n",
    "        \n",
    "        L1 = -torch.mean(torch.log(torch.relu(CopulaGradientObserved) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-CopulaGradientUnitSquare))\n",
    "        L3 = torch.abs(1 - torch.sum(CopulaGradientUnitSquare / CopulaGradientUnitSquare.shape[0]))\n",
    "        L4 = torch.sum(self(self.lowerBoundary)) + torch.sum(torch.abs(self(self.upperBoundary) - torch.min(self.upperBoundary, dim=1).values.view(-1,1)))      \n",
    "        L5 = (1/(n_unitsquare)) * torch.sum(torch.abs(pred_unitSquare.squeeze() - self.flagSumData/n_observed))\n",
    "\n",
    "        #Hvolume = self._HVolume(self.unitSquaretensor)\n",
    "        #L5 = torch.mean(torch.relu(-Hvolume))\n",
    "\n",
    "        Loss =  self.lossWeight[0]*L1 + self.lossWeight[1]*L2 + self.lossWeight[2]*L3 + self.lossWeight[3]*L4 + self.lossWeight[4]*L5\n",
    "        return Loss, L1, L2, L3, L4, L5\n",
    "    \n",
    "    def _generateLowerBoundPoints(self, d, num_points=100):\n",
    "        grid = np.linspace(0, 1, num_points)\n",
    "        all_surfaces = []\n",
    "        for k in range(d):\n",
    "            grid_points = np.meshgrid(*([grid] * (d - 1)), indexing=\"ij\")\n",
    "            points = np.stack(grid_points, axis=-1).reshape(-1, d - 1)\n",
    "            surface_points = np.insert(points, k, 0, axis=1)\n",
    "            all_surfaces.append(surface_points)\n",
    "        return torch.tensor(np.vstack(all_surfaces), dtype=torch.float32)\n",
    "\n",
    "    def _generateUpperBoundaryPoints(self, d, num_points=100):\n",
    "        oneArray = np.ones((d * num_points, d))\n",
    "        u = np.linspace(0, 1, num_points)\n",
    "        for i in range(d):\n",
    "            oneArray[i * num_points:(i + 1) * num_points, i] = u\n",
    "        return torch.tensor(oneArray, dtype=torch.float32)\n",
    "    \n",
    "    def _flag(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized version of _flag that returns a matrix where each element is a comparison result.\"\"\"\n",
    "        return torch.all(y.unsqueeze(0) < x.unsqueeze(1), dim=2).float()\n",
    "\n",
    "    def _FlagSum(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized computation of FlagSum without explicit Python loops.\"\"\"\n",
    "        return self._flag(x, y).sum(dim=1)\n",
    "\n",
    "    def train_model(self, X, epochs=5000, log_interval=500):\n",
    "            dataset = TensorDataset(X)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                for batch in dataloader:\n",
    "                    batch_X = batch[0]  # since TensorDataset returns a tuple\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss, L1, L2, L3, L4, L5 = self.Copula_loss_function(batch_X)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                if epoch % log_interval == 0:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss.item()}, Losses: L1: {L1.item()}, L2: {L2.item()}, L3: {L3.item()}, L4: {L4.item()}, L5: {L5.item()}')\n",
    "\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            self.isTrained = True  # Set the flag to indicate that the model has been trained  \n",
    "\n",
    "    def sample(self, n=1000):\n",
    "            if not self.isTrained:\n",
    "                print(\"Model not trained. Training model...\")\n",
    "                self.train_model(self.ObservedData)\n",
    "            # Build the grid to compute M\n",
    "            dim_range = torch.linspace(0, 1, 1000)\n",
    "            grid = torch.stack(torch.meshgrid(dim_range, dim_range, indexing='ij'), dim=-1).flatten(0, 1).requires_grad_()\n",
    "            C_uv = self(grid)\n",
    "            grad_u = torch.autograd.grad(C_uv, grid, torch.ones_like(C_uv), create_graph=True)[0][:, 0]\n",
    "            grad_uv = torch.autograd.grad(grad_u, grid, torch.ones_like(grad_u), create_graph=True)[0][:, 1]\n",
    "            M = grad_uv.max().item()\n",
    "\n",
    "            # Initialize empty list to collect samples\n",
    "            accepted_samples = []\n",
    "            total_accepted = 0\n",
    "\n",
    "            # While not enough samples\n",
    "            while total_accepted < n:\n",
    "                # Propose candidates\n",
    "                batch_size = 1*n  \n",
    "                u1 = torch.rand(batch_size, 1)\n",
    "                u2 = torch.rand(batch_size, 1)\n",
    "                u = torch.cat((u1, u2), dim=1).requires_grad_(True)\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self(u)\n",
    "                grad_u1 = torch.autograd.grad(y_pred, u, torch.ones_like(y_pred), create_graph=True)[0][:, 0]\n",
    "                grad_u1_u2 = torch.autograd.grad(grad_u1, u, torch.ones_like(grad_u1), create_graph=True)[0][:, 1]\n",
    "                acceptance_prob = grad_u1_u2 / M\n",
    "                random_uniform = torch.rand_like(acceptance_prob)\n",
    "                accepted_batch = u[random_uniform <= acceptance_prob]\n",
    "\n",
    "\n",
    "                total_accepted += accepted_batch.shape[0]\n",
    "                print(f'Accepted: {total_accepted}')\n",
    "                accepted_samples.append(accepted_batch)\n",
    "\n",
    "            # Concatenate all accepted batches\n",
    "            accepted_samples = torch.cat(accepted_samples, dim=0)\n",
    "\n",
    "            # Only keep exactly n samples\n",
    "            accepted_samples = accepted_samples[:n]\n",
    "            return accepted_samples\n",
    "    \n",
    "    def plotSamples(self, sample, ProbSpace = True):\n",
    "        df_samples = pd.DataFrame({\n",
    "        \"U1\": sample[:,0].flatten(),  \n",
    "        \"U2\": sample[:,1].flatten()\n",
    "        })\n",
    "        sns.jointplot(\n",
    "            data=df_samples, x=\"U1\", y=\"U2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "        if ProbSpace:\n",
    "            plt.suptitle(\"Sampled data in probability space\", y=1.02);  \n",
    "        else:\n",
    "            plt.suptitle(\"Sampled data in return space\", y=1.02);\n",
    "        plt.show()\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fe532",
   "metadata": {},
   "source": [
    "### neural copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd719b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCopula():\n",
    "    def __init__(self, data, params):\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.num_neurons = params['num_neurons']\n",
    "        self.lr = params['lr']\n",
    "        self.scheduler = params['scheduler']\n",
    "        self.solver = params['solver']\n",
    "        self.epochs = params['epochs']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.uniform_points = params['uniform_points']\n",
    "        self.boundary_points = params['boundary_points']\n",
    "        self.LossWeights = np.array([params['L1_weight'], params['L2_weight'], params['L3_weight'], params['L4_weight'], params['L5_weight']])\n",
    "\n",
    "        \n",
    "        self.Marginal1 = None\n",
    "        self.Marginal2 = None\n",
    "        self.Copula = None\n",
    "        self.initialCopulaWeights = None\n",
    "        self.SetInitialWeights = None\n",
    "        self.copulaTrainingTime = None\n",
    "        self.data = data\n",
    "        self.normalizedData = None\n",
    "        self.normalizedDataAsTensor = None\n",
    "        self.isNormalized = False\n",
    "\n",
    "        ## Normalization variables\n",
    "        self.scaling = 2.0\n",
    "        self.M1_upper = None\n",
    "        self.M1_lower = None\n",
    "        self.M2_upper = None\n",
    "        self.M2_lower = None\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"{self.device} is used to train model\")\n",
    "\n",
    "    def normalizeData(self,scaling=2.0):\n",
    "        scaling = 2.0\n",
    "        n = self.data.shape[0]\n",
    "        d = self.data.shape[1]\n",
    "\n",
    "        self.M1_upper = scaling * np.max(self.data[:,0])\n",
    "        self.M1_lower = scaling * np.min(self.data[:,0])\n",
    "        self.M2_upper = scaling * np.max(self.data[:,1])\n",
    "        self.M2_lower = scaling * np.min(self.data[:,1])\n",
    "\n",
    "        M1boundaryPoints =  np.array([self.M1_upper, self.M1_lower]) # Creates points for bounds of what the data generated can be\n",
    "        M2boundaryPoints =  np.array([self.M2_upper, self.M2_lower]) # Creates points for bounds of what the data generated can be\n",
    "        extendedData = np.zeros((n+2, d))\n",
    "        extendedData[:,0] = np.concatenate((self.data[:,0], M1boundaryPoints)) # Adding boundary points to the data\n",
    "        extendedData[:,1] = np.concatenate((self.data[:,1], M2boundaryPoints))\n",
    "        self.normalizedData= (extendedData - np.min(extendedData,axis=0)) / (np.max(extendedData,axis=0) - np.min(extendedData,axis=0))\n",
    "        self.normalizedDataAsTensor = torch.tensor(self.normalizedData, dtype=torch.float32).to(self.device)\n",
    "        self.isNormalized = True\n",
    "        pass\n",
    "\n",
    "    def denormalizeData(self,NormalizedData):\n",
    "        DeNormalizedData = NormalizedData * (np.max(self.data, axis=0) - np.min(self.data, axis=0)) + np.min(self.data, axis=0)\n",
    "        return DeNormalizedData\n",
    "\n",
    "    def fitModel(self, method = 'regular'):\n",
    "\n",
    "        ## Training marginals\n",
    "        self.Marginal1 = MarginalModel(device = self.device, num_layers=6, num_neurons=10, lr=0.01).to(self.device)\n",
    "        self.Marginal2 = MarginalModel(device = self.device, num_layers=6, num_neurons=10, lr=0.01).to(self.device)\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print('Marginal Model 1 Training')\n",
    "        self.Marginal1.train_model(self.normalizedDataAsTensor[:-2,0].view(-1, 1) , epochs=1500, log_interval=500)\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print('Marginal Model 2 Training')\n",
    "        self.Marginal2.train_model(self.normalizedDataAsTensor[:-2,1].view(-1, 1) , epochs=1500, log_interval=500)\n",
    "        ## Training copula\n",
    "        self.Copula = CopulaModel(self.device, self.normalizedDataAsTensor[:-2], self.Marginal1, self.Marginal2, num_layers=self.num_layers, num_neurons=self.num_neurons, lr=self.lr, solver=self.solver, scheduler=self.scheduler, batch_size=self.batch_size, boundary_points=self.boundary_points, uniform_points=self.uniform_points, lossWeight = self.LossWeights).to(self.device)\n",
    "        if self.SetInitialWeights is not None:\n",
    "            self.Copula.load_state_dict(self.SetInitialWeights)\n",
    "\n",
    "        self.initialCopulaWeights = self.Copula.state_dict()\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print('Training copula model')\n",
    "        start_time = time.time()\n",
    "        self.Copula.train_model(self.normalizedDataAsTensor[:-2], epochs=self.epochs, log_interval=500)\n",
    "        self.copulaTrainingTime = time.time() - start_time\n",
    "        print('Training done')\n",
    "        pass\n",
    "\n",
    "    def sample(self, Plot = False, n=1000):\n",
    "        copulaSample = self.Copula.sample(n)\n",
    "        marginalSamples = self.sample_marginals(copulaSample)\n",
    "        denormalizedSamples = self.denormalizeData(marginalSamples)\n",
    "        if Plot:\n",
    "            self.Copula.plotSamples(copulaSample, ProbSpace = True)\n",
    "            self.Copula.plotSamples(denormalizedSamples, ProbSpace = False)\n",
    "        return denormalizedSamples\n",
    "\n",
    "    def sample_marginals(self,probabilityValues, n=1000):\n",
    "        sample_marginal1 = self.Marginal1.newSamples(ProbabilityValues=probabilityValues[:,0])\n",
    "        sample_marginal2 = self.Marginal2.newSamples(ProbabilityValues=probabilityValues[:,1])\n",
    "        samples = np.column_stack((sample_marginal1, sample_marginal2))\n",
    "        return samples\n",
    "\n",
    "    def sample_copula(self, Plot = False, n = 1000):\n",
    "        copulaSample = self.Copula.sample(n)\n",
    "        if Plot:\n",
    "            self.Copula.plotSamples(copulaSample, ProbSpace = True)\n",
    "        return copulaSample\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad78b2",
   "metadata": {},
   "source": [
    "### nc validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0df137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NC_validator():\n",
    "    def __init__(self, NeuralCopula):\n",
    "        self.NC = NeuralCopula\n",
    "        self.Marginal1 = NeuralCopula.Marginal1\n",
    "        self.Marginal2 = NeuralCopula.Marginal2\n",
    "        self.Copula = NeuralCopula.Copula\n",
    "        self.normalizedDataASTensor = NeuralCopula.normalizedDataAsTensor\n",
    "        \n",
    "\n",
    "    def validate(self):\n",
    "        print('Marginal model validation')\n",
    "        self._validateMarginals()\n",
    "        print('Copula model validation')\n",
    "        self._validateCopula()\n",
    "        pass\n",
    "\n",
    "    def _validateMarginals(self): \n",
    "        ## Plot the marginal models\n",
    "        plt.fisize=(3, 3)\n",
    "        print('Maginal model 1')\n",
    "        self.Marginal1.PlotModel()\n",
    "        print('Maginal model 2')\n",
    "        self.Marginal2.PlotModel()\n",
    "\n",
    "        # ## Plot inverses of models to check if they are correct\n",
    "        # print('Maginal model 1')\n",
    "        # self.Marginal1.PlotInverse()\n",
    "        # print('Maginal model 2')\n",
    "        # self.Marginal2.PlotInverse()\n",
    "\n",
    "        ## Plot sampled data from the models\n",
    "        Marginal1_samples = self.Marginal1.newSamples(n = 5000)\n",
    "        Marginal2_samples = self.Marginal2.newSamples(n = 5000)\n",
    "\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\n",
    "        x_tensor.requires_grad = True\n",
    "\n",
    "        y_pred1 = self.Marginal1(x_tensor)\n",
    "        dydx1 = torch.autograd.grad(y_pred1, x_tensor, torch.ones_like(y_pred1), create_graph=True)[0]\n",
    "        plt.hist(Marginal1_samples, bins=100, density=True, alpha=1, label='Model 1 samples')\n",
    "        plt.plot(x, dydx1.detach().numpy(), label='Fitted distribution', color='red')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        y_pred2 = self.Marginal2(x_tensor)\n",
    "        dydx2 = torch.autograd.grad(y_pred2, x_tensor, torch.ones_like(y_pred2), create_graph=True)[0]\n",
    "        plt.hist(Marginal2_samples, bins=100, density=True, alpha=1, label='Model 2 samples')\n",
    "        plt.plot(x, dydx2.detach().numpy(), label='Fitted distribution', color='red')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        ## Plot initial data\n",
    "        OriginalMarginal1 = self.normalizedDataASTensor[:-2,0].view(-1, 1).detach().numpy()\n",
    "        OriginalMarginal2 = self.normalizedDataASTensor[:-2,1].view(-1, 1).detach().numpy()\n",
    "\n",
    "        df_returnSpace = pd.DataFrame({\n",
    "            \"X1\": OriginalMarginal1.flatten(),  \n",
    "            \"X2\": OriginalMarginal2.flatten()\n",
    "        })\n",
    "        sns.jointplot(\n",
    "            data=df_returnSpace, x=\"X1\", y=\"X2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "\n",
    "        # Plot Transformed data\n",
    "        TransformedMarginal1 = self.Marginal1(self.normalizedDataASTensor[:-2,0].view(-1, 1)).detach().numpy()\n",
    "        TransformedMarginal2 = self.Marginal2(self.normalizedDataASTensor[:-2,1].view(-1, 1)).detach().numpy()\n",
    "\n",
    "        df_probabilitySpace = pd.DataFrame({\n",
    "            \"U1\": TransformedMarginal1.flatten(),  # Flatten in case of (N,1) shape\n",
    "            \"U2\": TransformedMarginal2.flatten()\n",
    "        })\n",
    "        plt.suptitle(\"Original data in return space\", y=1.02);\n",
    "\n",
    "        # Plot the jointplot\n",
    "        sns.jointplot(\n",
    "            data=df_probabilitySpace, x=\"U1\", y=\"U2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "        plt.suptitle(\"Data when transformed to probability space\", y=1.02);\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _validateCopula(self):\n",
    "        # Create meshgrid\n",
    "        u1 = np.linspace(0, 1, 100)\n",
    "        u2 = np.linspace(0, 1, 100)\n",
    "        U1, U2 = np.meshgrid(u1, u2, indexing=\"ij\")\n",
    "        grid = np.column_stack((U1.ravel(), U2.ravel()))\n",
    "        grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "        # Get model predictions\n",
    "        self.Copula.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.Copula(grid_tensor)\n",
    "        Z = predictions.numpy().reshape(100, 100)  \n",
    "\n",
    "        # Plot\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(U1, U2, Z, cmap=\"viridis\")\n",
    "        ax.set_xlabel(\"u1\")\n",
    "        ax.set_ylabel(\"u2\")\n",
    "        ax.set_zlabel(\"C(u1, u2)\")\n",
    "        ax.set_title(\"Fitted Model Surface\")\n",
    "        ax.view_init(elev=15, azim=256)\n",
    "        plt.show()\n",
    "\n",
    "        print('Is the model 2-increasing?')\n",
    "        resultCol = Z[:, 1:] >= Z[:, :-1]\n",
    "        resultRow = Z[1:, :] >= Z[:-1, :]\n",
    "        has_false_col = np.any(resultCol == False)\n",
    "        has_false_row = np.any(resultRow == False)\n",
    "        print(\"Any False in resultCol?\", has_false_col)\n",
    "        print(\"Any False in resultRow?\", has_false_row)\n",
    "\n",
    "        if has_false_col or has_false_row:\n",
    "            print(\"Model is not 2-increasing\")\n",
    "            # Find indexes where the result is False\n",
    "            falseColIdx = np.where(resultCol == False)  # Indices in column-wise comparison\n",
    "            falseRowIdx = np.where(resultRow == False)  # Indices in row-wise comparison\n",
    "\n",
    "            # Adjust indices for full matrix (since resultCol and resultRow have reduced dimensions)\n",
    "            falseColIdx = (falseColIdx[0], falseColIdx[1] + 1)  # Shift column indices to match original Z\n",
    "            falseRowIdx = (falseRowIdx[0] + 1, falseRowIdx[1])  # Shift row indices to match original Z\n",
    "\n",
    "            # Combine results if needed\n",
    "            falseIdx = list(zip(*falseColIdx)) + list(zip(*falseRowIdx))\n",
    "\n",
    "            print(\"Indices where False in column-wise comparison:\", list(zip(*falseColIdx)))\n",
    "            print(\"Indices where False in row-wise comparison:\", list(zip(*falseRowIdx)))\n",
    "            print(\"All False indices:\", falseIdx)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef75ad6",
   "metadata": {},
   "source": [
    "## Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c39aa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTester():\n",
    "    def __init__(self, datasets, grid, runs = 1 ):\n",
    "        self.datasets = datasets\n",
    "        self.grid = grid\n",
    "        self.runs = runs\n",
    "        self.resultDict = {}\n",
    "        self.isTrained = False\n",
    "        self.LossDict = {}\n",
    "\n",
    "    def runTest(self):\n",
    "        for name, data in self.datasets.items():\n",
    "            # Creating ProgressLogger\n",
    "            file_path = 'Results_' + name + '.json'\n",
    "            logger = ProgressLogger(file_path=file_path)\n",
    "\n",
    "            print(f'######################### Running test with dataset: {name} ###########')\n",
    "            Mtester = MethodTester(data=data, grid = self.grid, runs = 1, logger=logger)\n",
    "            Mtester.runTest()\n",
    "            self.resultDict[name] = Mtester\n",
    "\n",
    "        self.isTrained = True\n",
    "        pass\n",
    "\n",
    "    def evaluateResults(self):\n",
    "        print('############################ Result printout ##############################')\n",
    "        DatasetsKeys = list(self.resultDict.keys())\n",
    "        ModelKeys = list(self.resultDict[DatasetsKeys[0]].resultDict.keys())\n",
    "\n",
    "        for modelKey in ModelKeys:\n",
    "            Loss = 0\n",
    "            trainingTime = 0\n",
    "            constraintLoss = 0\n",
    "            for dataKey in DatasetsKeys:\n",
    "                constraintLoss += self.resultDict[dataKey].resultDict[modelKey].L2 +self.resultDict[dataKey].resultDict[modelKey].L3 +self.resultDict[dataKey].resultDict[modelKey].L4 + self.resultDict[dataKey].resultDict[modelKey].L5\n",
    "                Loss += self.resultDict[dataKey].resultDict[modelKey].L1 +self.resultDict[dataKey].resultDict[modelKey].L2 +self.resultDict[dataKey].resultDict[modelKey].L3 + self.resultDict[dataKey].resultDict[modelKey].L4 + self.resultDict[dataKey].resultDict[modelKey].L5\n",
    "                trainingTime += self.resultDict[dataKey].resultDict[modelKey].copulaTrainingTime\n",
    "                \n",
    "            print('----------------------------------------------------------------------------')\n",
    "            print(f'Model: {modelKey} ')\n",
    "            print(f'Total loss: {Loss}')\n",
    "            print(f'Average training time of copula model: {trainingTime/len(DatasetsKeys)} seconds') \n",
    "            print(f'Constraint loss: {constraintLoss}')\n",
    "            self.LossDict[modelKey] = Loss \n",
    "        print('###########################################################################')\n",
    "\n",
    "class MethodTester():\n",
    "    def __init__(self, data, grid, runs = 1, logger = None):\n",
    "        self.logger = logger\n",
    "        self.data = data\n",
    "        self.grid = grid\n",
    "        self.runs = runs\n",
    "        self.resultDict = {}\n",
    "\n",
    "\n",
    "    def runTest(self):\n",
    "        for params in ParameterGrid(self.grid):\n",
    "            print(f\"Running test with options: {params}\")\n",
    "            ## Regular training\n",
    "            print('Regular training')\n",
    "            self._trainRegular(params)\n",
    "            print('----------------------------------------------------------------------------')\n",
    "\n",
    "    def _trainRegular(self, params):\n",
    "        testerList = []\n",
    "        for run in range(self.runs):\n",
    "            ## Set random seed for reproducibility (maybe)\n",
    "            print(f\"Run {run+1} of {self.runs}\")\n",
    "            tester = ParameterTester(self.data, params)\n",
    "            tester.runTest()\n",
    "            testerList.append(tester)\n",
    "\n",
    "        ## Inspect model results and choose best\n",
    "        #testerLosses = [tester.finalLoss.item()  for tester in testerList]   ## make this unweighted losses\n",
    "        testerLosses = [tester.L1.item() + tester.L2.item() + tester.L3.item() + tester.L4.item() + tester.L5.item()  for tester in testerList]   ## make this unweighted losses\n",
    "\n",
    "        ## store model with smallest loss \n",
    "        bestTester = testerList[np.argmin(testerLosses)] ## [testerConstraintLosses < 0.01]\n",
    "        key = frozenset(params.items())\n",
    "        TrainingData = {\"L1\": bestTester.L1.item(), \"L2\": bestTester.L2.item(), \"L3\": bestTester.L3.item(), \"L4\": bestTester.L4.item(), \"L5\": bestTester.L5.item(), \"time\":bestTester.trainingTime}\n",
    "\n",
    "        if self.logger is not None:\n",
    "            # Now you can call `logger.add_run()` anywhere in your program\n",
    "            key_json = \"__\".join(f\"{k}={v}\" for k, v in sorted(params.items()))\n",
    "\n",
    "            self.logger.add_run(key_json, TrainingData)\n",
    "\n",
    "        self.resultDict[key] = bestTester\n",
    "        \n",
    "\n",
    "class ParameterTester():\n",
    "    def __init__(self, data, parameters, initialWeights = None):\n",
    "        #self.data = data\n",
    "        self.NC = NeuralCopula(data, parameters)\n",
    "        self.NC.SetInitialWeights = initialWeights\n",
    "\n",
    "        self.NC.normalizeData()\n",
    "        self.initialWeights = None\n",
    "        self.finalWeights = None\n",
    "        self.finalLoss = None\n",
    "        self.L1 = None\n",
    "        self.L2 = None \n",
    "        self.L3 = None\n",
    "        self.L4 = None\n",
    "        self.L5 = None\n",
    "        self.trainingTime = None\n",
    "        \n",
    "    def runTest(self, method = 'regular'):\n",
    "        if method == 'regular':\n",
    "            self.NC.fitModel()\n",
    "\n",
    "        self.copulaTrainingTime = self.NC.copulaTrainingTime\n",
    "        self.finalLoss, self.L1, self.L2, self.L3, self.L4, self.L5 = self.NC.Copula.Copula_loss_function(self.NC.normalizedDataAsTensor[:-2])\n",
    "        self.initialWeights = self.NC.initialCopulaWeights\n",
    "        self.finalWeights = self.NC.Copula.state_dict()\n",
    "        self.trainingTime = self.NC.copulaTrainingTime\n",
    "        \n",
    "\n",
    "class ProgressLogger:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, file_path):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(ProgressLogger, cls).__new__(cls)\n",
    "            cls._instance.file_path = file_path\n",
    "        return cls._instance\n",
    "\n",
    "    def add_run(self, run_id, run_data):\n",
    "        data = self._load()\n",
    "        data[run_id] = run_data\n",
    "        self._save(data)\n",
    "\n",
    "    def _load(self):\n",
    "        if os.path.exists(self.file_path):\n",
    "            try:\n",
    "                with open(self.file_path, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "        return {}\n",
    "\n",
    "    def _save(self, data):\n",
    "        with open(self.file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce89a57",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "356746fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Running test with dataset: Independence ###########\n",
      "Running test with options: {'L1_weight': 1, 'L2_weight': 1, 'L3_weight': 1, 'L4_weight': 1, 'L5_weight': 1, 'batch_size': 1024, 'boundary_points': 100, 'epochs': 5000, 'lr': 0.01, 'num_layers': 3, 'num_neurons': 10, 'scheduler': 'exponential', 'solver': 'sgd', 'uniform_points': 100}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "cpu is used to train model\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Epoch 0, Loss: 2.01789927482605, Constraint losses: L1: 17.613677978515625, L2: 0.00012196644092909992, L3: 1.0000818967819214, L4: 1.0000817775726318\n",
      "Epoch 500, Loss: 0.0019716036040335894, Constraint losses: L1: -1.0827127695083618, L2: 0.0, L3: 0.002526402473449707, L4: 0.0005279139732010663\n",
      "Epoch 1000, Loss: 0.0012091752141714096, Constraint losses: L1: -1.1152089834213257, L2: 0.0, L3: 0.0021619796752929688, L4: 0.00016240455443039536\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 2 Training\n",
      "Epoch 0, Loss: 2.007840871810913, Constraint losses: L1: 8.293357849121094, L2: 5.496543735716841e-07, L3: 0.9997735023498535, L4: 0.9997735023498535\n",
      "Epoch 500, Loss: 0.0022012812551110983, Constraint losses: L1: -0.9428524374961853, L2: 0.0, L3: 0.00257110595703125, L4: 0.0005730277625843883\n",
      "Epoch 1000, Loss: 0.001285458100028336, Constraint losses: L1: -1.0471364259719849, L2: 0.0, L3: 0.0021660327911376953, L4: 0.0001665617892285809\n",
      "---------------------------------------------------------------------\n",
      "Training copula model\n",
      "Epoch 0, Loss: 154.18455505371094, Losses: L1: 18.324697494506836, L2: 0.003233840223401785, L3: 1.0032325983047485, L4: 134.60946655273438, L5: 0.24393415451049805\n",
      "Epoch 500, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 1000, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 1500, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 2000, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 2500, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 3000, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 3500, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 4000, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Epoch 4500, Loss: 17.68222999572754, Losses: L1: 4.177958011627197, L2: 0.49357476830482483, L3: 0.25957536697387695, L4: 12.665336608886719, L5: 0.08578488230705261\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "############################ Result printout ##############################\n",
      "----------------------------------------------------------------------------\n",
      "Model: frozenset({('batch_size', 1024), ('num_layers', 3), ('lr', 0.01), ('epochs', 5000), ('num_neurons', 10), ('solver', 'sgd'), ('L1_weight', 1), ('boundary_points', 100), ('L4_weight', 1), ('L5_weight', 1), ('L2_weight', 1), ('scheduler', 'exponential'), ('uniform_points', 100), ('L3_weight', 1)}) \n",
      "Total loss: 17.68222999572754\n",
      "Average training time of copula model: 119.11959457397461 seconds\n",
      "Constraint loss: 13.504271507263184\n",
      "###########################################################################\n"
     ]
    }
   ],
   "source": [
    "## Generating datasets \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "corrMat = np.array([[1, 0.7], [0.7, 1]])\n",
    "A = np.linalg.cholesky(corrMat)\n",
    "Z = np.random.standard_normal((1000, 2))\n",
    "X = (A @ Z.T).T\n",
    "\n",
    "corrMatFrechetUpper = np.array([[1, 0.9999], [0.9999, 1]])\n",
    "corrMatFrechetLower = np.array([[1, -0.9999], [-0.9999, 1]])\n",
    "A_U = np.linalg.cholesky(corrMatFrechetUpper)\n",
    "A_L = np.linalg.cholesky(corrMatFrechetLower)\n",
    "X_U = (A_U @ Z.T).T\n",
    "X_L = (A_L @ Z.T).T\n",
    "\n",
    "datasets = {\n",
    "    'Independence': Z,\n",
    "    # 'Positive dependence': X,\n",
    "    # 'Negative dependence': -X,\n",
    "    # 'FrechetUpper' : X_U,\n",
    "    # 'FrechetLower' : X_L,\n",
    "}\n",
    "\n",
    "# grid = {\n",
    "#     'num_layers': [2, 3, 4],\n",
    "#     'num_neurons': [5, 10, 15],\n",
    "#     'lr': [0.1, 0.01, 0.001], \n",
    "#     'scheduler': ['step', 'exponential', None],# 'step','exponential'\n",
    "#     'solver': ['adam', 'sgd'], # 'adam', 'sgd'\n",
    "#     'epochs': [5000, 10000, 15000],\n",
    "#     'batch_size': [256, 1024], \n",
    "#     'uniform_points': [100], # per dimension really n^2\n",
    "#     'boundary_points': [100], # per dimension\n",
    "#     'L1_weight': [1],\n",
    "#     'L2_weight': [1],\n",
    "#     'L3_weight': [1],\n",
    "#     'L4_weight': [1],\n",
    "#     'L5_weight': [1],\n",
    "# }\n",
    "\n",
    "grid = {\n",
    "    'num_layers': [ 3],\n",
    "    'num_neurons': [10],\n",
    "    'lr': [ 0.01], \n",
    "    'scheduler': [ 'exponential'],# 'step','exponential'\n",
    "    'solver': ['sgd'], # 'adam', 'sgd'\n",
    "    'epochs': [5000],\n",
    "    'batch_size': [1024], \n",
    "    'uniform_points': [100], # per dimension really n^2\n",
    "    'boundary_points': [100], # per dimension\n",
    "    'L1_weight': [1],\n",
    "    'L2_weight': [1],\n",
    "    'L3_weight': [1],\n",
    "    'L4_weight': [1],\n",
    "    'L5_weight': [1],\n",
    "}\n",
    "\n",
    "DataTester = DatasetTester(datasets, grid, runs = 1)\n",
    "DataTester.runTest()\n",
    "DataTester.evaluateResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e587b2b",
   "metadata": {},
   "source": [
    "# Results from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c37328",
   "metadata": {},
   "outputs": [],
   "source": [
    "IndependenceTestData = ProgressLogger(file_path=\"ResultsFromRuns/Results_Independence.json\")._load()\n",
    "IndepDF = pd.DataFrame.from_dict(IndependenceTestData, orient='index')\n",
    "IndepDF['TotalLoss'] = IndepDF.apply(lambda x: x['L1'] + x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "IndepDF['ConstraintLoss'] = IndepDF.apply(lambda x: x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "\n",
    "PosDepTestData = ProgressLogger(file_path=\"ResultsFromRuns/Results_Positive dependence.json\")._load()\n",
    "PosDepDF = pd.DataFrame.from_dict(PosDepTestData, orient='index')\n",
    "PosDepDF['TotalLoss'] = PosDepDF.apply(lambda x: x['L1'] + x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "PosDepDF['ConstraintLoss'] = PosDepDF.apply(lambda x: x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "\n",
    "NegDepTestData = ProgressLogger(file_path=\"ResultsFromRuns/Results_Negative dependence.json\")._load()\n",
    "NegDepDF = pd.DataFrame.from_dict(NegDepTestData, orient='index')\n",
    "NegDepDF['TotalLoss'] = NegDepDF.apply(lambda x: x['L1'] + x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "NegDepDF['ConstraintLoss'] = NegDepDF.apply(lambda x: x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "\n",
    "UpperFrechetTestData = ProgressLogger(file_path=\"ResultsFromRuns/Results_FrechetUpper.json\")._load()\n",
    "UpperFrechetDF = pd.DataFrame.from_dict(UpperFrechetTestData, orient='index')\n",
    "UpperFrechetDF['TotalLoss'] = UpperFrechetDF.apply(lambda x: x['L1'] + x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "UpperFrechetDF['ConstraintLoss'] = UpperFrechetDF.apply(lambda x: x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "\n",
    "LowerFrechetTestData = ProgressLogger(file_path=\"ResultsFromRuns/Results_FrechetLower.json\")._load()\n",
    "LowerFrechetDF = pd.DataFrame.from_dict(LowerFrechetTestData, orient='index')\n",
    "LowerFrechetDF['TotalLoss'] = LowerFrechetDF.apply(lambda x: x['L1'] + x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n",
    "LowerFrechetDF['ConstraintLoss'] = LowerFrechetDF.apply(lambda x: x['L2'] + x['L3'] + x['L4'] + x['L5'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7d83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IndepDF.sort_values(by='TotalLoss', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62aa5246",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PosDepDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rowName \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[0;32m     14\u001b[0m     resultingDf\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndependence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m IndepDF\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 15\u001b[0m     resultingDf\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositiveDependence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mPosDepDF\u001b[49m\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m     resultingDf\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegativeDependence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m NegDepDF\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m     resultingDf\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrechetUpper\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m UpperFrechetDF\u001b[38;5;241m.\u001b[39mloc[rowName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalLoss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PosDepDF' is not defined"
     ]
    }
   ],
   "source": [
    "rows = list(IndepDF.index) ## Change from indepDf to one of the others when done\n",
    "\n",
    "# Initialize the new DataFrame with the correct index and columns\n",
    "resultingDf = pd.DataFrame(index=rows, columns=[\n",
    "    'Independence', \n",
    "    'PositiveDependence', \n",
    "    'NegativeDependence', \n",
    "    'FrechetUpper', \n",
    "    'FrechetLower'\n",
    "])\n",
    "\n",
    "# Populate the DataFrame\n",
    "for rowName in rows:\n",
    "    resultingDf.loc[rowName, 'Independence'] = IndepDF.loc[rowName, 'TotalLoss']\n",
    "    resultingDf.loc[rowName, 'PositiveDependence'] = PosDepDF.loc[rowName, 'TotalLoss']\n",
    "    resultingDf.loc[rowName, 'NegativeDependence'] = NegDepDF.loc[rowName, 'TotalLoss']\n",
    "    resultingDf.loc[rowName, 'FrechetUpper'] = UpperFrechetDF.loc[rowName, 'TotalLoss']\n",
    "    resultingDf.loc[rowName, 'FrechetLower'] = LowerFrechetDF.loc[rowName, 'TotalLoss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e9d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list(IndepDF.index)\n",
    "\n",
    "# Create a new DataFrame with one column and the same index\n",
    "resultingDf = pd.DataFrame(index=rows, columns=['Independence'])\n",
    "\n",
    "# Fill the 'Independence' column with the TotalLoss values from IndepDF\n",
    "for rowName in rows:\n",
    "    resultingDf.loc[rowName, 'Independence'] = IndepDF.loc[rowName, 'TotalLoss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
