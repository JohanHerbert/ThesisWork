{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10103e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Running test with dataset: InDepWeight ###########\n",
      "[[ 1.76405235  0.40015721]\n",
      " [ 0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788]]\n",
      "Running test with options: {'L1_weight': 1, 'L2_weight': 2, 'L3_weight': 0.5, 'L4_weight': 1, 'L5_weight': 1, 'batch_size': 2048, 'boundary_points': 50, 'epochs': 10000, 'lr': 0.1, 'num_layers': 3, 'num_neurons': 10, 'scheduler': 'exponential', 'solver': 'adam', 'uniform_points': 50}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "cpu is used to train model\n",
      "in parameter tester\n",
      "[[ 1.76405235  0.40015721]\n",
      " [ 0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788]]\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Epoch 0, Loss: 2.01789927482605, Constraint losses: L1: 17.613677978515625, L2: 0.00012196644092909992, L3: 1.0000818967819214, L4: 1.0000817775726318\n",
      "Epoch 500, Loss: 0.0019716036040335894, Constraint losses: L1: -1.0827127695083618, L2: 0.0, L3: 0.002526402473449707, L4: 0.0005279139732010663\n",
      "Epoch 1000, Loss: 0.0012091752141714096, Constraint losses: L1: -1.1152089834213257, L2: 0.0, L3: 0.0021619796752929688, L4: 0.00016240455443039536\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 2 Training\n",
      "Epoch 0, Loss: 2.007840871810913, Constraint losses: L1: 8.293357849121094, L2: 5.496543735716841e-07, L3: 0.9997735023498535, L4: 0.9997735023498535\n",
      "Epoch 500, Loss: 0.0022012812551110983, Constraint losses: L1: -0.9428524374961853, L2: 0.0, L3: 0.00257110595703125, L4: 0.0005730277625843883\n",
      "Epoch 1000, Loss: 0.001285458100028336, Constraint losses: L1: -1.0471364259719849, L2: 0.0, L3: 0.0021660327911376953, L4: 0.0001665617892285809\n",
      "---------------------------------------------------------------------\n",
      "Training copula model\n",
      "Epoch 0, Loss: 86.6314926147461, Losses: L1: 18.324697494506836, L2: 0.0032334334682673216, L3: 1.0032318830490112, L4: 67.552734375, L5: 0.2459789216518402\n",
      "Epoch 500, Loss: 5.3016037940979, Losses: L1: 0.42094534635543823, L2: 0.11228238791227341, L3: 0.05197489261627197, L4: 4.609687328338623, L5: 0.020418845117092133\n",
      "Epoch 1000, Loss: 6.252431392669678, Losses: L1: 2.8691723346710205, L2: 0.060165319591760635, L3: 0.11740297079086304, L4: 3.161716938018799, L5: 0.04251015931367874\n",
      "Epoch 1500, Loss: 4.499581336975098, Losses: L1: 0.8344781398773193, L2: 0.04277576878666878, L3: 0.14686346054077148, L4: 3.470489263534546, L5: 0.03563084825873375\n",
      "Epoch 2000, Loss: 17.710323333740234, Losses: L1: 6.535855293273926, L2: 0.22545947134494781, L3: 0.29598134756088257, L4: 10.507608413696289, L5: 0.06794974952936172\n",
      "Epoch 2500, Loss: 2.097795009613037, Losses: L1: 0.5818426012992859, L2: 0.043679166585206985, L3: 0.09697884321212769, L4: 1.3616247177124023, L5: 0.018479837104678154\n",
      "Epoch 3000, Loss: 1.438164234161377, Losses: L1: 0.359926700592041, L2: 0.04247882217168808, L3: 0.09122580289840698, L4: 0.9242657423019409, L5: 0.023401284590363503\n",
      "Epoch 3500, Loss: 1.2589051723480225, Losses: L1: 0.32846295833587646, L2: 0.04413273185491562, L3: 0.08452010154724121, L4: 0.7758541107177734, L5: 0.024062689393758774\n",
      "Epoch 4000, Loss: 1.2192165851593018, Losses: L1: 0.31568360328674316, L2: 0.043259892612695694, L3: 0.08160006999969482, L4: 0.7530698776245117, L5: 0.023143291473388672\n",
      "Epoch 4500, Loss: 1.1956701278686523, Losses: L1: 0.3063570559024811, L2: 0.043331462889909744, L3: 0.0796959400177002, L4: 0.7402195930480957, L5: 0.02258250117301941\n",
      "Epoch 5000, Loss: 1.1776474714279175, Losses: L1: 0.30108606815338135, L2: 0.04293317720293999, L3: 0.07871478796005249, L4: 0.7295292019844055, L5: 0.02180854231119156\n",
      "Epoch 5500, Loss: 1.1662487983703613, Losses: L1: 0.2974392771720886, L2: 0.04272216558456421, L3: 0.07792961597442627, L4: 0.7230069041252136, L5: 0.021393531933426857\n",
      "Epoch 6000, Loss: 1.1581224203109741, Losses: L1: 0.2947043776512146, L2: 0.042595867067575455, L3: 0.0773928165435791, L4: 0.7183822393417358, L5: 0.02114759385585785\n",
      "Epoch 6500, Loss: 1.1519111394882202, Losses: L1: 0.2932713031768799, L2: 0.0424693264067173, L3: 0.07699692249298096, L4: 0.7143456935882568, L5: 0.02085697092115879\n",
      "Epoch 7000, Loss: 1.147500991821289, Losses: L1: 0.29116567969322205, L2: 0.04253728315234184, L3: 0.07680702209472656, L4: 0.7121871709823608, L5: 0.020670099183917046\n",
      "Epoch 7500, Loss: 1.1438920497894287, Losses: L1: 0.2906518578529358, L2: 0.042345818132162094, L3: 0.07663834095001221, L4: 0.7096627950668335, L5: 0.02056654542684555\n",
      "Epoch 8000, Loss: 1.1414791345596313, Losses: L1: 0.28974735736846924, L2: 0.042342741042375565, L3: 0.07653224468231201, L4: 0.7083001136779785, L5: 0.02048005908727646\n",
      "Epoch 8500, Loss: 1.1397080421447754, Losses: L1: 0.28911924362182617, L2: 0.042324699461460114, L3: 0.07646405696868896, L4: 0.7072973847389221, L5: 0.0204100850969553\n",
      "Epoch 9000, Loss: 1.1384133100509644, Losses: L1: 0.28868407011032104, L2: 0.042307060211896896, L3: 0.07641476392745972, L4: 0.7065557241439819, L5: 0.0203519519418478\n",
      "Epoch 9500, Loss: 1.1374541521072388, Losses: L1: 0.28832295536994934, L2: 0.04229504242539406, L3: 0.07638382911682129, L4: 0.7060199975967407, L5: 0.020329292863607407\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "######################### Running test with dataset: PosDepWeight ###########\n",
      "[[1.76405235 1.52060605]\n",
      " [0.97873798 2.28543443]\n",
      " [1.86755799 0.60937459]]\n",
      "Running test with options: {'L1_weight': 1, 'L2_weight': 2, 'L3_weight': 0.5, 'L4_weight': 1, 'L5_weight': 1, 'batch_size': 2048, 'boundary_points': 50, 'epochs': 10000, 'lr': 0.1, 'num_layers': 3, 'num_neurons': 10, 'scheduler': 'exponential', 'solver': 'adam', 'uniform_points': 50}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "cpu is used to train model\n",
      "in parameter tester\n",
      "[[1.76405235 1.52060605]\n",
      " [0.97873798 2.28543443]\n",
      " [1.86755799 0.60937459]]\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Epoch 0, Loss: 2.0195024013519287, Constraint losses: L1: 18.42068099975586, L2: 0.0003606359241530299, L3: 1.0003606081008911, L4: 1.0003604888916016\n",
      "Epoch 500, Loss: 0.0023773403372615576, Constraint losses: L1: -0.9971945881843567, L2: 0.0, L3: 0.002686142921447754, L4: 0.0006883920286782086\n",
      "Epoch 1000, Loss: 0.0012889961944893003, Constraint losses: L1: -1.1171863079071045, L2: 0.0, L3: 0.002202749252319336, L4: 0.00020343333017081022\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 2 Training\n",
      "Epoch 0, Loss: 2.0029141902923584, Constraint losses: L1: 6.348602771759033, L2: 0.0, L3: 0.9982828497886658, L4: 0.9982828497886658\n",
      "Epoch 500, Loss: 0.0027270042337477207, Constraint losses: L1: -0.8747875094413757, L2: 0.0, L3: 0.0027996301651000977, L4: 0.0008021616959013045\n",
      "Epoch 1000, Loss: 0.0013875191798433661, Constraint losses: L1: -1.0674070119857788, L2: 0.0, L3: 0.002227187156677246, L4: 0.00022773913224227726\n",
      "---------------------------------------------------------------------\n",
      "Training copula model\n",
      "Epoch 0, Loss: 71.70530700683594, Losses: L1: 6.16756010055542, L2: 0.0, L3: 0.9979088306427002, L4: 64.83191680908203, L5: 0.2068815380334854\n",
      "Epoch 500, Loss: 10.127411842346191, Losses: L1: 0.8195343613624573, L2: 0.11488879472017288, L3: 0.24360543489456177, L4: 8.916467666625977, L5: 0.03982950374484062\n",
      "Epoch 1000, Loss: 8.446833610534668, Losses: L1: 0.7955877780914307, L2: 0.11732450127601624, L3: 0.2391687035560608, L4: 7.245295524597168, L5: 0.051716435700654984\n",
      "Epoch 1500, Loss: 7.2308807373046875, Losses: L1: 0.500961184501648, L2: 0.12713764607906342, L3: 0.21533381938934326, L4: 6.326451301574707, L5: 0.04152585193514824\n",
      "Epoch 2000, Loss: 3.2824723720550537, Losses: L1: 0.34732556343078613, L2: 0.05652438849210739, L3: 0.15285921096801758, L4: 2.725726842880249, L5: 0.019941598176956177\n",
      "Epoch 2500, Loss: 1.9932159185409546, Losses: L1: 0.19759319722652435, L2: 0.05551518127322197, L3: 0.13774651288986206, L4: 1.5996613502502441, L5: 0.016057677567005157\n",
      "Epoch 3000, Loss: 1.7303301095962524, Losses: L1: 0.17636801302433014, L2: 0.046630725264549255, L3: 0.12826520204544067, L4: 1.3823071718215942, L5: 0.014260933734476566\n",
      "Epoch 3500, Loss: 1.5570474863052368, Losses: L1: 0.1125829666852951, L2: 0.03817278891801834, L3: 0.12529420852661133, L4: 1.292365312576294, L5: 0.013106515631079674\n",
      "Epoch 4000, Loss: 1.4809566736221313, Losses: L1: 0.09505432099103928, L2: 0.03383339196443558, L3: 0.12164461612701416, L4: 1.245150089263916, L5: 0.012263230979442596\n",
      "Epoch 4500, Loss: 1.41660737991333, Losses: L1: 0.08174542337656021, L2: 0.03149159625172615, L3: 0.11864560842514038, L4: 1.2009328603744507, L5: 0.01162311527878046\n",
      "Epoch 5000, Loss: 1.3762946128845215, Losses: L1: 0.070561483502388, L2: 0.03029562160372734, L3: 0.11670583486557007, L4: 1.1755741834640503, L5: 0.011214695870876312\n",
      "Epoch 5500, Loss: 1.346132755279541, Losses: L1: 0.06312474608421326, L2: 0.029565267264842987, L3: 0.11541008949279785, L4: 1.155219316482544, L5: 0.01095312274992466\n",
      "Epoch 6000, Loss: 1.3249521255493164, Losses: L1: 0.05885802209377289, L2: 0.0290740504860878, L3: 0.114360511302948, L4: 1.1399848461151123, L5: 0.010780961252748966\n",
      "Epoch 6500, Loss: 1.3093340396881104, Losses: L1: 0.05626773089170456, L2: 0.028650924563407898, L3: 0.11366802453994751, L4: 1.1282169818878174, L5: 0.010713485069572926\n",
      "Epoch 7000, Loss: 1.2983686923980713, Losses: L1: 0.05433099716901779, L2: 0.02838110737502575, L3: 0.11317682266235352, L4: 1.1200456619262695, L5: 0.010641496628522873\n",
      "Epoch 7500, Loss: 1.2904216051101685, Losses: L1: 0.05285166576504707, L2: 0.028171053156256676, L3: 0.11288940906524658, L4: 1.1141985654830933, L5: 0.01058454904705286\n",
      "Epoch 8000, Loss: 1.284578561782837, Losses: L1: 0.05230548232793808, L2: 0.028023339807987213, L3: 0.11257582902908325, L4: 1.1093640327453613, L5: 0.010574418120086193\n",
      "Epoch 8500, Loss: 1.280551791191101, Losses: L1: 0.051674485206604004, L2: 0.027941375970840454, L3: 0.11238408088684082, L4: 1.1062431335449219, L5: 0.010559453628957272\n",
      "Epoch 9000, Loss: 1.2777528762817383, Losses: L1: 0.051132213324308395, L2: 0.027898211032152176, L3: 0.11226487159729004, L4: 1.1041405200958252, L5: 0.010551275685429573\n",
      "Epoch 9500, Loss: 1.2758628129959106, Losses: L1: 0.05083847790956497, L2: 0.027860168367624283, L3: 0.11217403411865234, L4: 1.1026694774627686, L5: 0.010547560639679432\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "######################### Running test with dataset: NegDepWeight ###########\n",
      "[[ 1.76405235 -1.52060605]\n",
      " [ 0.97873798 -2.28543443]\n",
      " [ 1.86755799 -0.60937459]]\n",
      "Running test with options: {'L1_weight': 1, 'L2_weight': 2, 'L3_weight': 0.5, 'L4_weight': 1, 'L5_weight': 1, 'batch_size': 2048, 'boundary_points': 50, 'epochs': 10000, 'lr': 0.1, 'num_layers': 3, 'num_neurons': 10, 'scheduler': 'exponential', 'solver': 'adam', 'uniform_points': 50}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "cpu is used to train model\n",
      "in parameter tester\n",
      "[[ 1.76405235 -1.52060605]\n",
      " [ 0.97873798 -2.28543443]\n",
      " [ 1.86755799 -0.60937459]]\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Epoch 0, Loss: 2.0212602615356445, Constraint losses: L1: 18.42068099975586, L2: 0.0009466055780649185, L3: 1.0009466409683228, L4: 1.0009464025497437\n",
      "Epoch 500, Loss: 0.0022216015495359898, Constraint losses: L1: -1.0504876375198364, L2: 0.0, L3: 0.0026351213455200195, L4: 0.0006369679467752576\n",
      "Epoch 1000, Loss: 0.001274854876101017, Constraint losses: L1: -1.117990493774414, L2: 0.0, L3: 0.0021961331367492676, L4: 0.0001967123826034367\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 2 Training\n",
      "Epoch 0, Loss: 2.0387535095214844, Constraint losses: L1: 18.42068099975586, L2: 0.006777701899409294, L3: 1.0067776441574097, L4: 1.0067775249481201\n",
      "Epoch 500, Loss: 0.0024667258840054274, Constraint losses: L1: -1.02455472946167, L2: 0.0, L3: 0.002744615077972412, L4: 0.000746665638871491\n",
      "Epoch 1000, Loss: 0.0014133126242086291, Constraint losses: L1: -1.0682718753814697, L2: 0.0, L3: 0.002240478992462158, L4: 0.00024110556114464998\n",
      "---------------------------------------------------------------------\n",
      "Training copula model\n",
      "Epoch 0, Loss: 97.86161041259766, Losses: L1: 14.630542755126953, L2: 0.0006197398179210722, L3: 1.0004591941833496, L4: 82.3309326171875, L5: 0.3986642360687256\n",
      "Epoch 500, Loss: 56.53972625732422, Losses: L1: 5.114190578460693, L2: 0.0, L3: 0.977310299873352, L4: 50.76017379760742, L5: 0.17670731246471405\n",
      "Epoch 1000, Loss: 55.243385314941406, Losses: L1: 3.9881210327148438, L2: 0.0, L3: 0.970943808555603, L4: 50.59495544433594, L5: 0.17483589053153992\n",
      "Epoch 1500, Loss: 52.887298583984375, Losses: L1: 5.254030704498291, L2: 0.02492440491914749, L3: 0.9553964734077454, L4: 46.94527816772461, L5: 0.1604417860507965\n",
      "Epoch 2000, Loss: 47.092750549316406, Losses: L1: 5.444265365600586, L2: 0.14586007595062256, L3: 1.0088876485824585, L4: 40.65161895751953, L5: 0.20070302486419678\n",
      "Epoch 2500, Loss: 63.344146728515625, Losses: L1: 8.849764823913574, L2: 0.0013776064151898026, L3: 1.0009008646011353, L4: 53.806602478027344, L5: 0.1845722496509552\n",
      "Epoch 3000, Loss: 53.77452850341797, Losses: L1: 4.15196418762207, L2: 0.01827547699213028, L3: 0.9664526581764221, L4: 48.93483352661133, L5: 0.16795192658901215\n",
      "Epoch 3500, Loss: 46.559608459472656, Losses: L1: 4.867210865020752, L2: 0.17039211094379425, L3: 1.005598783493042, L4: 40.63695526123047, L5: 0.21185991168022156\n",
      "Epoch 4000, Loss: 46.33588790893555, Losses: L1: 4.843660354614258, L2: 0.17367859184741974, L3: 1.0053645372390747, L4: 40.433616638183594, L5: 0.20857124030590057\n",
      "Epoch 4500, Loss: 46.1095085144043, Losses: L1: 4.7170305252075195, L2: 0.15710236132144928, L3: 1.0046581029891968, L4: 40.37263488769531, L5: 0.20330655574798584\n",
      "Epoch 5000, Loss: 46.13219451904297, Losses: L1: 4.803439617156982, L2: 0.15732187032699585, L3: 1.0050735473632812, L4: 40.30712890625, L5: 0.2044452279806137\n",
      "Epoch 5500, Loss: 46.110416412353516, Losses: L1: 4.808498382568359, L2: 0.1555863469839096, L3: 1.005060076713562, L4: 40.28398132324219, L5: 0.20423579216003418\n",
      "Epoch 6000, Loss: 46.093997955322266, Losses: L1: 4.812806606292725, L2: 0.15422093868255615, L3: 1.005046010017395, L4: 40.266170501708984, L5: 0.20405426621437073\n",
      "Epoch 6500, Loss: 46.08148956298828, Losses: L1: 4.8162360191345215, L2: 0.1531543880701065, L3: 1.005033016204834, L4: 40.25250244140625, L5: 0.20392557978630066\n",
      "Epoch 7000, Loss: 46.072017669677734, Losses: L1: 4.8189239501953125, L2: 0.15227973461151123, L3: 1.0050235986709595, L4: 40.24220275878906, L5: 0.2038206160068512\n",
      "Epoch 7500, Loss: 46.06479263305664, Losses: L1: 4.820979595184326, L2: 0.15156227350234985, L3: 1.0050166845321655, L4: 40.234432220458984, L5: 0.2037472128868103\n",
      "Epoch 8000, Loss: 46.05923843383789, Losses: L1: 4.822642803192139, L2: 0.15101230144500732, L3: 1.0050104856491089, L4: 40.22837448120117, L5: 0.2036919891834259\n",
      "Epoch 8500, Loss: 46.05497360229492, Losses: L1: 4.823977947235107, L2: 0.15058623254299164, L3: 1.0050052404403687, L4: 40.22367858886719, L5: 0.20364564657211304\n",
      "Epoch 9000, Loss: 46.05170440673828, Losses: L1: 4.825015068054199, L2: 0.150260791182518, L3: 1.0050008296966553, L4: 40.22005844116211, L5: 0.20361115038394928\n",
      "Epoch 9500, Loss: 46.04920959472656, Losses: L1: 4.825854301452637, L2: 0.1500062346458435, L3: 1.0049971342086792, L4: 40.217262268066406, L5: 0.20358316600322723\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "######################### Running test with dataset: FrecUpWeight ###########\n",
      "[[1.76405235 1.76953488]\n",
      " [0.97873798 1.01033033]\n",
      " [1.86755799 1.85355078]]\n",
      "Running test with options: {'L1_weight': 1, 'L2_weight': 2, 'L3_weight': 0.5, 'L4_weight': 1, 'L5_weight': 1, 'batch_size': 2048, 'boundary_points': 50, 'epochs': 10000, 'lr': 0.1, 'num_layers': 3, 'num_neurons': 10, 'scheduler': 'exponential', 'solver': 'adam', 'uniform_points': 50}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "cpu is used to train model\n",
      "in parameter tester\n",
      "[[1.76405235 1.76953488]\n",
      " [0.97873798 1.01033033]\n",
      " [1.86755799 1.85355078]]\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Epoch 0, Loss: 2.003603935241699, Constraint losses: L1: 6.473109245300293, L2: 0.0, L3: 0.998565673828125, L4: 0.9985653162002563\n",
      "Epoch 500, Loss: 0.0026443307287991047, Constraint losses: L1: -1.1007893085479736, L2: 0.0, L3: 0.002871572971343994, L4: 0.0008735472802072763\n",
      "Epoch 1000, Loss: 0.0014547915197908878, Constraint losses: L1: -1.118035078048706, L2: 0.0, L3: 0.002286076545715332, L4: 0.000286750087980181\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 2 Training\n",
      "Epoch 0, Loss: 2.0216622352600098, Constraint losses: L1: 18.42068099975586, L2: 0.001080462709069252, L3: 1.0010805130004883, L4: 1.0010805130004883\n",
      "Epoch 500, Loss: 0.002527145203202963, Constraint losses: L1: -1.1105819940567017, L2: 0.0, L3: 0.0028179287910461426, L4: 0.0008197984425351024\n",
      "Epoch 1000, Loss: 0.0014256640570238233, Constraint losses: L1: -1.1161237955093384, L2: 0.0, L3: 0.0022705793380737305, L4: 0.0002712086134124547\n",
      "---------------------------------------------------------------------\n",
      "Training copula model\n",
      "Epoch 0, Loss: 79.75858306884766, Losses: L1: 4.954606056213379, L2: 4.400013494887389e-06, L3: 0.9945400357246399, L4: 74.05679321289062, L5: 0.24990467727184296\n",
      "Epoch 500, Loss: 1.5657492876052856, Losses: L1: -1.385257601737976, L2: 0.32042115926742554, L3: 0.10833722352981567, L4: 2.2284023761749268, L5: 0.02759357914328575\n",
      "Epoch 1000, Loss: 1.0932308435440063, Losses: L1: -2.1744561195373535, L2: 0.2953093647956848, L3: 0.08597362041473389, L4: 2.593794822692871, L5: 0.04028668999671936\n",
      "Epoch 1500, Loss: -0.9424853920936584, Losses: L1: -2.5443477630615234, L2: 0.25800687074661255, L3: 0.07101809978485107, L4: 1.0315080881118774, L5: 0.018831560388207436\n",
      "Epoch 2000, Loss: -1.5604897737503052, Losses: L1: -2.7416703701019287, L2: 0.21545258164405823, L3: 0.06417006254196167, L4: 0.6938846111297607, L5: 0.024305900558829308\n",
      "Epoch 2500, Loss: -1.9480540752410889, Losses: L1: -2.898681879043579, L2: 0.19363383948802948, L3: 0.05778241157531738, L4: 0.5047346949577332, L5: 0.029734190553426743\n",
      "Epoch 3000, Loss: -2.2815651893615723, Losses: L1: -3.10941481590271, L2: 0.20095551013946533, L3: 0.05347198247909546, L4: 0.3702073097229004, L5: 0.02899530529975891\n",
      "Epoch 3500, Loss: -2.551011085510254, Losses: L1: -3.2774672508239746, L2: 0.21908290684223175, L3: 0.05037897825241089, L4: 0.2360226809978485, L5: 0.027078060433268547\n",
      "Epoch 4000, Loss: -2.6344540119171143, Losses: L1: -3.3441178798675537, L2: 0.22457477450370789, L3: 0.04585278034210205, L4: 0.2096502184867859, L5: 0.0279377494007349\n",
      "Epoch 4500, Loss: -2.681565046310425, Losses: L1: -3.3876779079437256, L2: 0.230256125330925, L3: 0.04146397113800049, L4: 0.196189284324646, L5: 0.028679540380835533\n",
      "Epoch 5000, Loss: -2.712214469909668, Losses: L1: -3.4239869117736816, L2: 0.23739011585712433, L3: 0.03668642044067383, L4: 0.19007502496242523, L5: 0.028574101626873016\n",
      "Epoch 5500, Loss: -2.733032703399658, Losses: L1: -3.4476654529571533, L2: 0.2416432797908783, L3: 0.03237581253051758, L4: 0.18652194738388062, L5: 0.02863628789782524\n",
      "Epoch 6000, Loss: -2.747728109359741, Losses: L1: -3.4628872871398926, L2: 0.24424543976783752, L3: 0.028513967990875244, L4: 0.18363097310066223, L5: 0.028780357912182808\n",
      "Epoch 6500, Loss: -2.758460760116577, Losses: L1: -3.4747660160064697, L2: 0.24635851383209229, L3: 0.025616943836212158, L4: 0.18198168277740479, L5: 0.028797758743166924\n",
      "Epoch 7000, Loss: -2.7660324573516846, Losses: L1: -3.4823598861694336, L2: 0.2477882355451584, L3: 0.023517072200775146, L4: 0.18011052906513214, L5: 0.028882015496492386\n",
      "Epoch 7500, Loss: -2.771526575088501, Losses: L1: -3.4880948066711426, L2: 0.24898262321949005, L3: 0.022001981735229492, L4: 0.17868448793888092, L5: 0.028917228803038597\n",
      "Epoch 8000, Loss: -2.7754130363464355, Losses: L1: -3.491438865661621, L2: 0.24938388168811798, L3: 0.020899534225463867, L4: 0.17783847451210022, L5: 0.028969524428248405\n",
      "Epoch 8500, Loss: -2.7782504558563232, Losses: L1: -3.4932749271392822, L2: 0.24949026107788086, L3: 0.020149528980255127, L4: 0.17695453763008118, L5: 0.02901463583111763\n",
      "Epoch 9000, Loss: -2.7801973819732666, Losses: L1: -3.494586944580078, L2: 0.24950920045375824, L3: 0.01962679624557495, L4: 0.1765199899673462, L5: 0.029037801548838615\n",
      "Epoch 9500, Loss: -2.781531572341919, Losses: L1: -3.4953784942626953, L2: 0.24947817623615265, L3: 0.01930546760559082, L4: 0.1761816293001175, L5: 0.029055984690785408\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "######################### Running test with dataset: FrecLoWeight ###########\n",
      "[[ 1.76405235 -1.758217  ]\n",
      " [ 0.97873798 -0.94694989]\n",
      " [ 1.86755799 -1.88119169]]\n",
      "Running test with options: {'L1_weight': 1, 'L2_weight': 2, 'L3_weight': 0.5, 'L4_weight': 1, 'L5_weight': 1, 'batch_size': 2048, 'boundary_points': 50, 'epochs': 10000, 'lr': 0.1, 'num_layers': 3, 'num_neurons': 10, 'scheduler': 'exponential', 'solver': 'adam', 'uniform_points': 50}\n",
      "Regular training\n",
      "Run 1 of 1\n",
      "cpu is used to train model\n",
      "in parameter tester\n",
      "[[ 1.76405235 -1.758217  ]\n",
      " [ 0.97873798 -0.94694989]\n",
      " [ 1.86755799 -1.88119169]]\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 1 Training\n",
      "Epoch 0, Loss: 2.0002293586730957, Constraint losses: L1: 5.848822116851807, L2: 0.0, L3: 0.9971904158592224, L4: 0.9971902370452881\n",
      "Epoch 500, Loss: 0.002110040280967951, Constraint losses: L1: -1.0689796209335327, L2: 0.0, L3: 0.002588629722595215, L4: 0.0005903902929276228\n",
      "Epoch 1000, Loss: 0.0012508392101153731, Constraint losses: L1: -1.118085503578186, L2: 0.0, L3: 0.002184271812438965, L4: 0.00018465296307113022\n",
      "---------------------------------------------------------------------\n",
      "Marginal Model 2 Training\n",
      "Epoch 0, Loss: 2.023406982421875, Constraint losses: L1: 18.42068099975586, L2: 0.001662117661908269, L3: 1.0016621351242065, L4: 1.0016618967056274\n",
      "Epoch 500, Loss: 0.0022722999565303326, Constraint losses: L1: -1.0197988748550415, L2: 0.0, L3: 0.002645134925842285, L4: 0.0006469640065915883\n",
      "Epoch 1000, Loss: 0.00128517160192132, Constraint losses: L1: -1.116033911705017, L2: 0.0, L3: 0.002200305461883545, L4: 0.00020090016187168658\n",
      "---------------------------------------------------------------------\n",
      "Training copula model\n",
      "Epoch 0, Loss: 97.6631851196289, Losses: L1: 16.981069564819336, L2: 0.0012659052154049277, L3: 1.0012341737747192, L4: 79.77369689941406, L5: 0.4052661955356598\n",
      "Epoch 500, Loss: 4.339497089385986, Losses: L1: -2.7387640476226807, L2: 0.786552369594574, L3: 0.0998525619506836, L4: 5.408097267150879, L5: 0.04713302478194237\n",
      "Epoch 1000, Loss: 61.73842239379883, Losses: L1: 10.297776222229004, L2: 0.0912146344780922, L3: 0.9919628500938416, L4: 50.58977508544922, L5: 0.17246031761169434\n",
      "Epoch 1500, Loss: 50.467201232910156, Losses: L1: -1.335797905921936, L2: 0.136597141623497, L3: 0.9940521717071533, L4: 50.85820770263672, L5: 0.17457149922847748\n",
      "Epoch 2000, Loss: 49.7867546081543, Losses: L1: -1.9047257900238037, L2: 0.18944741785526276, L3: 1.0082799196243286, L4: 50.63496780395508, L5: 0.1734788566827774\n",
      "Epoch 2500, Loss: 49.88127517700195, Losses: L1: -1.8344018459320068, L2: 0.19812774658203125, L3: 1.0097776651382446, L4: 50.64103698730469, L5: 0.17349699139595032\n",
      "Epoch 3000, Loss: 49.684261322021484, Losses: L1: -1.995552659034729, L2: 0.21442446112632751, L3: 1.0205442905426025, L4: 50.56755065917969, L5: 0.17314277589321136\n",
      "Epoch 3500, Loss: 49.71050262451172, Losses: L1: -1.963929533958435, L2: 0.23055429756641388, L3: 1.019433856010437, L4: 50.530635833740234, L5: 0.172968327999115\n",
      "Epoch 4000, Loss: 49.67914962768555, Losses: L1: -1.9911853075027466, L2: 0.23792065680027008, L3: 1.0238916873931885, L4: 50.509681701660156, L5: 0.17286863923072815\n",
      "Epoch 4500, Loss: 49.66352081298828, Losses: L1: -2.0046894550323486, L2: 0.2419711947441101, L3: 1.0265793800354004, L4: 50.498165130615234, L5: 0.1728138029575348\n",
      "Epoch 5000, Loss: 49.654415130615234, Losses: L1: -2.0124995708465576, L2: 0.2443869560956955, L3: 1.0282795429229736, L4: 50.4912223815918, L5: 0.17278073728084564\n",
      "Epoch 5500, Loss: 49.64861297607422, Losses: L1: -2.017467498779297, L2: 0.24592448770999908, L3: 1.0294134616851807, L4: 50.48676300048828, L5: 0.17275948822498322\n",
      "Epoch 6000, Loss: 49.64469528198242, Losses: L1: -2.020822286605835, L2: 0.2469480037689209, L3: 1.0301895141601562, L4: 50.48378372192383, L5: 0.17274527251720428\n",
      "Epoch 6500, Loss: 49.64194869995117, Losses: L1: -2.0231752395629883, L2: 0.24765229225158691, L3: 1.030741572380066, L4: 50.481712341308594, L5: 0.17273537814617157\n",
      "Epoch 7000, Loss: 49.63996124267578, Losses: L1: -2.02486252784729, L2: 0.24814514815807343, L3: 1.031129240989685, L4: 50.48023986816406, L5: 0.17272838950157166\n",
      "Epoch 7500, Loss: 49.63850402832031, Losses: L1: -2.026092290878296, L2: 0.24849575757980347, L3: 1.0314124822616577, L4: 50.47917556762695, L5: 0.17272333800792694\n",
      "Epoch 8000, Loss: 49.63743209838867, Losses: L1: -2.026994228363037, L2: 0.24874450266361237, L3: 1.0316188335418701, L4: 50.47840881347656, L5: 0.17271964251995087\n",
      "Epoch 8500, Loss: 49.63662338256836, Losses: L1: -2.0276684761047363, L2: 0.2489238828420639, L3: 1.0317652225494385, L4: 50.47784423828125, L5: 0.17271696031093597\n",
      "Epoch 9000, Loss: 49.63602828979492, Losses: L1: -2.028118848800659, L2: 0.24903717637062073, L3: 1.0318691730499268, L4: 50.47742462158203, L5: 0.17271491885185242\n",
      "Epoch 9500, Loss: 49.635581970214844, Losses: L1: -2.0284955501556396, L2: 0.24912676215171814, L3: 1.0319451093673706, L4: 50.47713851928711, L5: 0.17271357774734497\n",
      "Training done\n",
      "----------------------------------------------------------------------------\n",
      "############################ Result printout ##############################\n",
      "----------------------------------------------------------------------------\n",
      "Model: frozenset({('L5_weight', 1), ('uniform_points', 50), ('batch_size', 2048), ('L2_weight', 2), ('boundary_points', 50), ('num_neurons', 10), ('L3_weight', 0.5), ('solver', 'adam'), ('scheduler', 'exponential'), ('L4_weight', 1), ('num_layers', 3), ('lr', 0.1), ('epochs', 10000), ('L1_weight', 1)}) \n",
      "Total loss: 95.71522521972656\n",
      "Average training time of copula model: 225.63966417312622 seconds\n",
      "Constraint loss: 96.07450103759766\n",
      "###########################################################################\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim\n",
    "from scipy.interpolate import BarycentricInterpolator\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class MarginalModel(nn.Module):\n",
    "    def __init__(self, device, num_layers=5, num_neurons=5, lr=0.01):\n",
    "        super(MarginalModel, self).__init__()\n",
    "\n",
    "        # Model specification\n",
    "        layers = [nn.Linear(1, num_neurons), nn.Tanh()]  # Input layer\n",
    "        for _ in range(num_layers - 1):  # Hidden layers\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons, 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.batch_size = 128\n",
    "\n",
    "        # Data for training\n",
    "        self.ObservedData = None\n",
    "        self.uniform_data = torch.tensor(np.linspace(0, 1, 500), dtype=torch.float32).view(-1, 1).to(device)\n",
    "        self.lower_bound = torch.tensor([[0.0]]).to(device)\n",
    "        self.upper_bound = torch.tensor([[1.0]]).to(device)\n",
    "\n",
    "        ## For sampling\n",
    "        self.domainUpper = torch.tensor([[1.0]]).to(device)\n",
    "        self.domainLower= torch.tensor([[0.0]]).to(device)\n",
    "        self.inverted = False\n",
    "        self.inverseInterpolator = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def loss_function(self, x):\n",
    "        x = x.detach().requires_grad_()\n",
    "        self.uniform_data= self.uniform_data.detach().requires_grad_()\n",
    "        y_pred = self(x)\n",
    "        y_pred_uniform = self(self.uniform_data)\n",
    "        dydx = torch.autograd.grad(y_pred, x, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "        dydx_uniform = torch.autograd.grad(y_pred_uniform, self.uniform_data, torch.ones_like(y_pred_uniform), create_graph=True)[0]\n",
    "\n",
    "        L1 = -torch.mean(torch.log(torch.relu(dydx) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-dydx_uniform))\n",
    "        L3 = torch.abs(1 - torch.sum(dydx_uniform)/self.uniform_data.shape[0])\n",
    "        L4 = self(self.lower_bound) + torch.abs(1 - self(self.upper_bound))\n",
    "        Loss = L1*0.001 + L2 + L3 +  L4\n",
    "        return Loss , L1, L2, L3, L4\n",
    "\n",
    "\n",
    "    def train_model(self, X, epochs=5000, log_interval=500):\n",
    "        self.ObservedData = X\n",
    "\n",
    "        # dataset = torch.utils.data.TensorDataset(X)\n",
    "        # data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            #for batch_idx, (data,) in enumerate(data_loader):\n",
    "                \n",
    "            self.optimizer.zero_grad()\n",
    "            loss, L1, L2, L3, L4 = self.loss_function(X)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if epoch % log_interval == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}, Constraint losses: L1: {L1.item()}, L2: {L2.item()}, L3: {L3.item()}, L4: {L4.item()}')\n",
    "\n",
    "    def newSamples(self, ProbabilityValues = None, n = 1000): \n",
    "        # Sampling method that replaces the interpolator which struggles with values close to 0 and 1\n",
    "        if ProbabilityValues is None:\n",
    "            ProbabilityValues = np.random.uniform(0, 1, n)\n",
    "        if torch.is_tensor(ProbabilityValues) == False:\n",
    "            ProbabilityValues = torch.tensor(ProbabilityValues, dtype=torch.float32).view(-1, 1)\n",
    "        sampledData = self._vectorized_bisection(ProbabilityValues).detach().numpy()\n",
    "        return sampledData\n",
    "\n",
    "    # def sample(self, n = 1000, ProbabilityValues = None): # Can sample only from marginal and with given probability values\n",
    "    #     if self.inverted == False:\n",
    "    #         print(\"Model not inverted. Inverting model...\")\n",
    "    #         self.inverseInterpolator = self._invertModel(asTensor=False)\n",
    "    #         self.inverted = True\n",
    "\n",
    "    #     if ProbabilityValues is None:\n",
    "    #         ProbabilityValues = np.random.uniform(0, 1, n)\n",
    "            \n",
    "    #     # Generate random samples from a uniform distribution\n",
    "    #     sampledData = self.inverseInterpolator(ProbabilityValues)\n",
    "    #     return sampledData\n",
    "\n",
    "    # def _invertModel(self, asTensor=True, plot=False):\n",
    "    #     # Invert the model to get inverse CDF function\n",
    "    #     rangeUpper = self(self.domainUpper)\n",
    "    #     rangeLower= self(self.domainLower)\n",
    "    #     # Generate Chebyshev nodes\n",
    "    #     n = 3000\n",
    "    #     rangePoints = self._chebyshev_nodes(n, rangeLower, rangeUpper).view(-1, 1)\n",
    "\n",
    "    #     # use nodes to find inverses\n",
    "    #     domainPoints = self._vectorized_bisection(rangePoints)\n",
    "\n",
    "    #     # Add boundary points\n",
    "    #     rangePoints = torch.cat((torch.tensor([0.0]), rangePoints.squeeze(), torch.tensor([1.0])))\n",
    "    #     domainPoints = torch.cat((torch.tensor([0.0]), domainPoints.squeeze(), torch.tensor([1.0])))\n",
    "    #     rangePoints_np = rangePoints.numpy()\n",
    "    #     domainPoints_np = domainPoints.numpy()\n",
    "\n",
    "    #     # Create interpolator\n",
    "    #     interpolant = BarycentricInterpolator(rangePoints_np, domainPoints_np)\n",
    "    #     interpolantTensor = lambda x: self._tensor_interpolant(x, interpolant) # interpolant using PyTorch tensors\n",
    "\n",
    "    #     if plot:\n",
    "    #         self.plotModel(model = interpolantTensor)\n",
    "    #     if asTensor:\n",
    "    #         return interpolantTensor\n",
    "    #     else:\n",
    "    #         return interpolant\n",
    "        \n",
    "    # def _tensor_interpolant(self, p_tensor, interpolant):\n",
    "    #     \"\"\"Interpolant function that takes PyTorch tensors as input.\"\"\"\n",
    "    #     p_numpy = p_tensor.detach().cpu().numpy()\n",
    "    #     x_numpy = interpolant(p_numpy)\n",
    "    #     return torch.tensor(x_numpy, dtype=torch.float32)\n",
    "\n",
    "    def PlotModel(self):\n",
    "        trainingData = self.ObservedData.detach().numpy()\n",
    "\n",
    "        # # Generate x values for plotting\n",
    "        x_points = np.linspace(0, 1, 100)\n",
    "        x_plot =torch.tensor(x_points, dtype=torch.float32).view(-1, 1)\n",
    "        x_plot.requires_grad = True  # Enable gradients for x_plot\n",
    "        y_pred = self(x_plot)  # Keep y_pred in computation graph\n",
    "        pdfPred = torch.autograd.grad(y_pred, x_plot, torch.ones_like(y_pred), create_graph=True)[0]\n",
    "\n",
    "        ### Convert to numpy for plotting\n",
    "        y_vals = y_pred.detach().numpy()\n",
    "        grad_vals = pdfPred.detach().numpy()\n",
    "\n",
    "        ### Plot the neural network approximation\n",
    "        plt.plot(x_points, y_vals, label='CDF approximation', linestyle='dashed')\n",
    "        ### Plot the derivative\n",
    "        plt.plot(x_points, grad_vals, label=\"PDF Approximation\", linestyle='solid')\n",
    "        plt.hist(trainingData, bins=1000, density=True, alpha=0.6, label='True Distribution of data');\n",
    "        plt.legend()\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Neural Network Approximation of PDF and CDF ')\n",
    "        plt.show()\n",
    "\n",
    "    # def PlotInverse(self):\n",
    "    #     if self.inverted == False:\n",
    "    #         print(\"Model not inverted. Inverting model...\")\n",
    "    #         self.inverseInterpolator = self._invertModel(asTensor=False)\n",
    "    #         self.inverted = True    \n",
    "        \n",
    "    #     x_points = np.linspace(0, 1, 1000)\n",
    "    #     y_pred = self.inverseInterpolator(x_points)  \n",
    "    #     #ChebyshovPoints = self._chebyshev_nodes(3000, 0, 1).numpy()\n",
    "    #     plt.plot(x_points, y_pred, color='blue', label='Inverse CDF approximation')  # Blue line\n",
    "    #     #plt.scatter(ChebyshovPoints, self.inverseInterpolator(ChebyshovPoints), color='red', label='Chebyshev nodes')  # Red points\n",
    "    #     plt.xlabel('x')\n",
    "    #     plt.ylabel('Inverse CDF')\n",
    "    #     plt.title('Inverse CDF Approximation')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "        \n",
    "    # def _chebyshev_nodes(self, n, a, b):\n",
    "    #     \"\"\"Generate n Chebyshev nodes in the interval [a, b].\"\"\"\n",
    "    #     return torch.tensor([0.5 * (a + b) + 0.5 * (b - a) * np.cos((2 * k + 1) * np.pi / (2 * n)) for k in range(n)], dtype=torch.float32)\n",
    "\n",
    "    def _vectorized_bisection(self, y, tol=1e-6, max_iter=100):\n",
    "        \"\"\"\n",
    "        Vectorized Bisection Method to find roots of a function f(y) in the interval [0,1] for multiple values of y simultaneously.\n",
    "        \n",
    "        Parameters:\n",
    "        f : function\n",
    "            The function whose roots are to be found.\n",
    "        y : torch.Tensor\n",
    "            Tensor of values for which roots are to be found.\n",
    "        tol : float, optional\n",
    "            The tolerance for stopping the iteration (default is 1e-6).\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations (default is 100).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor\n",
    "            Tensor of estimated root values.\n",
    "        \"\"\"\n",
    "        a, b = torch.zeros_like(y), torch.ones_like(y)\n",
    "        fa, fb = self(a) , self(b) \n",
    "        fa, fb = fa - y, fb - y\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            c = (a + b) / 2  # Midpoint\n",
    "            fc = self(c) - y\n",
    "            left_mask = fc * fa < 0\n",
    "            right_mask = fc * fb < 0\n",
    "            a, b = torch.where(left_mask, a, c), torch.where(right_mask, b, c)\n",
    "            if torch.all(torch.abs(b - a) < tol):\n",
    "                break\n",
    "        return (a + b) / 2\n",
    "\n",
    "\n",
    "class CopulaModel(nn.Module):\n",
    "    def __init__(self, device, dataPoints, Marginal1, Marginal2, num_layers=5, num_neurons=5, lr=0.01,\n",
    "                solver = 'sgd', scheduler = None, batch_size = 512 ,boundary_points = 100, uniform_points = 15, lossWeight = np.ones(5)):\n",
    "\n",
    "        super(CopulaModel, self).__init__()\n",
    "        dimensions = dataPoints.size(1)\n",
    "        layers = [nn.Linear(dimensions, num_neurons), nn.Tanh()]  # Input layer\n",
    "        for _ in range(num_layers - 1):  # Hidden layers\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(num_neurons, 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "        ## Optimizer\n",
    "        if solver == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        elif solver == 'adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        ## Learning rate scheduler\n",
    "        if scheduler == 'step':\n",
    "            self.scheduler = StepLR(self.optimizer, step_size=10, gamma=0.99)\n",
    "        elif scheduler == 'exponential':\n",
    "            self.scheduler = ExponentialLR(self.optimizer, gamma=0.999)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        ### Data for training\n",
    "        self.ObservedData = dataPoints\n",
    "        self.Marginal1 = Marginal1\n",
    "        self.Marginal2 = Marginal2\n",
    "\n",
    "        x1 = self.ObservedData[:,0]\n",
    "        x2 = self.ObservedData[:,1]\n",
    "        ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "        ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "        self.u = torch.cat((ProbVals1, ProbVals2), dim=1).to(device)\n",
    "\n",
    "        # Boundary points\n",
    "        self.upperBoundary = self._generateUpperBoundaryPoints(dimensions, num_points=boundary_points).to(device)\n",
    "        self.lowerBoundary = self._generateLowerBoundPoints(dimensions, num_points=boundary_points).to(device)\n",
    "        # Uniform grid points\n",
    "        u= np.linspace(0.0, 1.0, uniform_points)\n",
    "        U1, U2 = np.meshgrid(u, u, indexing=\"ij\")\n",
    "        unitSquarePoints= np.column_stack((U1.ravel(), U2.ravel()))\n",
    "        self.unitSquaretensor = torch.tensor(unitSquarePoints, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Look at later\n",
    "        self.flagSumData = self._FlagSum(self.unitSquaretensor, self.u).to(device)\n",
    "        self.delta_m = 1 / dataPoints.shape[0]\n",
    "\n",
    "        # Practicality\n",
    "        self.batch_size = batch_size\n",
    "        self.isTrained = False\n",
    "        self.lossWeight = lossWeight\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    # def _CopulaGradient(self,x):\n",
    "    #     x1 = x[:,0]\n",
    "    #     x2 = x[:,1]\n",
    "    #     # Probability integral transform\n",
    "    #     ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "    #     ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "    #     u = torch.cat((ProbVals1, ProbVals2), dim=1)\n",
    "    #     # Calculate marginal densities\n",
    "    #     gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True, allow_unused=True)[0]\n",
    "    #     gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True, allow_unused=True )[0]\n",
    "    #     # Prediction and gradient of copula CDF\n",
    "    #     y_pred = self(u)\n",
    "    #     gradCopulaModel = torch.autograd.grad(y_pred, u, torch.ones_like(y_pred), create_graph=True, allow_unused=True )[0]\n",
    "    #     #cross_derivative = torch.autograd.grad(outputs=gradCopulaModel[:, 0], inputs=u, grad_outputs=torch.ones_like(gradCopulaModel[:, 0]),retain_graph=True)[0][:, 1]\n",
    "    #     CopulaGradient = gradCopulaModel[:, 0] * gradM1 * gradCopulaModel[:, 1] * gradM2\n",
    "    #     #CopulaGradient = cross_derivative * gradM1 * gradM2  \n",
    "    #     return CopulaGradient\n",
    "        \n",
    "    def _CopulaGradient(self, x, AsUnitsquare = False):\n",
    "        if AsUnitsquare == False:\n",
    "            x1 = x[:, 0]\n",
    "            x2 = x[:, 1]\n",
    "            \n",
    "            # Compute PIT-transformed inputs\n",
    "            ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "            ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "            u = torch.cat((ProbVals1, ProbVals2), dim=1)\n",
    "            \n",
    "            # Marginal densities: f1(x1), f2(x2)\n",
    "            # gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True)[0]\n",
    "            # gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True)[0]\n",
    "            \n",
    "        else:\n",
    "            u = x\n",
    "            # x1 = torch.tensor(self.Marginal1.newSamples(ProbabilityValues = x[:, 0].detach().numpy()))\n",
    "            # x2 = torch.tensor(self.Marginal2.newSamples(ProbabilityValues = x[:, 1].detach().numpy()))\n",
    "            # x1.requires_grad = True\n",
    "            # x2.requires_grad = True\n",
    "            # ProbVals1 = self.Marginal1(x1.view(-1, 1))\n",
    "            # ProbVals2 = self.Marginal2(x2.view(-1, 1))\n",
    "            # # Marginal densities: f1(x1), f2(x2)\n",
    "            # gradM1 = torch.autograd.grad(ProbVals1, x1, torch.ones_like(ProbVals1), create_graph=True)[0]\n",
    "            # gradM2 = torch.autograd.grad(ProbVals2, x2, torch.ones_like(ProbVals2), create_graph=True)[0]\n",
    "        # Evaluate model C(u,v)\n",
    "        y_pred = self(u)\n",
    "\n",
    "        # First partial: âˆ‚C/âˆ‚u\n",
    "        grad_u = torch.autograd.grad(outputs=y_pred,inputs=u,grad_outputs=torch.ones_like(y_pred),create_graph=True)[0][:, 0]\n",
    "        # Second partial: âˆ‚Â²C/âˆ‚uâˆ‚v\n",
    "        grad_uv = torch.autograd.grad(outputs=grad_u,inputs=u,grad_outputs=torch.ones_like(grad_u),create_graph=True)[0][:, 1]\n",
    "\n",
    "        # Combine to form full joint density\n",
    "        #joint_density = grad_uv * gradM1 * gradM2\n",
    "        return grad_uv\n",
    "\n",
    "    # def _HVolume(self, unitsquareTensor):\n",
    "    #     lowerLeftPoints = unitsquareTensor[(unitsquareTensor[:,0] < 0.999) & (unitsquareTensor[:,1] < 0.999)]\n",
    "    #     lowerRightPoints = lowerLeftPoints.clone()\n",
    "    #     upperRightPoints = lowerLeftPoints.clone()\n",
    "    #     upperLeftPoints = lowerLeftPoints.clone()\n",
    "\n",
    "    #     lowerRightPoints[:,0] += 0.001 \n",
    "    #     upperRightPoints += 0.001\n",
    "    #     upperLeftPoints[:,1] += 0.001 \n",
    "\n",
    "    #     #print(lowerLeftPoints.shape, lowerRightPoints.shape, upperRightPoints.shape, upperLeftPoints.shape)\n",
    "\n",
    "    #     Hvolume = self(lowerLeftPoints) - self(lowerRightPoints) + self(upperRightPoints) - self(upperLeftPoints)\n",
    "    #     return Hvolume\n",
    "\n",
    "    def Copula_loss_function(self, x): ## MAKE SURE TO PASS IN THE SAME DATAPOINTS AS IN THE INITIALIZATION\n",
    "        x.requires_grad = True\n",
    "        self.unitSquaretensor.requires_grad = True\n",
    "        CopulaGradientObserved = self._CopulaGradient(x)\n",
    "        n_observed = x.shape[0]\n",
    "        CopulaGradientUnitSquare = self._CopulaGradient(self.unitSquaretensor, AsUnitsquare=True)  \n",
    "        n_unitsquare = self.unitSquaretensor.shape[0]\n",
    "        #flagSumData = self._FlagSum(x, self.unitSquaretensor)\n",
    "        pred_unitSquare = self(self.unitSquaretensor)\n",
    "\n",
    "        \n",
    "        L1 = -torch.mean(torch.log(torch.relu(CopulaGradientObserved) + 1e-8))\n",
    "        L2 = torch.mean(torch.relu(-CopulaGradientUnitSquare))\n",
    "        L3 = torch.abs(1 - torch.sum(CopulaGradientUnitSquare / CopulaGradientUnitSquare.shape[0]))\n",
    "        L4 = torch.sum(self(self.lowerBoundary)) + torch.sum(torch.abs(self(self.upperBoundary) - torch.min(self.upperBoundary, dim=1).values.view(-1,1)))      \n",
    "        L5 = (1/(n_unitsquare)) * torch.sum(torch.abs(pred_unitSquare.squeeze() - self.flagSumData/n_observed))\n",
    "\n",
    "        #Hvolume = self._HVolume(self.unitSquaretensor)\n",
    "        #L5 = torch.mean(torch.relu(-Hvolume))\n",
    "\n",
    "        Loss =  self.lossWeight[0]*L1 + self.lossWeight[1]*L2 + self.lossWeight[2]*L3 + self.lossWeight[3]*L4 + self.lossWeight[4]*L5\n",
    "        return Loss, L1, L2, L3, L4, L5\n",
    "    \n",
    "    def _generateLowerBoundPoints(self, d, num_points=100):\n",
    "        grid = np.linspace(0, 1, num_points)\n",
    "        all_surfaces = []\n",
    "        for k in range(d):\n",
    "            grid_points = np.meshgrid(*([grid] * (d - 1)), indexing=\"ij\")\n",
    "            points = np.stack(grid_points, axis=-1).reshape(-1, d - 1)\n",
    "            surface_points = np.insert(points, k, 0, axis=1)\n",
    "            all_surfaces.append(surface_points)\n",
    "        return torch.tensor(np.vstack(all_surfaces), dtype=torch.float32)\n",
    "\n",
    "    def _generateUpperBoundaryPoints(self, d, num_points=100):\n",
    "        oneArray = np.ones((d * num_points, d))\n",
    "        u = np.linspace(0, 1, num_points)\n",
    "        for i in range(d):\n",
    "            oneArray[i * num_points:(i + 1) * num_points, i] = u\n",
    "        return torch.tensor(oneArray, dtype=torch.float32)\n",
    "    \n",
    "    def _flag(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized version of _flag that returns a matrix where each element is a comparison result.\"\"\"\n",
    "        return torch.all(y.unsqueeze(0) < x.unsqueeze(1), dim=2).float()\n",
    "\n",
    "    def _FlagSum(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Vectorized computation of FlagSum without explicit Python loops.\"\"\"\n",
    "        return self._flag(x, y).sum(dim=1)\n",
    "\n",
    "    def train_model(self, X, epochs=5000, log_interval=500):\n",
    "            dataset = TensorDataset(X)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                for batch in dataloader:\n",
    "                    batch_X = batch[0]  # since TensorDataset returns a tuple\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss, L1, L2, L3, L4, L5 = self.Copula_loss_function(batch_X)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                if epoch % log_interval == 0:\n",
    "                    print(f'Epoch {epoch}, Loss: {loss.item()}, Losses: L1: {L1.item()}, L2: {L2.item()}, L3: {L3.item()}, L4: {L4.item()}, L5: {L5.item()}')\n",
    "\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            self.isTrained = True  # Set the flag to indicate that the model has been trained  \n",
    "\n",
    "    def sample(self, n=1000):\n",
    "            if not self.isTrained:\n",
    "                print(\"Model not trained. Training model...\")\n",
    "                self.train_model(self.ObservedData)\n",
    "            # Build the grid to compute M\n",
    "            dim_range = torch.linspace(0, 1, 1000)\n",
    "            grid = torch.stack(torch.meshgrid(dim_range, dim_range, indexing='ij'), dim=-1).flatten(0, 1).requires_grad_()\n",
    "            C_uv = self(grid)\n",
    "            grad_u = torch.autograd.grad(C_uv, grid, torch.ones_like(C_uv), create_graph=True)[0][:, 0]\n",
    "            grad_uv = torch.autograd.grad(grad_u, grid, torch.ones_like(grad_u), create_graph=True)[0][:, 1]\n",
    "            M = grad_uv.max().item()\n",
    "\n",
    "            # Initialize empty list to collect samples\n",
    "            accepted_samples = []\n",
    "            total_accepted = 0\n",
    "\n",
    "            # While not enough samples\n",
    "            while total_accepted < n:\n",
    "                # Propose candidates\n",
    "                batch_size = 1*n  \n",
    "                u1 = torch.rand(batch_size, 1)\n",
    "                u2 = torch.rand(batch_size, 1)\n",
    "                u = torch.cat((u1, u2), dim=1).requires_grad_(True)\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self(u)\n",
    "                grad_u1 = torch.autograd.grad(y_pred, u, torch.ones_like(y_pred), create_graph=True)[0][:, 0]\n",
    "                grad_u1_u2 = torch.autograd.grad(grad_u1, u, torch.ones_like(grad_u1), create_graph=True)[0][:, 1]\n",
    "                acceptance_prob = grad_u1_u2 / M\n",
    "                random_uniform = torch.rand_like(acceptance_prob)\n",
    "                accepted_batch = u[random_uniform <= acceptance_prob]\n",
    "\n",
    "\n",
    "                total_accepted += accepted_batch.shape[0]\n",
    "                print(f'Accepted: {total_accepted}')\n",
    "                accepted_samples.append(accepted_batch)\n",
    "\n",
    "            # Concatenate all accepted batches\n",
    "            accepted_samples = torch.cat(accepted_samples, dim=0)\n",
    "\n",
    "            # Only keep exactly n samples\n",
    "            accepted_samples = accepted_samples[:n]\n",
    "            return accepted_samples\n",
    "    \n",
    "    def plotSamples(self, sample, ProbSpace = True):\n",
    "        df_samples = pd.DataFrame({\n",
    "        \"U1\": sample[:,0].flatten(),  \n",
    "        \"U2\": sample[:,1].flatten()\n",
    "        })\n",
    "        sns.jointplot(\n",
    "            data=df_samples, x=\"U1\", y=\"U2\", kind=\"scatter\",\n",
    "            marginal_kws=dict(bins=30, fill=True),\n",
    "            joint_kws={\"s\": 10, \"edgecolor\": \"none\"}  # Removes white outline\n",
    "        )\n",
    "        if ProbSpace:\n",
    "            plt.suptitle(\"Sampled data in probability space\", y=1.02);  \n",
    "        else:\n",
    "            plt.suptitle(\"Sampled data in return space\", y=1.02);\n",
    "        plt.show()\n",
    "        pass\n",
    "\n",
    "class NeuralCopula():\n",
    "    def __init__(self, data, params):\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.num_neurons = params['num_neurons']\n",
    "        self.lr = params['lr']\n",
    "        self.scheduler = params['scheduler']\n",
    "        self.solver = params['solver']\n",
    "        self.epochs = params['epochs']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.uniform_points = params['uniform_points']\n",
    "        self.boundary_points = params['boundary_points']\n",
    "        self.LossWeights = np.array([params['L1_weight'], params['L2_weight'], params['L3_weight'], params['L4_weight'], params['L5_weight']])\n",
    "\n",
    "        \n",
    "        self.Marginal1 = None\n",
    "        self.Marginal2 = None\n",
    "        self.Copula = None\n",
    "        self.initialCopulaWeights = None\n",
    "        self.SetInitialWeights = None\n",
    "        self.copulaTrainingTime = None\n",
    "        self.data = data\n",
    "        self.normalizedData = None\n",
    "        self.normalizedDataAsTensor = None\n",
    "        self.isNormalized = False\n",
    "\n",
    "        ## Normalization variables\n",
    "        self.scaling = 2.0\n",
    "        self.M1_upper = None\n",
    "        self.M1_lower = None\n",
    "        self.M2_upper = None\n",
    "        self.M2_lower = None\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"{self.device} is used to train model\")\n",
    "\n",
    "    def normalizeData(self,scaling=2.0):\n",
    "        scaling = 2.0\n",
    "        n = self.data.shape[0]\n",
    "        d = self.data.shape[1]\n",
    "\n",
    "        self.M1_upper = scaling * np.max(self.data[:,0])\n",
    "        self.M1_lower = scaling * np.min(self.data[:,0])\n",
    "        self.M2_upper = scaling * np.max(self.data[:,1])\n",
    "        self.M2_lower = scaling * np.min(self.data[:,1])\n",
    "\n",
    "        M1boundaryPoints =  np.array([self.M1_upper, self.M1_lower]) # Creates points for bounds of what the data generated can be\n",
    "        M2boundaryPoints =  np.array([self.M2_upper, self.M2_lower]) # Creates points for bounds of what the data generated can be\n",
    "        extendedData = np.zeros((n+2, d))\n",
    "        extendedData[:,0] = np.concatenate((self.data[:,0], M1boundaryPoints)) # Adding boundary points to the data\n",
    "        extendedData[:,1] = np.concatenate((self.data[:,1], M2boundaryPoints))\n",
    "        self.normalizedData= (extendedData - np.min(extendedData,axis=0)) / (np.max(extendedData,axis=0) - np.min(extendedData,axis=0))\n",
    "        self.normalizedDataAsTensor = torch.tensor(self.normalizedData, dtype=torch.float32).to(self.device)\n",
    "        self.isNormalized = True\n",
    "        pass\n",
    "\n",
    "    def denormalizeData(self,NormalizedData):\n",
    "        DeNormalizedData = NormalizedData * (np.max(self.data, axis=0) - np.min(self.data, axis=0)) + np.min(self.data, axis=0)\n",
    "        return DeNormalizedData\n",
    "\n",
    "    def fitModel(self, method = 'regular'):\n",
    "\n",
    "        ## Training marginals\n",
    "        self.Marginal1 = MarginalModel(device = self.device, num_layers=6, num_neurons=10, lr=0.01).to(self.device)\n",
    "        self.Marginal2 = MarginalModel(device = self.device, num_layers=6, num_neurons=10, lr=0.01).to(self.device)\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print('Marginal Model 1 Training')\n",
    "        self.Marginal1.train_model(self.normalizedDataAsTensor[:-2,0].view(-1, 1) , epochs=1500, log_interval=500)\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print('Marginal Model 2 Training')\n",
    "        self.Marginal2.train_model(self.normalizedDataAsTensor[:-2,1].view(-1, 1) , epochs=1500, log_interval=500)\n",
    "        ## Training copula\n",
    "        self.Copula = CopulaModel(self.device, self.normalizedDataAsTensor[:-2], self.Marginal1, self.Marginal2, num_layers=self.num_layers, num_neurons=self.num_neurons, lr=self.lr, solver=self.solver, scheduler=self.scheduler, batch_size=self.batch_size, boundary_points=self.boundary_points, uniform_points=self.uniform_points, lossWeight = self.LossWeights).to(self.device)\n",
    "        if self.SetInitialWeights is not None:\n",
    "            self.Copula.load_state_dict(self.SetInitialWeights)\n",
    "\n",
    "        self.initialCopulaWeights = self.Copula.state_dict()\n",
    "        print('---------------------------------------------------------------------')\n",
    "        print('Training copula model')\n",
    "        start_time = time.time()\n",
    "        self.Copula.train_model(self.normalizedDataAsTensor[:-2], epochs=self.epochs, log_interval=500)\n",
    "        self.copulaTrainingTime = time.time() - start_time\n",
    "        print('Training done')\n",
    "        pass\n",
    "\n",
    "    def sample(self, Plot = False, n=1000):\n",
    "        copulaSample = self.Copula.sample(n)\n",
    "        marginalSamples = self.sample_marginals(copulaSample)\n",
    "        denormalizedSamples = self.denormalizeData(marginalSamples)\n",
    "        if Plot:\n",
    "            self.Copula.plotSamples(copulaSample, ProbSpace = True)\n",
    "            self.Copula.plotSamples(denormalizedSamples, ProbSpace = False)\n",
    "        return denormalizedSamples\n",
    "\n",
    "    def sample_marginals(self,probabilityValues, n=1000):\n",
    "        sample_marginal1 = self.Marginal1.newSamples(ProbabilityValues=probabilityValues[:,0])\n",
    "        sample_marginal2 = self.Marginal2.newSamples(ProbabilityValues=probabilityValues[:,1])\n",
    "        samples = np.column_stack((sample_marginal1, sample_marginal2))\n",
    "        return samples\n",
    "\n",
    "    def sample_copula(self, Plot = False, n = 1000):\n",
    "        copulaSample = self.Copula.sample(n)\n",
    "        if Plot:\n",
    "            self.Copula.plotSamples(copulaSample, ProbSpace = True)\n",
    "        return copulaSample\n",
    "    \n",
    "class DatasetTester():\n",
    "    def __init__(self, datasets, grid, runs = 1 ):\n",
    "        self.datasets = datasets\n",
    "        self.grid = grid\n",
    "        self.runs = runs\n",
    "        self.resultDict = {}\n",
    "        self.isTrained = False\n",
    "        self.LossDict = {}\n",
    "\n",
    "    def runTest(self):\n",
    "        for name, data in self.datasets.items():\n",
    "            # Creating ProgressLogger\n",
    "            file_path = 'Results_' + name + '.json'\n",
    "            logger = ProgressLogger(file_path=file_path)\n",
    "\n",
    "            print(f'######################### Running test with dataset: {name} ###########')\n",
    "            print(data[0:3,:])\n",
    "            Mtester = MethodTester(data=data, grid = self.grid, runs = 1)\n",
    "            Mtester.runTest()\n",
    "            self.resultDict[name] = Mtester\n",
    "\n",
    "        self.isTrained = True\n",
    "        pass\n",
    "\n",
    "    def evaluateResults(self):\n",
    "        print('############################ Result printout ##############################')\n",
    "        DatasetsKeys = list(self.resultDict.keys())\n",
    "        ModelKeys = list(self.resultDict[DatasetsKeys[0]].resultDict.keys())\n",
    "\n",
    "        for modelKey in ModelKeys:\n",
    "            Loss = 0\n",
    "            trainingTime = 0\n",
    "            constraintLoss = 0\n",
    "            for dataKey in DatasetsKeys:\n",
    "                constraintLoss += self.resultDict[dataKey].resultDict[modelKey].L2 +self.resultDict[dataKey].resultDict[modelKey].L3 +self.resultDict[dataKey].resultDict[modelKey].L4 + self.resultDict[dataKey].resultDict[modelKey].L5\n",
    "                Loss += self.resultDict[dataKey].resultDict[modelKey].L1 +self.resultDict[dataKey].resultDict[modelKey].L2 +self.resultDict[dataKey].resultDict[modelKey].L3 + self.resultDict[dataKey].resultDict[modelKey].L4 + self.resultDict[dataKey].resultDict[modelKey].L5\n",
    "                trainingTime += self.resultDict[dataKey].resultDict[modelKey].copulaTrainingTime\n",
    "                \n",
    "            print('----------------------------------------------------------------------------')\n",
    "            print(f'Model: {modelKey} ')\n",
    "            print(f'Total loss: {Loss}')\n",
    "            print(f'Average training time of copula model: {trainingTime/len(DatasetsKeys)} seconds') \n",
    "            print(f'Constraint loss: {constraintLoss}')\n",
    "            self.LossDict[modelKey] = Loss \n",
    "        print('###########################################################################')\n",
    "\n",
    "class MethodTester():\n",
    "    def __init__(self, data, grid, runs = 1, logger = None):\n",
    "        self.logger = logger\n",
    "        self.data = data\n",
    "        self.grid = grid\n",
    "        self.runs = runs\n",
    "        self.resultDict = {}\n",
    "\n",
    "\n",
    "    def runTest(self):\n",
    "        for params in ParameterGrid(self.grid):\n",
    "            print(f\"Running test with options: {params}\")\n",
    "            ## Regular training\n",
    "            print('Regular training')\n",
    "            self._trainRegular(params)\n",
    "            print('----------------------------------------------------------------------------')\n",
    "\n",
    "    def _trainRegular(self, params):\n",
    "        testerList = []\n",
    "        for run in range(self.runs):\n",
    "            ## Set random seed for reproducibility (maybe)\n",
    "            print(f\"Run {run+1} of {self.runs}\")\n",
    "            tester = ParameterTester(self.data, params)\n",
    "            tester.runTest()\n",
    "            testerList.append(tester)\n",
    "\n",
    "        ## Inspect model results and choose best\n",
    "        #testerLosses = [tester.finalLoss.item()  for tester in testerList]   ## make this unweighted losses\n",
    "        testerLosses = [tester.L1.item() + tester.L2.item() + tester.L3.item() + tester.L4.item() + tester.L5.item()  for tester in testerList]   ## make this unweighted losses\n",
    "\n",
    "        ## store model with smallest loss \n",
    "        bestTester = testerList[np.argmin(testerLosses)] ## [testerConstraintLosses < 0.01]\n",
    "        key = frozenset(params.items())\n",
    "        TrainingData = {\"L1\": bestTester.L1.item(), \"L2\": bestTester.L2.item(), \"L3\": bestTester.L3.item(), \"L4\": bestTester.L4.item(), \"L5\": bestTester.L5.item(), \"time\":bestTester.trainingTime}\n",
    "\n",
    "        if self.logger is not None:\n",
    "            # Now you can call `logger.add_run()` anywhere in your program\n",
    "            key_json = \"__\".join(f\"{k}={v}\" for k, v in sorted(params.items()))\n",
    "\n",
    "            self.logger.add_run(key_json, TrainingData)\n",
    "\n",
    "        self.resultDict[key] = bestTester\n",
    "        \n",
    "\n",
    "class ParameterTester():\n",
    "    def __init__(self, data, parameters, initialWeights = None):\n",
    "        #self.data = data\n",
    "        self.NC = NeuralCopula(data, parameters)\n",
    "        self.NC.SetInitialWeights = initialWeights\n",
    "\n",
    "        self.NC.normalizeData()\n",
    "        self.initialWeights = None\n",
    "        self.finalWeights = None\n",
    "        self.finalLoss = None\n",
    "        self.L1 = None\n",
    "        self.L2 = None \n",
    "        self.L3 = None\n",
    "        self.L4 = None\n",
    "        self.L5 = None\n",
    "        self.trainingTime = None\n",
    "        print('in parameter tester')\n",
    "        print(data[0:3,:])\n",
    "        \n",
    "    def runTest(self, method = 'regular'):\n",
    "        if method == 'regular':\n",
    "            self.NC.fitModel()\n",
    "\n",
    "        self.copulaTrainingTime = self.NC.copulaTrainingTime\n",
    "        self.finalLoss, self.L1, self.L2, self.L3, self.L4, self.L5 = self.NC.Copula.Copula_loss_function(self.NC.normalizedDataAsTensor[:-2])\n",
    "        self.initialWeights = self.NC.initialCopulaWeights\n",
    "        self.finalWeights = self.NC.Copula.state_dict()\n",
    "        self.trainingTime = self.NC.copulaTrainingTime\n",
    "        \n",
    "class ProgressLogger:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def add_run(self, run_id, run_data):\n",
    "        data = self._load()\n",
    "        data[run_id] = run_data\n",
    "        self._save(data)\n",
    "\n",
    "    def _load(self):\n",
    "        if os.path.exists(self.file_path):\n",
    "            try:\n",
    "                with open(self.file_path, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "        return {}\n",
    "\n",
    "    def _save(self, data):\n",
    "        with open(self.file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "###############  Running the code ###############      \n",
    "#################################################\n",
    "\n",
    "## Generating datasets \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "corrMat = np.array([[1, 0.7], [0.7, 1]])\n",
    "A = np.linalg.cholesky(corrMat)\n",
    "Z = np.random.standard_normal((1000, 2))\n",
    "X = (A @ Z.T).T\n",
    "\n",
    "corrMatFrechetUpper = np.array([[1, 0.9999], [0.9999, 1]])\n",
    "corrMatFrechetLower = np.array([[1, -0.9999], [-0.9999, 1]])\n",
    "A_U = np.linalg.cholesky(corrMatFrechetUpper)\n",
    "A_L = np.linalg.cholesky(corrMatFrechetLower)\n",
    "X_U = (A_U @ Z.T).T\n",
    "X_L = (A_L @ Z.T).T\n",
    "X_neg = X.copy()\n",
    "X_neg[:,1] = -X_neg[:,1]\n",
    "\n",
    "datasets = {\n",
    "    'InDepWeight': Z,\n",
    "    'PosDepWeight': X,\n",
    "    'NegDepWeight': X_neg,\n",
    "    'FrecUpWeight': X_U,\n",
    "    'FrecLoWeight': X_L,\n",
    "}\n",
    "\n",
    "### Grid used in Big test \n",
    "# grid = {\n",
    "#      'num_layers': [ 2, 3, 4],\n",
    "#      'num_neurons': [5, 10],\n",
    "#      'lr': [0.1, 0.01], \n",
    "#      'scheduler': ['step', 'exponential', None],\n",
    "#      'solver': ['adam', 'sgd'], \n",
    "#      'epochs': [ 10000, 5000],\n",
    "#      'batch_size': [1024, 2048], \n",
    "#      'uniform_points': [50], # per dimension really n^2\n",
    "#      'boundary_points': [50], # per dimension\n",
    "#      'L1_weight': [1],\n",
    "#      'L2_weight': [1],\n",
    "#      'L3_weight': [1],\n",
    "#      'L4_weight': [1],\n",
    "#      'L5_weight': [1],\n",
    "#  }\n",
    "\n",
    "\n",
    "### Grid used in weight test (These are the best parameter values of the tested parameter values)\n",
    "grid = {\n",
    "      'num_layers': [3],\n",
    "      'num_neurons': [10],\n",
    "      'lr': [0.1], \n",
    "      'scheduler': ['exponential'],\n",
    "      'solver': ['adam'], \n",
    "      'epochs': [10000],\n",
    "      'batch_size': [2048], \n",
    "      'uniform_points': [50], # per dimension really n^2\n",
    "      'boundary_points': [50], # per dimension\n",
    "      'L1_weight': [1],\n",
    "      'L2_weight': [2],\n",
    "      'L3_weight': [0.5],\n",
    "      'L4_weight': [1],\n",
    "      'L5_weight': [1],\n",
    "  }\n",
    "\n",
    "\n",
    "DataTester = DatasetTester(datasets, grid, runs = 3)\n",
    "DataTester.runTest()\n",
    "DataTester.evaluateResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the finished models\n",
    "def PlotCopulaSurface(copula, title = \"\"):\n",
    "    # Create meshgrid\n",
    "    u1 = np.linspace(0, 1, 500)\n",
    "    u2 = np.linspace(0, 1, 500)\n",
    "    U1, U2 = np.meshgrid(u1, u2, indexing=\"ij\")\n",
    "    grid = np.column_stack((U1.ravel(), U2.ravel()))\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "    # Get model predictions\n",
    "    copula.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = copula(grid_tensor)\n",
    "    Z_pred = predictions.numpy().reshape(500, 500)  \n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(U1, U2, Z_pred, cmap=\"viridis\")\n",
    "    ax.set_xlabel(\"u1\")\n",
    "    ax.set_ylabel(\"u2\")\n",
    "    ax.set_zlabel(\"C(u1, u2)\")\n",
    "    ax.set_title(f\"Model Prediction Surface for {title} data\")\n",
    "    ax.view_init(elev=15, azim=226)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "DatasetsKeys = list(datasets.keys())\n",
    "titleList = ['Independent', 'Positive Dependence', 'Negative Dependence', 'Frechet Upper', 'Frechet Lower']\n",
    "counter = 0\n",
    "for dataKey in DatasetsKeys:\n",
    "    MethodTesterKey = list(DataTester.resultDict[dataKey].resultDict.keys())[0]\n",
    "\n",
    "    PlotCopulaSurface(DataTester.resultDict[dataKey].resultDict[MethodTesterKey].NC.Copula, title = titleList[counter])\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
